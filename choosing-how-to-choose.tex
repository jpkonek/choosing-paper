\documentclass[a4paper]{article}
\usepackage[pagewise]{lineno}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{a4wide}
\usepackage{setspace}
\usepackage{makeidx}
\usepackage[skip=5pt plus1pt]{parskip}
\usepackage{natbib}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{makecell}

\usepackage[breaklinks,colorlinks,linkcolor=black,citecolor=black,urlcolor=gray,hypertexnames=false]{hyperref} % the addition of hypertexnames is for autonum
\usepackage[capitalize]{cleveref}
\usepackage{tikz}
\usepackage{xfrac}
\usepackage{wrapfig}
\usepackage{mathpazo}
\usepackage{lscape}
\usepackage{stmaryrd}
%\usepackage{caption}\captionsetup{font=footnotesize} %% CCM: it was breaking labels that come after table!
%\usepackage[USenglish]{babel}
\usepackage{graphicx}

\usepackage{multirow}


% Define "struts" as suggested by Claudio Beccari in
% a piece in TeX and TUG News, Vol. 2, 1993.
\newcommand\Tstrut{\rule{0pt}{2.6ex}}       % "top" strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts


\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{metatheorem}[theorem]{Metatheorem}
\newtheorem{definition}{Definition}
\newtheorem{metadefinition}[theorem]{Metadefinition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{metaproposition}[theorem]{Metaproposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{metalemma}[theorem]{Metalemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{metacorollary}{Metacorollary}
\newtheorem{example}{Example}
\newtheorem{ruleinf}{Rule of Inference}
%\doublespacing

%\linenumbersx

\DeclareMathOperator*{\argmin}{\arg\,\min}
\DeclareMathOperator*{\argmax}{\arg\,\max}

\DeclareMathOperator*{\arginf}{\arg\,\inf}
\DeclareMathOperator*{\argsup}{\arg\,\sup}


% Load amsfonts for \mathbb (optional, but recommended)
%\usepackage{amsfonts}
% Load mathbbol to extend \mathbb to Greek letters






\newcommand\F{\mathcal{F}}
\newcommand\X{\mathcal{X}}
\newcommand\Y{\mathcal{Y}}
\newcommand\A{\mathcal{A}}
\renewcommand\P{\mathbb{P}} % impreicse prob 
\newcommand\cl{\mathrm{cl}}
%\newcommand\PowN{\imprecpickstrat} % power set of set of prob strategies
\newcommand\Uset{\mathbb{U}} % set of utility functions
\newcommand\Prob{\mathcal{P}} % set of all probs

\newcommand\C{\mathcal{C}}
\newcommand\Exp{\mathsf{Exp}}
\newcommand\RExp{\mathrm{R}\Exp} % Risk-weighted expected utility 
\newcommand\EU{\mathrm{EU}}
\newcommand\MEU{\mathrm{MEU}}
\newcommand\REU{\mathrm{REU}}
\newcommand\EAd{\mathrm{EAd}}
\renewcommand\O{\mathcal{O}}
\newcommand\U{\mathfrak{U}} % utility random variable 
\newcommand\Uwald{\mathcal{U}} % utility random variable 
\newcommand\ut{u}
\newcommand\Maxi{\Gamma\mathrm{M}}
\newcommand\Maximality{\mathrm{Max}}
\newcommand\Maximin{\Gamma}
\newcommand\GD{\mathrm{GD}}

%\newcommand\D{\mathsf{DecProblems}}
\newcommand{\D}{\mathcal{D}}
	\newcommand{\Decs}{\mathcal{D}}
	
\renewcommand\S{\mathcal{S}}
\newcommand\s{\mathsf{s}}
\renewcommand\c{\mathsf{c}} % choice function

\newcommand{\prpickstrat}{\mathsf{n}}
\newcommand\PrPickStrategies{\mathcal{N}}
\newcommand{\imprecpickstrat}{\mathbb{N}}

\newcommand{\IB}{\mathbb{B}}
\newcommand{\ID}{\mathbb{M}}
\newcommand{\IP}{\P}
\newcommand{\pb}{b}
%\newcommand{\pb}{}
%\renewcommand{\color}[2]{}
\newcommand{\Dom}{\mathsf{Dom}}


\renewcommand{\Re}{\mathbb{R}}


\renewcommand{\mathsterling}{\text{\normalfont\textsterling}} % to be able to use \pounds in math mode
\newcommand{\million}{\mathrm{m}}   % million shorthand


% CCM added
\usepackage[textsize=footnotesize,obeyFinal,backgroundcolor=orange!50,bordercolor=gray!20,linecolor=gray!20]{todonotes} % use "\PassOptionsToPackage{disable}{todonotes}" to disable in a document
\newcommand{\todoinfo}[2][]{\todo[inline,{#1}]{#2}}
\newcommand{\todoold}[2][]{\todo[backgroundcolor=white,bordercolor=orange!10,linecolor=gray!10, #1,caption={},textcolor=gray]{Pre-rev: #2}}
\newcommand{\comment}[2][]{\todoold[#1]{#2}}
\newcommand{\todooldinfo}[2][]{\todoold[#1]{#2}}
\usepackage{booktabs}  % For improved table formatting


\makeatletter
\@ifclasswith{article}{final}{
	% Redefine commands or settings for the final version.
\renewcommand{\color}[1]{}
}{
	% Alternative definition for when final is not present.
%	\renewcommand{\mycommand}{Draft Version}
}
\makeatother


\usepackage{mathtools} % using it for definition of \Set
\usepackage{autonum} % only number equations referenced to
\usepackage{bm} % get bold greek symbols
\newenvironment{colored}[1]{\leavevmode\color{#1}}{}
\usepackage{nicefrac}

\newcommand{\Strategies}{\S}
\newcommand{\Conv}{\mathsf{ConvHull}}

\newcommand\SetDelimiter[1][]{
	\nonscript\,#1\vert \allowbreak \nonscript\,\mathopen{}}
\providecommand\given{\SetDelimiter}
\DeclarePairedDelimiterX\Set[1]{\lbrace}{\rbrace}%
{ \renewcommand\given{\SetDelimiter[\delimsize]} #1 } % Spacing around \given in a set. \Set{x\given blah} has good spacing. Can also use starred version \Set*{x\given blah} when blah is multilined so it'll stretch to contain everything. 
\DeclarePairedDelimiterX\seq[1]{\langle}{\rangle}%
{ \renewcommand\given{\SetDelimiter[\delimsize]} #1 } % Spacing around \given in a set. \Set{x\given blah} has good spacing. Can also use starred version \Set*{x\given blah} when blah is multilined so it'll stretch to contain everything. 
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%

%\newrobustcmd*{\citefirstlastauthor}{\AtNextCite{\DeclareNameAlias{labelname}{given-family}}\citeauthor}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
%\newcommand{\vec}{def}

\renewcommand{\emptyset}{\varnothing}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}




\newenvironment{CCM rewritten}
{\begingroup\color{blue}} % Begin environment
{\endgroup}              % End environment


\usepackage{versions}
\excludeversion{infversion}

\begin{document}

\title{Choosing How to Choose}
%\date{\today}
\author{Richard Pettigrew, Catrin Campbell-Moore, Jason Konek}
\maketitle

%\tableofcontents


\begin{comment}
	\todooldinfo[inline]{CCM
	
Todo/questions:
\begin{enumerate}
	\item Refs to Teddy?
	\item Indep stuff
	\item imprecise nu and ead?
	\item Add a bit more explicit proof of result for maximality stuff, it's not immediate and I need to think it through to get the non-independence version clear in my head!
	\item ...?
\end{enumerate}
}
\end{comment}


\todo{somewhere need reference to Seidenfeld ``DECISION THEORY WITHOUT
	"INDEPENDENCE" OR WITHOUT
	"ORDERING"''}


\todo[inline]{referee: I think the summary of results on p. 29 [concln] is very helpful, but it might be helpful to have a chart as well, and also to have some sort of summary of these results in the introduction too.}
%\todooldinfo[color=red]{19/2 - I've come to a halt at least today. The imprecise nu etc is still fluffy. not sure if I'll come back to it soon}
%\todooldinfo{from CCM: red text =  delete, but not ready to do so yet. Purple is stuff I'm considering and think would be good to be}
%CCM trying to rewrite with strategies. 

It is our lot to face decisions when it is uncertain which act from among those available to us will lead to the best outcome. Uncertain about the day's weather, you must choose whether or not to take an umbrella when you leave your house; uncertain about what it would help them most to hear, you must choose what to say to a friend who is going through a bad time; and uncertain what effect different approaches will have, a parent must choose how to raise their child. How are we to make such choices? It is the task of decision theory to provide an answer. And philosophers, economists, and psychologists have met this remit by developing a slew of rival theories of rational decision. Expected utility theory is the most well known and widely used, but there are many alternatives available, and we will meet a good few in the course of this paper.

There are various ways to argue for your preferred decision theory. You might note that it agrees with your intuitive verdict about a specific decision problem that you describe, while its rivals don't. For instance, you might intuitively judge the Allais or Ellsberg preferences rationally permissible, and note that certain risk- or ambiguity-sensitive decision theories permit them, while expected utility theory does not. Or you might note that your favoured theory has a formal feature that you find intuitively desirable, while its rivals lack that feature. For instance, you might intuitively judge the Independence Axiom or the Sure Thing Principle a requirement of rationality, and note that expected utility theory satisfies both, while its risk-sensitive rivals don't.
	
But there is another approach, and it has the advantage that it avoids such appeals to our intuitive judgments and the stalemates in which they often result. It begins with the observation that a decision theory is an account of rational means-ends reasoning: agnostic about whether your ends are good or bad, desirable or undesirable, benevolent or malign, it purports to tell you the rational way to pick between different possible means to the ends you in fact have. Granted this, it seems natural to assess a decision theory by asking how well it performs in the role of getting you those ends. The only problem with this approach is that, in order to assess a decision theory or anything else as a means to your ends, we need an account of which means to your ends it is rational to use. And without a decision theory, we don't have that. 
	
Yet all is not lost, for this line of thinking nonetheless furnishes us with a test we can conduct on a theory of decision-making, and while it might not tell in favour of the theory if it passes, it seems to tell against it if it fails. We can ask of the theory: If I were to use you not only to make my normal day-to-day decisions, but also to make the higher-order decision about which decision theory to use, would you recommend yourself? If it would, we call it \emph{self-recommending}; if it wouldn't, we call it \emph{self-undermining}. We claim that no self-undermining decision theory can be correct. This is not to say that a self-recommending theory is thereby adequate---for instance, the theory that says that any available act is rationally permissible is self-recommending, but it is not correct. Nonetheless, we can use this test to winnow the list of candidate decision theories, removing those that fail it.




%\section{What does it mean to be self-undermining?}

%We now try to make the foregoing more precise. Throughout, we assume a finite state space $\Omega$ and a set of possible acts $\A$. A utility function $\ut$ is a function that takes any act $a$ in $\A$ and state $\omega$ in $\Omega$ and returns a real number $\ut(a, \omega)$ that gives the utility of $a$ at $\omega$. A \emph{decision problem} $D$ is a finite non-empty set of acts. A \emph{choice function} $C$ takes any decision problem $D$ and returns a non-empty subset $C(D)$ of $D$. A \emph{picking strategy} $\s$ takes any decision problem $D$ and returns a specific act $\s(D)$ in $D$. A \emph{decision theory} is a function that takes in certain representations of an agent's attitudes and returns a choice function. So, for instance, expected utility theory takes in your utility function $u$ and your precise probabilities $p$ over $\Omega$ and returns a choice function $\EU_p$ that selects from any decision problem those acts that maximize expected utility by the lights of your probabilities.% $\EU_p(D) = \Set{a \in D \given (\forall a' \in D)(\Exp_p(\ut(a')) \leq \Exp_p(\ut(a)))}.$

%Now, we wish to take a decision theory and ask it to evaluate itself. We wish to ask it: were you to face a choice between using yourself or using any other decision theory, would you consider it permissible to choose to use yourself? In fact, in the first instance, we will ask decision theories to evaluate not decision theories but picking strategies. This is because it is straightforward to define the utility of a picking strategy: faced with a decision problem $D$ and in a state $\omega$, the utility of $\s$ is $\s(D)(\omega)$; that is, it is the utility, at that state, of the act the picking strategy recommends you choose when faced with that decision problem.





%Decision theories provide an account of which available acts are good or bad at getting you to your ends, whether those ends be love or money or something else, and which we capture with a measure of utility. Now, suppose I ask you to decide how you will choose; that is, I ask you to choose which decision theory will guide your choices. We want to say the utility of a particular decision theory is the utility of the act it leads you to choose. But, of course, for most decision theories, there are decisions in the face of which they will not specify a unique recommended act: they may be indifferent between various acts; or, in the case of imprecise probabilities, they may take various actions to be incomparable and not identify a single action to do but instead just rule out some as impermissible. So, in the first instance, we will use decision theories not to judge decision theories, but to judge what we call \emph{picking strategies}. Given a decision problem, a picking strategy specifies a unique act. So we take the utility of a picking strategy to be the utility of the act it recommends you choose.  This utility depends on what the world is like---for instance, does it rain or not---and also which decision problem you face with the decision theory you use---for instance, do you have to choose whether to take an umbrella when you leave the house, or do you have to choose whether to cancel a picnic you've planned. It is the task of a decision theory to say which means to your ends are acceptable or permissible or choiceworthy, and which should be unacceptable or impermissible or rejectionworthy. Some accounts of decision making undermine themselves: when asked which decision theory to choose, they say of themselves that they are unacceptable or impermissible or rejectionworthy. On the face of it, this is a bad thing for a decision theory to do. We say that such a decision theory is \emph{self-undermining}; of the others, we say they're \emph{self-recommending}. We do not say a self-recommending theory is thereby adequate---for instance, the theory that says that any available act is rationally permissible is self-recommending, but it is not correct. Nonetheless, we can use this test to winnow the list of candidate decision theories, removing those that fail it.




%
%It is our lot to have to make decisions between options with uncertain outcomes. Uncertain about the day's weather, you must choose whether or not to take your umbrella when you leave your house. Uncertain about X, you must choose Y. Decision theories can offer some guidance. 

%Expected utility theory is the most well known theory of rational decision. Depending on your credence that it'll rain, and how much you dislike getting wet, it gives recommendations of whether you should take an umbrella or not. Other decision theories have been developed and are argued for, for example, Lara Buchak's (\citeyear{buchak2014rr}) risk-weighted expected utility theory (REU) and Chris Bottomley\ \&\ Timothy Luke Williamson's (\citeyear{bottomley2024rra}) weighted-linear utility theory (WLU) also take into account one's attitudes towards risk.\footnote{Buchak's theory adapts John Quiggin's (\citeyear{quiggin1982tau,quiggin1993geut})  \emph{rank-dependent utility theory} to allow for subjective probabilities, while Bottomley\ \&\ Williamson's adapts a definition due to Soo Hong Chew (\citeyear{chew1983wlu,chew1989aut}).} Still more decision theories say that one's uncertainty should be captured not by precise probabilities, as is done for EUT but instead by imprecise probabilities, and then there are various decision theories to choose from, including E-Admissibility, $\Gamma$-Maximin, and Maximality.

%Now, we said that the utility of a decision theory is the utility of In some cases it is easy to apply the decision theory to judge itself, in particular, when there is a unique recommended action for you to take; in which case we just judge the utility of it to be the utility of what it will lead you to do. But decision theories typically don't specify a unique recommended action; they may be indifferent between various actions or, in the case of imprecise probabilities, they may take various actions to be incomparable and not identify a single action to do but instead just rule out some as impermissible. We will simplify our task by in the first instance asking the decision theory to judge various \emph{picking strategies} which are required to specify a unique action to be undertaken for each decision, in which case it is then easy for us to judge the utility of such a picking strategy by looking at its fruits. 


\section{Risk-sensitive decision theories}\label{sect:reu}

Expected utility theory rules out as irrational certain natural ways of taking risk into account in decision-making. In particular, as noted above, it rules out the so-called \emph{Allais preferences} \citep{allais1953c}. In response, decision theorists have presented a range of alternatives that permit those preferences \citep{kahneman1979pt,machina1982eua,quiggin1982tau, buchak2014rr}. Let's begin by showing a straightforward way in which any such decision theory is self-undermining.

%{\color{red}\todoinfo{CCM playing with rewriting. Actually 3 versions/attempts. }
%	\subsection*{v2}
%			\todoinfo{v2 }
%			
%	A decision problem is given by a set of acts which are available in it. 
%	A decision theory is an account of means-ends reasoning, guiding agents on how to best get to their specified ends. 
%	The agent specifies various parameters, say her utilities and credences, and, at least in the simple case, it tells an agent which option to pick in each decision problem. 
%	
%	We might think of it a bit like any kind of advisor, for example a financial advisor or just a best friend; someone who has the agent's best interests at heart and makes recommendations purely on the basis of the agent's ends, advising the agent on what to do in the various decision problems she might face. 
%	
%	Consider a decision problem with two options, a risky bet and a safe option. Let's say the risky one depends on some particular outcome, say whether this particular company goes bust; and let's suppose that the agent thinks that the odds of this company going bust are $1:10$. She may thus be faced with this decision problem:
%	$$
%	\begin{array}{r|cc}
%		\text{Decision problem } {D}^* & \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		p & \sfrac{1}{11} & \sfrac{10}{11} \Bstrut \\\hline \hline\Tstrut 
%		\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
%		\text{Risky} & \text{\pounds 0m}&\text{\pounds 5m}
%	\end{array}
%	$$	
%%	Consider a decision theory, $\s$ which recommend taking the safe option over the risky option in $D^*$. 
%	
%	A decision theory will specify which is a better option to choose in this decision problem. 
%	Consider two candidate decision theories which differ by means of what they recommend picking in $D^*$. 
%	They provide picking strategies $\s_{\text{Safe}}$ and $\s_{\text{Risky}}$ which pick, respectively, Safe and Risky in $D^*$. 
%	
%	Now suppose that our agent is unsure which decision problem she'll be faced with.
%	Let's suppose her credence in being offered $D^*$ are just $11\%$, and that otherwise she'll get \pounds1m for sure. And, moreover, that you think that whether you'll be facing $D^*$ is probabilistically independent of whether the company will go bust. \todo{why would that be?}
%	Consider a higher-order decision: a decision about whether to use picking strategy $\s_{\text{Safe}}$ and $\s_{\text{Risky}}$.
%	
%	We will evaluate this in the same way we evaluate other decisions: by evaluating which is best at getting to the agent's goals. 
%	We are thus facing this decision: 		
%	$$
%	\begin{array}{c|ccc}
%		\multirow{2}{*}{\text{Version 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%		& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		1A: \s_{\text{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
%		1B: \s_{\text{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
%	\end{array}
%	$$
%	
%	This is extensionally equivalent to one of the choices from the Allais preferences, and the empirically observed preference is to judge 1A over 1B; i.e., $\s_{\text{Safe}}$ as preferable to $\s_{\text{Safe}}$.
%	
%	The other choice offered in the Allais paradox is extensionally equivalent to one where, when the choice is not offered, she is given nothing.
%	$$
%	\begin{array}{c|ccc}
%		\multirow{2}{*}{\text{Version 2}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		P & \sfrac{1}{100} &  \sfrac{10}{100} &  \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		2A:\s_{\text{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 0m}               \\
%		2A: \s_{\text{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 0m}
%	\end{array}
%	$$
%	And here, the empirically observed preference is to prefer 2B to 1A; i.e., now judge $\s_{\text{Risky}}$ as preferable to $\s_{\text{Risky}}$.
%	
%	This is not possible in expected utility theory: her preference cannot depend on the other amount offered. This is essentially Savage's Sure Thing Principle. It thus cannot capture these Allais preferences as rational. 
%	
%	If one adopts a decision theory that permits these Allais preferences, then in one of the cases, it switches its judgement from what is preferable in the individual decision problem $D^*$ to what is preferable if there's only some chance that she's offered it, either in version 1 or version 2. 
%	
%	Let's consider a decision theory which results in picking Safe in $D^*$ but in version 1 of the higher order decision problem, judges 1B to be preferable. 
%	In this case, it judges the only picking strategy which is compatible with its recommendations, $\s_{\text{Safe}}$ to be worse than an alternative picking strategy: $\s_{\text{Risky}}$.
%	The decision theory rules as impermissible picking in accordance with its own recommendations, judging that some other way of picking would be a better means to the agents goals. 
%	
%	The same happens if instead of picking Safe in $D^*$ the theory recommends picking Risky; now in version 2 of the higher order decision problem it judges an alternative picking strategy to be better. 
%	
%	Being self-undermining is \emph{prima facie} bad. Your decision theory makes recommendations both about \emph{picking strategies} and which \emph{options} to choose in different decision problems. But these recommendations pull you in different directions. The picking strategy it recommends tells you to do one thing when faced with $D_1$, and the decision theory itself tells you to do another thing. So there's an instability in the theory's recommendations. The oddity of the situation is akin to one that David \citet[56]{lewis1971iim} identified in a different context:
%	\begin{quote}
%		It is as if Consumer Bulletin were to advise you that Consumer Reports was a best
%		buy whereas Consumer Bulletin itself was not acceptable; you could not possibly
%		trust Consumer Bulletin completely thereafter.
%	\end{quote}
%	
%	Any theory that deems the Allais preferences permissible has this \emph{prima facie} bad-making feature. It is self-undermining in the following sense: (1) there is some precise state of uncertainty you could have about which decision problem you'll face---uncertainty that is represented by a precise probability distribution over the possible decision problems; (2) if you are in that state and apply your decision theory to the question of which picking strategy to use, it rules out any compatible strategy, \emph{i.e.}, any strategy that always avoids picking options that are impermissible according to the original decision theory. %always picks options that are choiceworthy according to the original decision theory. 
%	It demands that you use some picking strategy that is not compatible with it. 
%	
%	
%		This isn't unique to the Allais preferences. Other failures of the Savage's Sure Thing Principle would generate such a way of being uncertain over which decision problem you'll face where the decision theory undermines its recommendations, at least when the failure generates a preference reversal. Similarly for von Neumann-Morgenstern's Independence Principle: if your decision theory deems action $a$ to be preferable to action $b$, but when evaluating an action which tosses a biased coin and selects $a$ (or $b$) if the coin lands Heads and $c$ if the coin lands Tails, the decision theory evaluates the $b$ version to be preferable, then similarly we can generate a way of being uncertain over which decision problem you'll be faced with so that the unique picking strategy which is compatible with the decision theory is itself impermissible according to the theory. 
%		
%		%such that, if you have that uncertainty, and you then apply your decision theory to the question of which picking to use when you face whichever decision problem you in fact face, it says that none of the picking strategies compatible with it are rationally permissible; it demands you use some alternative picking strategy that is not compatible with it. 
%		
%		
%		%We will be able to generate such a challenge using any failure of von Neumann-Morgenstern independence principle or Savage's sure thing principle. 
%		%If your decision theory deems action $a$ to be preferable to $b$ but a certain probabilistic mixture of $b$ with $c$ to be preferable to the same mixture of $a$ with $c$, then supposing you're uncertain if you'll be faced with $D_1$ a choice between $a$ and $b$ and $D_2$ the trivial choice of $c$, with your credences matching the independence violation, then we have a case where the unique picking strategy compatible with the decision theory is deemed impermissible by the decision theory. 
%		%If your decision theory deems action $a$ preferable to $b$ but when there's only a certain probability $p$ that you get $a$ or $b$, and otherwise will get $c$, it deems the mixture with $b$ to be preferable to the mixture with $a$. This 
%	
%	
%	 
%	
%	
%	
%	
%	
%	
%%	Consider also the 
%%	
%%	Consider a higher-order decision: which decision theory to use. 
%%	This is another kind of decision, and we can apply the decision theory to help make this kind of decision too. 
%%	We will ask which decision theory is the best means to the agents ends. 
%%	\todo{picking startegy vs dec theory??}
%%	
%%
%%	
%%	
%%	Now, suppose she is choosing amongst \emph{decision theories}. 
%%	We will judge decision theories by their fruits: by their 
%%	Or, rather, choosing amongst picking strategies. 
%%	In this case, a picking strategy amounts 
%	
%	
%	
%	---
%	
%	\subsection*{v1}
%	
%		\todoinfo{v1 - I'd had in mind to make it like a person recommending rather than the abstract dec theory... but it feels so much just like VoI stuff!}
%	
%	Suppose that you're a financial advisor.
%	You are offering your client advice about how to choose between two options, a risky one and a safe one. Let's say the risky one depends on some particular outcome, say whether this particular company goes bust. Let's suppose that you think the odds of the market crashing are $1:10$. 
%	(For simplicity, we will assume that you are adopting the clients utilities to give the guidance).
%	$$
%	\begin{array}{r|cc}
%		\text{Decision problem} & \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		p & \sfrac{1}{11} & \sfrac{10}{11} \Bstrut \\\hline \hline\Tstrut 
%		\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
%		\text{Risky} & \text{\pounds 0m}&\text{\pounds 5m}
%	\end{array}
%	$$	
%	Perhaps you recommend taking the safe option over the risky option. 
%	
%	Now, suppose that you think that your client is only possibly going to be offered this decision in the morning, let's say your credence is $11\%$. And otherwise she just gets nothing. 
%	You are advising her over what she should pick if offered. 
%	Now the decision problem looks like:
%	$$
%	\begin{array}{r|ccc}
%	\multirow{2}{*}{\text{Scenario 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%		& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		\text{Safe if offered (scenario 1)} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 0m}               \\
%		\text{Risky if offered (scenario 1)}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 0m}
%	\end{array}
%	$$
%	
%	
%	Perhaps you still recommend that if she is offered the choice, she takes the risky option. 
%	Suppose your client has the option of ringing you in the morning to ask what she should do now that she has been offered it. You think she should ring you and do this. 
%	
%	Now suppose that instead, you think that when she's not offered the choice, she is given \pounds 1m rather than nothing. 
%	$$
%	\begin{array}{r|ccc}
%		\multirow{2}{*}{\text{Scenario 2}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut%		P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%	P	&  \sfrac{1}{100} & \sfrac{10}{100} &  \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		\text{Safe if offered (scenario 2)} &\text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
%		\text{Risky if offered (scenario 2)}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
%	\end{array}
%	$$	
%	
%	
%	Expected utility theory says that your evaluation in scenario 1 and scenario 2 should be the same. Whether you recommend taking the risky or the safe option if offered does not depend on the amount you will get if not offered. 
%	This is essentially Savage's Sure Thing Principle. 
%	
%	It is, however, rejected by various alternative theories. The Scenario 1 and Scenario 2 choice are extensionally equivalent to the Allais scenario and the empirically observed preferences are to take the risky option in scenario 1 but the safe option in scenario 2. 
%		
%	What this means as a financial advisor is that if you are in Scenario 2, you think that it would be better for your client to not ring you up in the morning, once she knows what she's faced with, to check what she should do; she should instead just stick with her safe option, or, put another way, you think she should ring up a different financial advisor, one who will tell her to take the safe option. 
%	
%	This is an instability in your recommendation. It cannot be resolved by saying that she recommends the safe option, as this would create instability instead in Scenario 1. Any decision theory which violates the Sure Thing Principle is susceptible to such cases. 
%
%	
%	
%	\bigskip ---
%	
%	This is how we are suggesting to consider how decision theories evaluate their own recommendations.
%	
%	A decision problem is a set of available acts. 
%	Simplifying a bit, a decision theory specifies which act to pick in each decision problem, thus providing what we call a picking strategy. 
%	
%	A decision theory specifies which act to choose in each decision problem, or at least specifies some acts as bad choices. It provides you with a picking strategy 
%	
%	A picking strategy selects one of the available acts from each decision problem. Decision theories give guidance on what are sensible ways to pick when faced with various decision problems.
%	
%	A decision theory gives an account of means-end reasoning, specifying how to choose to best trade off the risk and benefits of various actions that may be available. We can also apply a decision theory itself to evaluate various picking strategies. 
%
%	
%	
%	\bigskip 
%	\subsection*{v3}
%	
%	\todoinfo{v3 - A more minimal alteration:}
%	
%	
%	
%	Here are the four gambles over which the Allais preferences are defined, with the payout of the actions depending on, lets say, the outcome of a lottery with 100 tickets. 
%	$$
%	\begin{array}{r|cccc}
%		& \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\Bstrut \\\hline \Tstrut		P & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut \\\hline 		\hline\Tstrut
%		\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} \\
%		\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}\\\hline
%		\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m} \\
%		\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m}  \\
%	\end{array}
%	$$
%	And the empirically observed preferences are these: $\text{1A} \succ \text{1B}$ and $\text{2A} \prec \text{2B}$. 
%	
%	
%	Now, suppose our agent is facing a decision problem 
%		Consider a decision problem with two options, a risky bet and a safe option. Let's say the risky one depends on some particular outcome, say whether this particular company goes bust; and let's suppose that the agent thinks that the odds of this company going bust are $1:10$. She may thus be faced with this decision problem:
%	$$
%	\begin{array}{r|cc}
%		\text{Decision problem } D^* & \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		p & \sfrac{1}{11} & \sfrac{10}{11} \Bstrut \\\hline \hline\Tstrut 
%		\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
%		\text{Risky} & \text{\pounds 0m}&\text{\pounds 5m}
%	\end{array}
%	$$	
%	
%
%	A decision problem is given by a set of acts which are available in it. 
%	A decision theory is an account of means-ends reasoning, guiding agents on how to best get to their specified ends. 
%	The agent specifies various parameters, say her utilities and credences, and, at least in the simple case, it tells an agent which option to pick in each decision problem. 
%	
%	We might think of it a bit like any kind of advisor, for example a financial advisor or just a best friend; someone who has the agent's best interests at heart and makes recommendations purely on the basis of the agent's ends, advising the agent on what to do in the various decision problems she might face. 
%	
%	A decision theory will specify which is a better option to choose in this decision problem. It guides the agent not only in this particular decision problem but also in a whole range of other decision problems which the agent might face. In simple cases, in each decision problem it identifies the particular act which should be chosen to best get the agent to her goals. 
%	Sometimes, however, there may be matters of incomparability or indifference, in which case it just judges various bad ways of picking.
%	
%	Consider two candidate decision theories which differ by means of what they recommend picking in $D^*$, but which both recommend the Allais preferences. These are spelled out by what we will call a picking strategy, which selects an individual act from each possible decision problem. So consider picking strategies 
%	$\s_{\text{Safe}}$ and $\s_{\text{Risky}}$ with 
%	$$\begin{array}{r|ccc}
%		\s(D)=?&D^*&D_{\text{Allais1}}&D_{\text{Allais2}}\\\hline
%		\s_{\text{Safe}}&\text{Safe}&1A&2B\\
%		\s_{\text{Risky}}&\text{Risky}&1A&2B\\
%	\end{array}$$
%	
%	
%	A \emph{picking strategy} is a function $\s$ that takes a decision problem $D$, which is just a set of available acts, and returns one of those available acts $\s(D)$ from $D$. As \cite{ullman1977} emphasize, there is an important difference between \emph{picking} and \emph{choosing}. Choosing is a matter of settling on what to do in a decision problem for reasons grounded in your preferences. Picking on the other hand is a matter of settling on what to do even after your reasons have run out, either because you are indifferent between the options that you have not rejected or because you find them incomparable. Picking strategies fully settle what to do in every decision problem. We will understand picking inclusively in what follows. If you choose, then you pick, but not vice versa. That is, if you settle on an option for reasons grounded in your preferences, then you count as both choosing and picking that option. If you settle on an option not for preference-based reasons but simply because you must \emph{do something}, then you count as picking but not choosing that option.
%	
%	Now, suppose that you are unsure whether you will be offered $D^*$ or just be given \pounds 1m for sure. 
%	
%	
%	
%
%	
%
%		
%	Now suppose that our agent is unsure which decision problem she'll be faced with.
%	Let's suppose her credence in being offered $D^*$ are just $11\%$, and that otherwise she'll get \pounds1m for sure. And, moreover, that you think that whether you'll be facing $D^*$ is probabilistically independent of whether the company will go bust. \todo{why would that be?}
%	Consider a higher-order decision: a decision about whether to use picking strategy $\s_{\text{Safe}}$ and $\s_{\text{Risky}}$.
%	
%	Just as we said above we'd judge decision theories by how good they are as means to our ends, let us first do that for picking strategies. So, the utility of a picking strategy in a decision problem is just the utility of the act it tells you to pick in that problem. %is just the utility of the act it requires you to choose. That is, we can specify the utility of $\s_1$ and $\s_2$ if we specify the decision problem you face and the state of the world.  
%		To specify the utility of a picking strategy, $\s$, then, we need to specify both the decision problem that you face, $D$, and the state of the world, $\omega$. The strategy selects a unique option from $D$, $\s(D)$, and the world determines the utility of that option, $\s(D)(\omega)$. 
%	We are thus facing this decision: 		
%	$$
%	\begin{array}{c|ccc}
%		\multirow{2}{*}{\text{Version 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%		& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		 \s_{\text{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
%		 \s_{\text{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
%	\end{array}
%	$$
%	But note that this is extensionally equivalent to $D_{\text{Allais1}}$. So since 1A is preferred to 1B by both these picking strategies, $\s_{\text{Safe}}$ judges $\s_{\text{Risky}}$ as preferable to itself. 
%	
%	
%	So if your decision theory rationalises the Allais preferences and also deems Safe as uniquely rational in $D^*$ then if offered the decision over which decision theory to use, it will recommend instead picking $\s_{\text{Risky}}$ rather than in accordance with its own recommendations. Such a decision theory is self-undermining. 
%	
%	Being self-undermining is \emph{prima facie} bad. Your decision theory makes recommendations both about \emph{picking strategies} and which \emph{options} to choose in different decision problems. But these recommendations pull you in different directions. The picking strategy it recommends tells you to do one thing when faced with $D_1$, and the decision theory itself tells you to do another thing. So there's an instability in the theory's recommendations. The oddity of the situation is akin to one that David \citet[56]{lewis1971iim} identified in a different context:
%	\begin{quote}
%		It is as if Consumer Bulletin were to advise you that Consumer Reports was a best
%		buy whereas Consumer Bulletin itself was not acceptable; you could not possibly
%		trust Consumer Bulletin completely thereafter.
%	\end{quote}
%	
%	
%	An analogous challenge arises for a decision theory which recommends Safe over Risky in $D^*$. 
%	Whilst it is self-recommending in our version 1 case, where the agent thinks there's $11\%$ probability that she'll be offered $D^*$ choice and otherwise is given \pounds1m, it is self-undermining when the agent is uncertain whether she'll be offered $D^*$, with probability $11\%$, or whether she'll be given nothing for sure. 
%	$$
%	\begin{array}{c|ccc}
%		\multirow{2}{*}{\text{Version 2}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%		& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut		P & \sfrac{1}{100} &  \sfrac{10}{100} &  \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%		2A:\s_{\text{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 0m}               \\
%		2A: \s_{\text{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 0m}
%	\end{array}
%	$$
%	This is now extensionally equivalent to $D_{\text{Allais2}}$ where it is specified that she prefers 2B to 1A, and thus judges $\s_{\text{Safe}}$ as preferable to $\s_{\text{Risky}}$.
%	
%	
%	\todo{I seemed to need to assume a strict preference of Risky or Safe. What if they're deemed equivalent??}
%	
%	And there is no funny business going on here. Learning that you face decision problem $D_1$ does not tell you anything about which ticket will win, for we assumed that the decision problem is independent of the state of the world. And picking one option or another in $D_1$ does not tell you anything about which state of the world you're in, for we assumed the acts are independent of the states of the world as well.
%	
%	
%	Any theory that deems the Allais preferences permissible has this \emph{prima facie} bad-making feature. It is self-undermining in the following sense: (1) there is some precise state of uncertainty you could have about which decision problem you'll face---uncertainty that is represented by a precise probability distribution over the possible decision problems; (2) if you are in that state and apply your decision theory to the question of which picking strategy to use, it rules out any compatible strategy, \emph{i.e.}, any strategy that always avoids picking options that are impermissible according to the original decision theory. %always picks options that are choiceworthy according to the original decision theory. 
%	It demands that you use some picking strategy that is not compatible with it. 
%	
%	
%	This isn't unique to the Allais preferences. Other failures of the Savage's Sure Thing Principle would generate such a way of being uncertain over which decision problem you'll face where the decision theory undermines its recommendations, at least when the failure generates a preference reversal. Similarly for von Neumann-Morgenstern's Independence Principle: if your decision theory deems action $a$ to be preferable to action $b$, but when evaluating an action which tosses a biased coin and selects $a$ (or $b$) if the coin lands Heads and $c$ if the coin lands Tails, the decision theory evaluates the $b$ version to be preferable, then similarly we can generate a way of being uncertain over which decision problem you'll be faced with so that the unique picking strategy which is compatible with the decision theory is itself impermissible according to the theory. 
%}
%
%{\color{red}

%\subsection*{v4}
%
%\todoinfo{v4}
%
%
%%
%Here are the four gambles over which the Allais preferences are defined, with the payout of the actions depending on, lets say, the outcome of a lottery with 100 tickets. 
%$$
%\begin{array}{r|cccc}
%	& \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\Bstrut \\\hline \Tstrut	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut \\\hline	\hline\Tstrut
%	\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} \\
%	\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}\\\hline
%	\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m} \\
%	\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m}  \\
%\end{array}
%$$
%The empirically observed preferences are $\text{1A} \succ \text{1B}$ and $\text{2A} \prec \text{2B}$. 
%Let's suppose that our agent has a decision theory which endorses these preferences. 
%
%%A decision problem is given by a set of acts which are available in it. 
%%A decision theory is an account of means-end reasoning which, when parameters such as credences and utilities are specified, specifies which options should be picked, or which shouldn't be pic...???
%
%Now, instead consider this decision problem. It has two options, a risky bet and a safe option which depend on, lets say, whether this particular company goes bust. The options might be, for example, options for stocks she can invest in. Let's suppose that the agent thinks that the odds of this company going bust are $1:10$. \todo{I'm happy to instead keep the example as one with lottery tickets, now with 11 tickets... And then whether she's offered it is determined by a toss of a biased coin??}
%She is thus be faced with this decision problem:
%$$
%\begin{array}{r|cc}
%	\text{Decision problem } D^{\mathrm{Allais}}_{\mathrm{local}} & \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut	p & \sfrac{1}{11} & \sfrac{10}{11} \Bstrut \\\hline 	\hline\Tstrut
%	\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
%	\text{Risky} & \text{\pounds 0m}&\text{\pounds 5m}
%\end{array}
%$$	
%Perhaps her decision theory prefers Safe over Risky.
%
%We will now turn to a higher-order decision that the agent might face: a choice amongst various so-called picking strategies. She is unsure which decision problem she will face, and is choosing a strategy for what to pick in each possible decision problem. 
%%She has some credence over various decision problems that she might be faced with.
%
%Perhaps she is leaving instructions for which options her stockbroker should pick, unsure what will be on offer. 
%Or perhaps she has to choose a proxy to act on her behalf, knowing what they'll pick in each decision problem, but unsure which decision problem will be in play. 
%%As she is unavailable to make the decision at the later time. 
%
%
%The form that these options have are what we call \emph{picking strategies}. They are functions, $\s$, which select one of the available acts from each (relevant) decision problem. 
%For example, a picking function will specify either Safe or Risky as its pick in $D^{\mathrm{Allais}}_{\mathrm{local}}$.
%
%The choice between picking functions is another kind of decision problem, a higher-order decision. She can make use of her decision theory to determine which picking function to choose, depending on her credences over which decision problem she'll be faced with, and the utility that the picking strategy leads to in each possibility. We will evaluate picking strategies by their fruits: by the utility of what they select when faced with the specified decision problem and given the state of the world. 
%
%Suppose that she thinks she might be faced with $D^{\mathrm{Allais}}_{\mathrm{local}}$ or just be given \pounds1m for sure. And suppose that her credence that she'll face $D^{\mathrm{Allais}}_{\mathrm{local}}$ is $11\%$. She is deciding amongst various picking strategies. 
%Here, the only relevant consideration in choosing amongst picking strategies is what they will pick when facing $D^{\mathrm{Allais}}_{\mathrm{local}}$. There are thus two options: $\s_{\mathrm{Safe}}$ or $\s_{\mathrm{Risky}}$, which pick Safe or Risky, respectively, in $D^{\mathrm{Allais}}_{\mathrm{local}}$. 
%The decision amongst picking strategies thus amounts to this decision:
%$$
%\begin{array}{c|ccc}
%	\multirow{2}{*}{\text{Version 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%	& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut	p &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%	& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100} \Bstrut \\\hline 	\hline\Tstrut
%	\s_{\mathrm{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
%	\s_{\mathrm{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
%\end{array}
%$$
%This is extensionally equivalent to the choice of 1A vs 1B in $D^{\mathrm{Allais}}_1$. Assuming her decision theory is structural, in the sense that its recommendations don't depend on the content of the outcomes but just the list of credences of various utility outcomes, then she should also judge $\s_{\mathrm{Safe}}$ to be preferable to $\s_{\mathrm{Risky}}$. 
%
%That is, she should want to instruct her stockbroker to take the Safe option when facing $D^{\mathrm{Allais}}_{\mathrm{local}}$, and select a proxy who will pick the Safe course of action. 
%
%Now, however, suppose instead, that whilst she still has the same credence that she'll be offered $D^{\mathrm{Allais}}_{\mathrm{local}}$, she thinks that if it is not on offer, the alternative case is that she gets nothing. 
%Now, she is facing the following decision problem over which picking strategy to select. 
%$$
%\begin{array}{c|ccc}
%	\multirow{2}{*}{\text{Version 2}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%	& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut	p&  \sfrac{1}{100} &  \sfrac{10}{100} &  \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
%	\s_{\mathrm{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 0}               \\
%	\s_{\mathrm{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 0}
%\end{array}
%$$
%This is now extensionally equivalent to the choice of 2A vs 2B in $D^{\mathrm{Allais}}_2$, and we have supposed that she preferred 2B over 2A, matching the observed preferences. In this scenario, then, she will evaluate the picking strategy $\s_{\mathrm{Risky}}$ to be preferable to $\s_{\mathrm{Safe}}$, even though, when faced with  $D^{\mathrm{Allais}}_{\mathrm{local}}$ herself, she prefers Safe.
%
%Given the choice of what to write as instructions for her stockbroker, she prefers to instruct them to act differently to how she would act were she facing the decision herself. Given the choice over which proxy to nominate, she thinks it is better to nominate one that she knows won't choose in the way she would were she in the situation herself. 
%Given the choice of whether she should tie herself to the mast, and pre-commit to a particular course of action, she is willing to pay money to get that rope.\todo{isn't this making it like the others???}
%
%If our agent, using her decision theory, endorses the Allais preferences and recommends Safe over Risky in $D^{\mathrm{Allais}}_{\mathrm{local}}$, then in Scenario 2, she will prefer to write down a picking strategy which does not match her own. 
%That is, her adopted decision theory recommends a picking strategy which is not compatible with its own recommendations. Despite the fact that it recommends picking Safe, it thinks it would be better to use a decision theory which recommends Risky in $D^{\mathrm{Allais}}_{\mathrm{local}}$.
%
%
%This is \emph{prima facie} bad feature of her decision theory. Your decision theory makes recommendations both about \emph{picking strategies} and which \emph{options} to choose in different decision problems. But these recommendations pull you in different directions. The picking strategy it recommends tells you to do one thing when faced with $D^{\mathrm{Allais}}_{\mathrm{local}}$, and the decision theory itself tells you to do another thing. There's an instability in the theory's recommendations. The oddity of the situation is akin to one that David \citet[56]{lewis1971iim} identified in a different context:
%\begin{quote}
%	It is as if Consumer Bulletin were to advise you that Consumer Reports was a best
%	buy whereas Consumer Bulletin itself was not acceptable; you could not possibly
%	trust Consumer Bulletin completely thereafter.
%\end{quote}
%
%%This is, \emph{prima facie}, a bad-making feature of a decision theory. 
%It is self-undermining, in the sense that in this particular scenario, with the particular precise probability distribution over the possible decision problems she might face ($11\%$ for $D^{\mathrm{Allais}}_{\mathrm{local}}$; $89\%$ for sure-\pounds0, a trivial decision problem), if she applies her decision theory to the question of which picking strategy to use, it rules out as impermissible, what she thinks she should do in that scenario.
%
%
%And there is no funny business going on here. Learning that you face decision problem $D^{\mathrm{Allais}}_{\mathrm{local}}$ does not, we are supposing, provide any information about whether the company will go bust. We assumed that the decision problem is independent of the state of the world. We are also assuming act-state independence: picking one option or another in $D^{\mathrm{Allais}}_{\mathrm{local}}$ does not tell you anything about which state of the world you're in.
%
%This was all based on the asumption not only that her decision theory endorsed the Allais preferences, but also that it recommended Safe over Risky in $D^{\mathrm{Allais}}_{\mathrm{local}}$. If instead she preferred Risky over Safe in $D^{\mathrm{Allais}}_{\mathrm{local}}$, however, then an analogous undermining feature would have arisen in version 1: if she thinks she'll face decision problems at $11\%$ for $D^{\mathrm{Allais}}_{\mathrm{local}}$; $89\%$ for sure-\pounds1m, then she prefers to write down a strategy which does not match her own.
%
%What if, however, she is indifferent between Risky over Safe in $D^{\mathrm{Allais}}_{\mathrm{local}}$, then both the picking strategies $\s_{\mathrm{Risky}}$ and $\s_{\mathrm{Safe}}$ are compatible with her preferences. In acting in the world, an agent does actually need to pick an option, even if her reasons for one or the other have run out. Decision theories often leave open various ways to pick. 
%Picking is a matter of settling on what to do, even if her reasons have run out, unlike \emph{choosing} ...\todo{talk about picking vs choosing here???} \citet{ullman1977}. 
%
%With this indifference, if she adopts the Allais preferences, then, whilst which picking strategy she deems optimal changes depending on version 1 or version 2, in neither case does she rule out all the picking strategies which are compatible with her recommendations.
%
%However, assuming that not only she endorses the Allais preferences, but is willing to pay a small price to select the one over the other, then indifference no longer offers a route out. For this argument we need to consider a different probability over possible decisions. 
%Suppose she thinks that she prefers $1A$ to $1B+\pounds 1$,
%and $2B$ to $2A+\pounds 1$. And suppose she thinks with $50\%$ credence that she'll be faced with the choice of $1A$ and $1B+\pounds 1$, and with 50\% credence she'll be faced with the choice of $2B$ and $2A+\pounds 1$. She is thus selecting amongst four possible strategies and her decision is:
%\[
%\begin{array}{r|ccc}
%	& \text{Ticket 1} & \text{Tickets 2--11} & \text{Tickets 12--100} \Bstrut \\\hline \Tstrut	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \\
%	\hline\hline
%	\s_{1A,2A^+}  & \pounds2\million+\pounds1   & \pounds2\million+\pounds1   & \pounds1\million+\pounds1   \\
%	\s_{1A,2B}    & \pounds1\million            & \pounds6\million            & \pounds1\million            \\
%	\s_{1B^+,2A^+}& \pounds1\million+\pounds2   & \pounds6\million+\pounds2   & \pounds1\million+\pounds2   \\
%	\s_{1B^+,2B}  & \pounds1              & \pounds10\million+\pounds1  & \pounds1\million+\pounds1  
%\end{array}
%\]
%
%Even though she prefers $1A$ and $2B$ in the local decisions, the strategy of picking these both is dominated by picking $1B^+$ and $2A^+$, and thus, any plausible decision theory will not deem the picking strategy $\s_{1A,2B}$ to be permissible. It is thus self-undermining decision theory.
%
%This shows that any decision theory which permits the sweetened Allais preferences is self-undermining. Assuming it permits the Allais preferences and is Archimadean, it will permit some sweetened Allais preferences. 
%
%Moreover, any theory which violates the Sure Thing Principle and is Archimadean will generate such a case. Similarly for vNM Independence Principle. 
%
%
%\todoinfo{still to be included: STP claim}
%
%}

%\subsection*{v5}



%{\color{violet}
%
Here are the four options over which the Allais preferences are defined. The payout of each depends on the outcome of a lottery with 100 tickets. 
%$$
%\begin{array}{r|cccc}
%	& \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\\\hline \Tstrut
%	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut\\\hline\hline\Tstrut 
%	\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} \\
%	\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}\Bstrut\\\hline\Tstrut 
%	
%		\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m} \\
%		\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m} 
%\end{array}
%$$
$$
\begin{array}{r|ccc}
	D^{\mathrm{Allais}}_1 & \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\Bstrut \\\hline \Tstrut	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
	\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} \\
	\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}
\end{array}
$$$$
\begin{array}{r|ccc}
	D^{\mathrm{Allais}}_2 & \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\Bstrut \\\hline \Tstrut	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
	\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m} \\
	\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m} 
\end{array}
$$
As Allais notes, many people prefer 1A to 1B in $D^{\mathrm{Allais}}_1$ and 2B to 2A in $D^{\mathrm{Allais}}_2$.
% $\text{1A} \succ \text{1B}$ and $\text{2A} \prec \text{2B}$. 
Let's suppose that our agent has a decision theory which endorses these preferences, given parameters such as her credences and utilities, and possibly also some representation of her attitudes to risk.

%A decision problem is given by a set of acts which are available in it. 
%A decision theory is an account of means-end reasoning which, when parameters such as credences and utilities are specified, specifies which options should be picked, or which shouldn't be pic...???

Now, consider instead a related decision problem, which we will call $D^{\mathrm{Allais}}_{\mathrm{local}}$. There are two actions available to the agent---Safe and Risky---whose outcomes depend on whether a particular company goes bust or not; they might be, for example, different stocks she can invest in. Let's suppose the agent thinks the odds of this company going bust are $1:10$. 
$$
\begin{array}{r|cc}
	\text{Decision problem } D^{\mathrm{Allais}}_{\mathrm{local}} & \text{goes bust} &  \text{not bust} \Bstrut\\\hline \Tstrut
	p & \sfrac{1}{11} & \sfrac{10}{11}\Bstrut\\\hline\hline \Tstrut
	\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
	\text{Risky} & \text{\pounds 0}&\text{\pounds 5m}
\end{array}
$$
%There are two actions that the agent could take: take a risky bet or the safe course of action. Let's say the outcome of the risky bet depends on whether this particular company goes bust. These actions might be, 
Perhaps her decision theory prefers Risky to Safe. 

We will now turn to a higher-order decision that the agent might face.   She is unsure which decision problem she will face, and she must choose a strategy for which option to pick in each possible decision problem. Perhaps she is leaving instructions for what her stockbroker should do, unsure what will be on offer. Or perhaps she has to choose a proxy to act on her behalf, knowing what they'll pick in each decision problem, but unsure which decision problem will be in play. 
%As she is unavailable to make the decision at the later time. 


The form that these options have are what we call \emph{picking strategies}. 
A picking strategy is a functions, $\s$, which selects one of the available options from each (relevant) decision problem. 
For example, a picking function will specify either Safe or Risky as its pick in $D^{\mathrm{Allais}}_{\mathrm{local}}$.

The choice between picking functions is another kind of decision problem, a higher-order decision. Our agent can make use of her decision theory to determine which picking function to choose, depending on her credences over which decision problem she'll face, and the utility that the picking strategy leads to in each possibility. We evaluate a picking strategy by its fruits, that is, by the utility of the option it selects when faced with the specified decision problem and given the state of the world.

Suppose our agent knows she'll either face $D^{\mathrm{Allais}}_{\mathrm{local}}$ or she'll be given \pounds1m for sure. And suppose her credence that she'll face $D^{\mathrm{Allais}}_{\mathrm{local}}$ is $11\%$. She is deciding amongst various picking strategies. 
Here, the only relevant consideration in choosing amongst picking strategies is what they will pick when facing $D^{\mathrm{Allais}}_{\mathrm{local}}$. There are thus two options: $\s_{\mathrm{Safe}}$ or $\s_{\mathrm{Risky}}$, which pick Safe or Risky, respectively, in $D^{\mathrm{Allais}}_{\mathrm{local}}$. 
The decision among picking strategies thus amounts to this decision:
%	$$
%\begin{array}{c|ccc}
%	\multirow{2}{*}{\text{Version 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
%	& \text{goes bust} &  \text{not bust}  \\
%	\hline
%	p &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
%	& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100} \\
%	\hline
%	\hline
%	\s_{\mathrm{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
%	\s_{\mathrm{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
%\end{array}
%$$
%This is extensionally equivalent to the choice of 1A vs 1B in $D^{\mathrm{Allais}}_1$. Assuming her decision theory is structural, in the sense that its recommendations don't depend on the content of the outcomes but just the list of credences of various utility outcomes, then she should also judge $\s_{\mathrm{Safe}}$ to be preferable to $\s_{\mathrm{Risky}}$. 
%
%That is, she should want to instruct her stockbroker to take the Safe option when facing $D^{\mathrm{Allais}}_{\mathrm{local}}$, and select a proxy who will pick the Safe course of action. 
%
%Now, however, suppose instead, that whilst she still has the same credence that she'll be offered $D^{\mathrm{Allais}}_{\mathrm{local}}$, she thinks that if it is not on offer, the alternative case is that she gets nothing. 
%Now, she is facing the following decision problem over which picking strategy to select. 
	$$
\begin{array}{c|ccc}
	\multirow{2}{*}{\text{Version 1}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
	& \text{goes bust} &  \text{not bust}  \Bstrut\\\hline\Tstrut	
	p &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{89}{100} \\
	& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{100}
		\Bstrut\\\hline\hline\Tstrut	
	\s_{\mathrm{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 1m}               \\
	\s_{\mathrm{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 1m}
\end{array}
$$
But of course, this is extensionally equivalent to the choice of 1A vs 1B in $D^{\mathrm{Allais}}_1$. And so, assuming her decision theory is structural, in the sense that its recommendations don't depend on the content of the outcomes but just the list of credences of various utility outcomes, then she should also judge $\s_{\mathrm{Safe}}$ to be preferable to $\s_{\mathrm{Risky}}$, even though, when faced with  $D^{\mathrm{Allais}}_{\mathrm{local}}$ herself, she prefers Risky.
%This is now extensionally equivalent to the choice of 2A vs 2B in $D^{\mathrm{Allais}}_2$, and we have supposed that she preferred 2B over 2A, matching the observed preferences. In this scenario, then, she will evaluate the picking strategy $\s_{\mathrm{Risky}}$ to be preferable to $\s_{\mathrm{Safe}}$, even though, when faced with  $D^{\mathrm{Allais}}_{\mathrm{local}}$ herself, she prefers Safe.

Given the choice of what to write as instructions for her stockbroker, she prefers to instruct them to act differently from how she would act were she facing the decision herself. Given the choice over which proxy to nominate, she thinks it is better to nominate one that she knows won't choose in the way she would were she in the situation herself. 
Given the choice of whether she should tie herself to the mast, and pre-commit to a particular course of action, she reaches for that rope.%\todo{isn't this making it like the others??? RP: which others? CCM: I meant VoI or sequential choice. }\todo{C: pay money? We haven't shown that... it depends on the Archimadeanicity. }

Her adopted decision theory requires her to pick a picking strategy which is not compatible with its own recommendations. 
%If our agent, using her decision theory, endorses the Allais preferences and recommends Risky over Safe in $D^{\mathrm{Allais}}_{\mathrm{local}}$, then her adopted decision theory recommends a picking strategy which is not compatible with its own recommendations. 
Despite the fact that her decision theory recommends picking Risky, it thinks it would be better to use a decision theory which recommends picking Safe.

%\begin{colored}{red}
	%\todo{I moved this para up here}
%There is no funny business going on here. Learning that you face decision problem $D^{\mathrm{Allais}}_{\mathrm{local}}$ does not, we are supposing, provide any information about whether the company will go bust. We assumed that the decision problem is independent of the state of the world. We are also assuming act-state independence: picking one option or another in $D^{\mathrm{Allais}}_{\mathrm{local}}$ does not tell you anything about which state of the world you're in.
%\end{colored}

This is a \emph{prima facie} bad feature of her decision theory. Her decision theory makes recommendations both about what actions to perform when faced with different decision problems and which picking strategy is best. But these recommendations pull her in different directions. The decision theory itself tells you do do one thing when faced with the decision, but recommends using a picking strategy which does a different thing. 
%The picking strategy it recommends tells her to do one thing when faced with $D^{\mathrm{Allais}}_{\mathrm{local}}$, and the decision theory itself tells her to do another thing. 
This is a conflict in the theory's recommendations. The oddity of the situation is akin to one that David \citet[56]{lewis1971iim} identified in a different context:
\begin{quote}
	It is as if Consumer Bulletin were to advise you that Consumer Reports was a best
	buy whereas Consumer Bulletin itself was not acceptable; you could not possibly
	trust Consumer Bulletin completely thereafter.
\end{quote}

%This is, \emph{prima facie}, a bad-making feature of a decision theory. 
The decision theory is self-undermining in the sense that there is some particular precise probability distribution over the possible decision problems she might face---it's $11\%$ likely she'll face $D^{\mathrm{Allais}}_{\mathrm{local}}$, and $89\%$ likely she'll get \pounds 1m for sure---where if she applies her decision theory to the question of which picking strategy to use, it rules out as impermissible its own recommended course of action. 




This was all based on the assumption not only that her decision theory endorses the Allais preferences, but also that it recommends Risky over Safe in $D^{\mathrm{Allais}}_{\mathrm{local}}$. If it instead recommends Safe over Risky in $D^{\mathrm{Allais}}_{\mathrm{local}}$, we can consider a different decision problem: Suppose that, whilst she still thinks that her credence that she'll be offered $D^{\mathrm{Allais}}_{\mathrm{local}}$ is $11\%$, she thinks that if she is not offered it, then she is given nothing. 
Now, she is facing the following decision problem over which picking strategy to select. 
$$
\begin{array}{c|ccc}
	\multirow{2}{*}{\text{Version 2}}	&\multicolumn{2}{c}{\text{choice offered}}&\text{choice not offered}\\
	& \text{goes bust} &  \text{not bust}  \Bstrut \\\hline \Tstrut	p&  \sfrac{1}{100} &  \sfrac{10}{100} &  \sfrac{89}{100} \Bstrut \\\hline \hline\Tstrut 
	\s_{\mathrm{Safe}} & \text{\pounds 1m}&\text{\pounds 1m}         &\text{\pounds 0}               \\
	\s_{\mathrm{Risky}}& \text{\pounds 0m}&\text{\pounds 5m}& \text{\pounds 0}
\end{array}
$$
This is now extensionally equivalent to the choice of 2A vs 2B in $D^{\mathrm{Allais}}_2$, and we have supposed that she holds the Allais preferences of 2B over 2A. In this scenario, then, she will evaluate the picking strategy $\s_{\mathrm{Risky}}$ to be preferable to $\s_{\mathrm{Safe}}$, even though, when faced with  $D^{\mathrm{Allais}}_{\mathrm{local}}$ herself, she prefers Safe. 

Thus, if her decision theory endorses the Allais preferences and is opinionated over what to choose in $D^{\mathrm{Allais}}_{\mathrm{local}}$ then it is undermining in the sense that there is some uncertainty over which decision problem she'll face where the decision theory rules its own way of picking as impermissible. 

What if, however, her decision theory has neither a strict preference for Safe over Risky or Risky over Safe in $D^{\mathrm{Allais}}_{\mathrm{local}}$. Then both ways of picking are compatible with her decision theory. Whilst which picking strategy she deems optimal changes depending on the amount she'll receive when not facing $D^{\mathrm{Allais}}_{\mathrm{local}}$, in neither case does she rule out all the picking strategies that are compatible with her recommendations---she just rules out one of them. Whilst her decision theory would then be undermining in some sense, it is much weaker than what we had previously, where the decision theory rules out as impermissible \textit{all} picking strategies compatible with its recommendations (there is only one).% \todo{what do we actually say about this?? EAdmiss?? RP: yes, this is covered below. And this is the point of the probabilistic picking strategies.}


%A decision theory often does leave some decisions unsettled. This might be because they are indifferent between various option or it might be because they consider them incomparable. In these cases, they say that our reasons for action do not determine a unique option. In the language of \citep{ullman1977}, such a decision theory leaves open various ways to pick, where picking is a matter of settling on what to do after your reasons for action have run out and a range of options have not been ruled out as impermissible.

However, with some additional assumptions we can modify the case to again show that it is still self-undermining. 
The decision theories that endorse the Allais preferences are usually motivated by avoidance of risk rather than considerations of ambiguity or imprecision. They thus typically say that cases in which an agent doesn't have a strict preference either way between two options are those in which  she is indifferent between them; and, in those cases, any slight sweetening of one option is sufficient to make her strictly prefer  that. %\todo{is this the ``ordering principle''?}
So, in this decision, she will prefer Risky$^+$ to Safe:
$$
\begin{array}{r|cc}
 D^{\mathrm{Allais}}_{\mathrm{local}^+} & \text{goes bust} &  \text{not bust}  \Bstrut\\\hline
	p & \sfrac{1}{11} & \sfrac{10}{11}\Bstrut\\\hline\hline\Tstrut
	\text{Safe} &\text{\pounds 1m}&\text{\pounds 1m}                      \\
	\text{Risky$^+$} & \text{\pounds 1}&\text{\pounds 5m+\pounds 1}
\end{array}
$$ What's more, for some small enough sweetening, it is plausible that she will still prefer 1A to 1B$^+$:\footnote{This is an Archimedeanicity principle.}
$$
\begin{array}{r|cccc}
	& \text{ticket 1} &\text{ticket 2-11}& \text{ticket 12-100}\Bstrut\\\hline
	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{100} \Bstrut\\\hline\hline\Tstrut
	\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} \\
	\text{1B$^+$} & \text{\pounds 1} & \text{\pounds 5m+\pounds1}  & \text{\pounds 1m}
\end{array}
$$
And so again, the decision theory deems it impermissible to pick in accordance with its own recommendations; instead it recommends using a different decision theory. 


Any failure of von Neumann-Morgenstern's Independence Principle will generate a case where the decision theory is self-undermining.
This principle says that if the decision theory deems action $a_1$ preferable to $a_2$ then it deems a probabilistic mixture $\alpha a_1+(1-\alpha)b$ as preferable to $\alpha a_2+(1-\alpha)b$. 
Such mixtures are often interpreted as performing acts depending on the outcome of a randomization device, say a toss of a biased coin. 
But it can similarly be seen as evaluating picking strategies if you're uncertain over which decision problem you're faced with: with probability $\alpha$ you'll be offered the decision of $a_1$ vs $a_2$ and with probability $1-\alpha$ you'll be given a decision just with one option: to perform $b$. The picking strategies are determined by their pick of either $a_1$ or $a_2$, and they are extensionally equivalent to the mixed acts. A failure of the independence axiom thus results in a different evaluation of the picking strategy to the individual actions, i.e., a case of self-underminingness.%
\footnote{
	In the case where we have $a_1\succ a_2$ and $\alpha a_1 +(1-\alpha)b\prec \alpha a_2 + (1-\alpha)b$ this is immediate. 
	If the reversal is merely weak, we can make use of an Ordering and Archimedeanicity principle by assuming it is due to indifference, then making a slight improvement which ensures it is strictly preferred, but with that improvement being small enough that it doesn't break the other strict preference. 
}
%	When we merely have  $a_1\succeq a_2$ and $\alpha a_1 +(1-\alpha)b\prec \alpha a_2 + (1-\alpha)b$ we can make an argument similar to that above, making use of ordering and Archimadeanicity assumptions: when $a_1\succeq a_2$, for small enough price, $\epsilon$, still $\alpha (a_1-\epsilon) +(1-\alpha)b\prec \alpha a_2 + (1-\alpha)b$, but  }

These cases are formally related to cases of sequential incoherence for such decision theories \citep{hammond1988cfeu,machina1989}, but the formal motivation for our investigation was instead \citet{levinstein2017pgeu} who, following \citet{schervish2009psr}, also considers uncertainty over which decision problem is being faced and makes evaluations on that basis. 
The aim of their investigation is to judge and compare credences rather than decision theories.
They hold fixed expected utility theory as the decision theory and instead use this uncertainty over which decision you'll face to evaluate credences by their guidance value, assuming what they guide you to do is in accordance with expected utility theory. 
%is to evaluate credences according to their expected guidance value in each state of the world, which differs from ours by both associated a picking function with credence functions in accordance with expected utility theory and then reduces it using expectations. 



%\todo{the original has been commented out below}
%}
%{\color{red}
	
%\subsection{Original}


%Here are the four gambles over which the Allais preferences are defined---for reasons that will become clear, we present them as if they are defined over four possible states of the world, $\omega_1, \ldots, \omega_4$:
%$$
%\begin{array}{r|cccc}
%	& \omega_1 & \omega_2 & \omega_3 & \omega_4 \Bstrut \\\hline \Tstrut	p & \sfrac{1}{100} & \sfrac{10}{100} & \sfrac{89}{200} & \sfrac{89}{200} \Bstrut \\\hline \hline\Tstrut 
%	\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} & \text{\pounds 1m}  \\
%	\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}& \text{\pounds 1m}  \\
%	\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m}  & \text{\pounds 0m}  \\
%	\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m}  & \text{\pounds 0m}  
%\end{array}
%$$
%And the preferences are these: $\text{1A} \succ \text{1B}$ and $\text{2A} \prec \text{2B}$. 

%Here are the four gambles over which the Allais preferences are defined---for reasons that will become clear, we present them as if they are defined over four possible states of the world, $\omega_1, \ldots, \omega_4$:
%$$
%\begin{array}{r|cccc}
%& \omega_1 & \omega_2 & \omega_3 & \omega_4 \Bstrut \\\hline \Tstrut p & \sfrac{1}{100} & %\sfrac{10}{100} & \sfrac{89}{200} & \sfrac{89}{200} \Bstrut \\\hline \hline\Tstrut 
%\text{1A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 1m} & \text{\pounds 1m}  \\
%\text{1B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 1m}& \text{\pounds 1m}  \\
%\text{2A} & \text{\pounds 1m} & \text{\pounds 1m}  & \text{\pounds 0m}  & \text{\pounds 0m}  \\
%\text{2B} & \text{\pounds 0m} & \text{\pounds 5m}  & \text{\pounds 0m}  & \text{\pounds 0m}  
%\end{array}
%$$
%And the preferences are these: $\text{1A} \succ \text{1B}$ and $\text{2A} \prec \text{2B}$. 

%\todo{Referee: names for these particular ones}
%Now, consider the following two decision problems, $D_1$ and $D_2$, where the acts available in them are defined over two states of the world, $\omega'_1, \omega'_2$:
%$$
%\begin{array}{r|cc}
%D_1 & \omega'_1 & \omega'_2  \Bstrut \\\hline \Tstrut p & \sfrac{1}{11} & \sfrac{10}{11} \Bstrut \\\hline \hline\Tstrut 
%\text{A} & \text{\pounds 1m} & \text{\pounds 1m}   \\
%\text{B} & \text{\pounds 0m} & \text{\pounds 5m} 
%\end{array}\hspace{20mm}
%\begin{array}{r|cc}
%D_2 & \omega'_1 & \omega'_2  \Bstrut \\\hline \Tstrut p & \sfrac{1}{2} & \sfrac{1}{2} \Bstrut \\\hline \hline\Tstrut 
%\text{C} & \text{\pounds 0m} & \text{\pounds 0m} 
%\end{array}
%$$
%Suppose first that, faced with $D_1$, your decision theory tells you to pick A over B, while faced with $D_2$, it tells you to pick the only available option, C. (We'll return to the case in which your decision theory tells you to pick B over A below.)

%Now suppose you are certain you'll face $D_1$ or $D_2$, but you're uncertain which. Your credence you'll face $D_1$ is $\sfrac{11}{100}$ and your credence you'll face $D_2$ is $\sfrac{89}{100}$. And, what's more, you think which decision you face is independent of what the world is like.

%A \emph{picking strategy} is a function $\s$ that takes a decision problem $D$, which is just a set of available acts, and returns one of those available acts $\s(D)$ from $D$. As \cite{ullman1977} emphasize, there is an important difference between \emph{picking} and \emph{choosing}. Choosing is a matter of settling on what to do in a decision problem for reasons grounded in your preferences. Picking on the other hand is a matter of settling on what to do even after your reasons have run out, either because you are indifferent between the options that you have not rejected or because you find them incomparable. Picking strategies fully settle what to do in every decision problem. We will understand picking inclusively in what follows. If you choose, then you pick, but not vice versa. That is, if you settle on an option for reasons grounded in your preferences, then you count as both choosing and picking that option. If you settle on an option not for preference-based reasons but simply because you must \emph{do something}, then you count as picking but not choosing that option.


%Just as we said above we'd judge decision theories by how good they are as means to our ends, let us first do that for picking strategies. So, the utility of a picking strategy in a decision problem is just the utility of the act it tells you to pick in that problem. %is just the utility of the act it requires you to choose. That is, we can specify the utility of $\s_1$ and $\s_2$ if we specify the decision problem you face and the state of the world.  
%To specify the utility of a picking strategy, $\s$, then, we need to specify both the decision problem that you face, $D$, and the state of the world, $\omega$. The strategy selects a unique option from $D$, $\s(D)$, and the world determines the utility of that option, $\s(D)(\omega)$. 

%Now return to our example. There are two picking strategies, $\s_1$ and $\s_2$, available to you when you consider facing either $D_1$ or $D_2$:
%$$
%\begin{array}{r|cc}
%& D_1 & D_2 \Bstrut \\\hline \Tstrut\s_1 & \text{A}& \text{C} \\
%\s_2 & \text{B}& \text{C} \\
%\end{array}
%$$
%If you face $D_1$ and the world is in state $\omega'_1$, then the utility of $\s_1$ is the utility of \pounds 1m. The reason: $\s_1$ picks A from $D_1$ and A gets you \pounds 1m at $\omega'_1$. More generally, the utility of $\s_1$ and $\s_2$ in any decision problem and world is given by the utility of the following outcomes:
%$$
%\begin{array}{r|cccc}
%& \omega'_1\ \&\ D_1 &  \omega'_2\ \&\ D_1 &  \omega'_1\ \&\ D_2 &  \omega'_2\ \&\ D_2 \Bstrut \\\hline \Tstrut P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{1}{2}\times\sfrac{89}{100} & \sfrac{1}{2}\times\sfrac{89}{100} \\
%& = \sfrac{1}{100} & = \sfrac{10}{100} & = \sfrac{89}{200} & = \sfrac{89}{200} \Bstrut \\\hline \hline\Tstrut 
%\s_1 & \text{\pounds 1m} & \text{\pounds 1m} & \text{\pounds 0m} & \text{\pounds 0m} \\
%\s_2 & \text{\pounds 0m} & \text{\pounds 5m} & \text{\pounds 0m} & \text{\pounds 0m} \\
%\end{array}
%$$
%But notice: the choice between $\s_1$ and $\s_2$ is extensionally equivalent to the choice between 2A and 2B in the Allais set up---once we equate $\omega_1$ with $\omega'_1\ \&\ D_1$, $\omega_2$ with $\omega'_2\ \&\ D_1$, and so on, it has the same pay-offs with the same probabilities. So, if you have the Allais preferences, and hence prefer 2B to 2A, then you prefer $\s_2$ to $\s_1$. But we specified that, when faced with $D_1$, your decision theory demands that you choose A over B. That is, it demands that you choose (and hence pick) what $\s_1$ picks and not what $\s_2$ picks. So, your decision theory makes two claims: First, if you're uncertain whether you'll face $D_1$ or $D_2$, you prefer to choose according to picking strategy $\s_2$ and consider it irrational to use $\s_1$. Second, when actually faced with $D_1$, you should pick as $\s_1$ does. In this sense, then, your decision theory is self-undermining. It judges the only picking strategy compatible with its demands to be {rationally impermissible}. Uncertain which of two decision problems you will face, it says that, when you face whichever you do, you should not choose as it demands.

%Now, we specified above that, faced with $D_1$, your decision theory demands you choose A over B. But what if it demands you choose B over A? In that case, we just replace $D_2$, the trivial decision problem that contains only C, which pays out \pounds 0m in either state of the world, with the trivial decision problem that contains only the act D, which pays out \pounds 1m in either state of the world. Then the choice between $\s_1$ and $\s_2$ is extensionally equivalent to the choice between 1A and 1B. So, by the Allais preferences, you'll prefer $\s_1$ to $\s_2$, but, faced with $D_1$, you'll prefer B to A, which is what $\s_2$ tells you to pick and what $\s_1$ tells you not to pick. So again the decision theory is self-undermining.

%And there is no funny business going on here. Learning that you face decision problem $D_1$ does not tell you anything about which ticket will win, for we assumed that the decision problem is independent of the state of the world. And picking one option or another in $D_1$ does not tell you anything about which state of the world you're in, for we assumed the acts are independent of the states of the world as well.

%So, the picking strategy your decision theory judges to be optimal guides you to choose in one way when faced with $D_1$, even though, when actually faced with decision $D_1$, your decision theory guides you to choose the other way. 

%Being self-undermining is \emph{prima facie} bad. Your decision theory makes recommendations both about \emph{picking strategies} and which \emph{options} to choose in different decision problems. But these recommendations pull you in different directions. The picking strategy it recommends tells you to do one thing when faced with $D_1$, and the decision theory itself tells you to do another thing. So there's an instability in the theory's recommendations. The oddity of the situation is akin to one that David \citet[56]{lewis1971iim} identified in a different context:
%\begin{quote}
%It is as if Consumer Bulletin were to advise you that Consumer Reports was a bestbuy whereas Consumer Bulletin itself was not acceptable; you could not possibly trust Consumer Bulletin completely thereafter.
%\end{quote}

%Any theory that deems the Allais preferences permissible has this \emph{prima facie} bad-making feature. It is self-undermining in the following sense: (1) there is some precise state of uncertainty you could have about which decision problem you'll face---uncertainty that is represented by a precise probability distribution over the possible decision problems; (2) if you are in that state and apply your decision theory to the question of which picking strategy to use, it rules out any compatible strategy, \emph{i.e.}, any strategy that always avoids picking options that are impermissible according to the original decision theory. %always picks options that are choiceworthy according to the original decision theory. 
%It demands that you use some picking strategy that is not compatible with it. 


%This isn't unique to the Allais preferences. Other failures of the Savage's Sure Thing Principle would generate such a way of being uncertain over which decision problem you'll face where the decision theory undermines its recommendations, at least when the failure generates a preference reversal. Similarly for von Neumann-Morgenstern's Independence Principle: if your decision theory deems action $a$ to be preferable to action $b$, but when evaluating an action which tosses a biased coin and selects $a$ (or $b$) if the coin lands Heads and $c$ if the coin lands Tails, the decision theory evaluates the $b$ version to be preferable, then similarly we can generate a way of being uncertain over which decision problem you'll be faced with so that the unique picking strategy which is compatible with the decision theory is itself impermissible according to the theory. 

%such that, if you have that uncertainty, and you then apply your decision theory to the question of which picking to use when you face whichever decision problem you in fact face, it says that none of the picking strategies compatible with it are rationally permissible; it demands you use some alternative picking strategy that is not compatible with it. 


%We will be able to generate such a challenge using any failure of von Neumann-Morgenstern independence principle or Savage's sure thing principle. 
%If your decision theory deems action $a$ to be preferable to $b$ but a certain probabilistic mixture of $b$ with $c$ to be preferable to the same mixture of $a$ with $c$, then supposing you're uncertain if you'll be faced with $D_1$ a choice between $a$ and $b$ and $D_2$ the trivial choice of $c$, with your credences matching the independence violation, then we have a case where the unique picking strategy compatible with the decision theory is deemed impermissible by the decision theory. 
%If your decision theory deems action $a$ preferable to $b$ but when there's only a certain probability $p$ that you get $a$ or $b$, and otherwise will get $c$, it deems the mixture with $b$ to be preferable to the mixture with $a$. This 


%
%\begin{quote}
%	\textbf{Independence } Suppose that, when faced with the decision problem $\{a, b\}$, in which $a$ and $b$ are the two available options, your decision theory declares only $a$ permissible. Then, for $0 < p < 1$, when faced with the decision problem $\{[p:a,(1-p):c], [p:a,(1-p):c]\}$, your decision theory should declare only $[p:a,(1-p):c]$ permissible.
%\end{quote}

%}

\subsection{Is it a problem to be self-undermining?}\label{sect:reu:phil-discussion}

There are at least two ways to argue that being self-undermining tells against a decision theory. We'll describe them and then consider some responses.

%{\color{cyan}
%	First: A decision theory categorises available acts into those that are rationally permissible for a particular individual and those that are rationally impermissible for them. Now suppose the decision theory is correct: that is, the acts it declares permissible are indeed exactly those that are permissible for the individual. But one of its declarations is that using it is itself impermissible---that's what makes it self-undermining. Now, how could it ever be rationally impermissible to use a decision theory that gives the correct verdicts about rational permissibility? Surely it couldn't! And if that's so, then the decision theory cannot be correct, and we have a contradiction.
%}
%
%{\color{violet}
%	First: Suppose a decision theory is correct: that is, the acts it declares impermissible are indeed exactly those that are in fact impermissible for the individual. If the decision theory is undermining, then one of its declarations is that picking strategies that are compatible with itself are itself impermissible.
%	Now, how could it ever be rationally impermissible to use a decision theory that gives the correct verdicts about rationality? Surely it couldn't! 
%	And if that's so, then the decision theory cannot be correct, and we have a contradiction.
%}

%{\color{violet}
%	First: if we suppose that a decision theory is in fact the true theory of rational decision making, i.e., that its judgements are the correct ones for that individual. But if one of its declarations is that it is impermissible to pick in any way that is compatible with its recommendations, then any way of acting in accordance with the decision theory is deemed impermissible by the decision theory. Having supposed that the decision theory is the true theory of rational decision making, it is thus impermissible. 
%	  then it is in fact impermissible to act in accordance with the given decision theory. But then the supposed decision theory simply cannot give the correct judgements. 
%	\todooldinfo{rewrite in imperssible language}
%}


%\todo{Maybe worth seeing if we can reword. RP: I actually liked this when I re-read it.}
%\begin{colored}{violet}
\begin{colored}{red}
	\todoinfo{I'm unhappy with this section. I'm not sure my rewriting is correct... }
	
First: Could a self-undermining decision theory be correct? 
That is, could its verdicts match with what is actually impermissible for the agent. 
If it is self-undermining, then one of its verdicts is that some alternative picking strategy is better than its own. 
And thus it is in fact impermissible to pick in accordance with it. 

This is assuming that that alternative picking strategy is available to the agent. 
If the alternative picking strategy is so complex and computationally demanding that it lies beyond the agent's cognitive capacities to use reliably, then this is no bad mark against the self-undermining one. 
Similarly, if in \emph{attempting to use} a given decision theory, it may misfire and result in even worse decisions, it may be permissible, or even required, to adopt the simpler decision theory. 
However, we are assuming that we are not in this sort of case, and that it is within the agent's control to choose any picking strategy.

\end{colored}


\begin{colored}{violet}
	\todoinfo{RP: suggested rewrite }
	
First: Suppose a self-undermining decision theory is correct. That is, it deems impermissible exactly those options that are indeed impermissible. But, since it is self-undermining, it deems impermissible the option of picking in accordance with itself. And so it is indeed impermissible to pick in accordance with it, since it is correct. But surely it cannot be impermissible to pick in accordance with the true theory of rational choice. Therefore, the decision theory cannot be correct, which gives a reductio.

You might think that it could be impermissible to pick in accordance with the correct theory of rational choice, if for instance it is very costly to do so and there is a low cost alternative available that reasonably approximates the true theory, or if your attempt to pick in this way is likely to misfire, or if it is simply infeasible and so impossible for you to do it. But, when a theory is self-undermining, it is not for any of those reasons that it deems itself impermissible: it says it is impermissible even if it is cost-free to use it, and your attempt to use it always goes perfectly.

%Could a self-undermining decision theory be correct? 
%That is, could its verdicts match with what is actually impermissible for the agent. 
%If it is self-undermining, then one of its verdicts is that some alternative picking strategy is better than its own. 
%And thus it is in fact impermissible to pick in accordance with it. 

%This is assuming that that alternative picking strategy is available to the agent. 
%If the alternative picking strategy is so complex and computationally demanding that it lies beyond the agent's cognitive capacities to use reliably, then this is no bad mark against the self-undermining one. 
%Similarly, if in \emph{attempting to use} a given decision theory, it may misfire and result in even worse decisions, it may be permissible, or even required, to adopt the simpler decision theory. 
%However, we are assuming that we are not in this sort of case, and that it is within the agent's control to choose any picking strategy.

\end{colored}



%
%A decision theory for which it is not correct, where it  for you to pick in accordance with might nonetheless be the best one to adopt if, for example, the alternative decision theories which give the correct verdicts are all so complex and computationally demanding that they lie beyond the agent's cognitive capacities to use reliably---not much good trying to use something that gives the right verdicts every time if you can never discover what verdicts it gives. But we have 
%
%\todo{Let's go back and double check these two paragraphs}A decision theory for which it is impermissible for you to pick in accordance with might nonetheless be a good one for you to \emph{adopt} if, for example, the alternative decision theories which give the correct verdicts are all so complex and computationally demanding that they lie beyond the agent's cognitive capacities to use reliably---not much good trying to use something that gives the right verdicts every time if you can never discover what verdicts it gives. And in attempting to use such a theory, you might be led further astray then adopting a simpler theory at the outset even if it does not actually correct. 
%%In attempting to follow its recommendations, it might lead you further astray than if you'd started with a simpler theory. 
%%But that is not the case we are considering here. It just needed to specify what to do when faced with $D^{\mathrm{Allais}}_{\mathrm{local}}$, certainly a theory that can be followed. 
%We have assumed that all possible picking strategies are available to the agent. We have assumed that it is within the agent's control to choose a picking strategy which is correct and never picks impermissible options. This is not beyond the agent's ken. And she can perfectly reliably implement this strategy. That is why we take the outcome of a picking strategy to be the outcome of whichever act it recommends.
%%\end{colored}
%
%\begin{colored}{red}
%{First: for any decision problem (set of available acts), $D$, a decision theory deems some subset of $D$ to be \emph{rejected} or \emph{rationally impermissible}. Now suppose the decision theory is correct: that is, the acts it declares impermissible are indeed impermissible for the individual. But one of its declarations is that picking strategies that never recommend impermissible acts are themselves impermissible---that's what makes it self-undermining. In that case, you might face a rational dilemma. That is, nature might thrust a sequence of decision problems on you where you rationally \emph{must} do something rationally impermissible. Suppose you face the choice of which picking strategy to use. If you choose a rationally impermissible picking strategy, you (obviously) do something impermissible in that decision problem. If you choose some other picking strategy, then there is some other decision problem where it recommends an impermissible act. If nature thrusts this decision problem on you, then implementing your picking strategy will lead you to pick an impermissible act. So you will have done something impermissible in this latter decision problem. This is a rational dilemma. And it is a particularly pernicious sort of rational dilemma, since it is not generated by any self-referentiality or act-state dependence. If there are no genuine rational dilemmas of this sort, then our initial assumption must be false: the decision theory simply cannot be correct.}
%
%Now, perhaps you might reply that it could be impermissible to use a decision theory that gives the correct verdicts if that decision theory were so complex and computationally demanding that it lay beyond the agent's cognitive capacities to use it reliably---not much good using something that gives the right verdicts every time if you can never discover what verdicts it gives. But that is not the case we are considering here. Indeed, we have assumed explicitly that, if I use a decision theory when I face a particular decision problem, I pick one of the acts it deems rationally permissible---that's why the outcome of a picking strategy is taken to be the outcome of whichever act it demands you choose.
%\end{colored}
%{\color{violet} Now, you might argue that the right decision theory to use is still one which results in }
%Now, it is possible that a good theory may not be the correct one. 
%Now, perhaps we shouldn't be looking for a correct theory, as perhaps they have some other flaws. For example, 
%{Now, you might (rightly) point out that correct theories might be flawed in some other way, and as a result, it may be no decisive mark against a self-undermining theory that it is not correct. For example,} 
%Perhaps this is not a decisive mark against a self-undermining theory. It could be impermissible to use any decision theory that gives the correct verdicts if, for example, those theories were so complex and computationally demanding that they lay beyond the agent's cognitive capacities to use reliably---not much good using something that gives the right verdicts every time if you can never discover what verdicts it gives. But that is not the case we are considering here. {Indeed, we have assumed that all possible picking strategies are available to the agent. So, in particular, it is within the agent's control to choose a picking strategy that never recommends impermissible acts. This is not beyond the agent's ken. And she can perfectly reliably implement this strategy. That is why we take the outcome of a picking strategy to be the outcome of whichever act it recommends.}
%\end{colored}


%{\todo{this needs fixing for above. But haven't we said it all above anyway? What's this section adding? RP: I'd cut the paragraph higher up where we say there's nothing funny going on. At that point, we're just introducing the idea and making the case it's odd prima facie. I think we leave the spelling out of how it's bad for here as well as deflecting the potential responses. C: yes, but I'm not sure exactly how to do that.}
Second: Relatedly, for someone who uses the decision theory, it gives contradictory advice at different points. Recall the case above in which you were initially uncertain whether you'd face $D^\mathrm{Allais}_\mathrm{local}$ or get \pounds 1m for sure, and your decision theory would choose Risky over Safe when faced with $D^\mathrm{Allais}_\mathrm{local}$ but would choose the picking strategy $\s_\mathrm{Safe}$ over $\s_\mathrm{Risky}$. Standing facing the decision problem $D^\mathrm{Allais}_\mathrm{local}$, you might ask yourself: my decision theory tells me to choose Safe and not to choose Risky, but it also tells me that, if I could have chosen how I would choose, it would have told me to choose Risky and not to choose Safe---which should I do?

If we again suppose that the self-undermining decision theory is correct, it leads to a rational dilemma, where one is rationally required to adopt a picking strategy which picks in an rationally impermissible way. 

%{\todo{this needs fixing for above. But haven't we said it all above anyway? What's this section adding? RP: I'd cut the paragraph higher up where we say there's nothing funny going on. At that point, we're just introducing the idea and making the case it's odd prima facie. I think we leave the spelling out of how it's bad for here as well as deflecting the potential responses. C: yes, but I'm not sure exactly how to do that.}
%	Second: Relatedly, for someone who uses the decision theory, it gives contradictory advice at different points.
%	Recall the case above in which you were initially uncertain whether you'd face $D^{\mathrm{Allais}}_{\mathrm{local}}$ or get a sure amount of money. Your decision theory then recommended, say, the picking strategy which selects Safe. But yet when you face the decision problem itself, it recommends Risky. Which of these perspectives is privileged? Standing facing the decision problem, you might ask yourself: my decision theory tells me to choose Safe rather than Risky, but it also tells me that, if I could have chosen how I would choose, it would have told me to choose B and not to choose A---which should I do?

%It's worth adding that, for those decision theories that permit the Allais preferences and also satisfy an Archimedeanicity postulate, they consider it rationally required of you, at the point when you're uncertain which decision problem you'll face, to pay money if it's possible to thereby to bind yourself to doing something other than what they will recommend when you actually face that decision. %\todo{C: we need to add Arch somewhere above!}.

These self-undermining decision theories would prefer to bind themselves to do something other than what they recommend when you actually face that decision. \todo{this paragraph was above ``second'', which didn't seem to make sense. I've now put it next to the act-state dep}
Of course, we're used to that in cases of temptation---Ulysses should pay his sailors to bind him to the mast as their ship passes the Sirens---but that's because we think your utilities will change under the pressure of temptation or your probabilities will change in an irrational way or you won't choose rationally on the basis of your utilities and probabilities. Nothing like that is going on here.

We're also used to that in cases of act-state dependence. Causal decision theorists say that in Newcomb's problem you should take both boxes, but if you can pay to take a pill to turn yourself into a one-boxer before the prediction is made, then you should. But that's because choosing how to choose, in this case, causes you to face a better decision problem down the line. Nothing like that is going on here. And perhaps we think that, at least when choosing a picking strategy has no causal or evidential impact on which decision problems you will face or the value of the options in those problems, the correct theory of rational choice should not give rise to such dilemmas.


%In this section, we consider some responses to the objection we've outlined above. What might the defender of a risk-sensitive decision theory that permits Allais preferences say in response to the charge that their theory is self-undermining in the way we've described?

%\todooldinfo{Todo: Plan is for RP to make progress here.\\ Perhaps to base it on sec 1.2 of v14b, but I actually think I guess there are three things to discuss: (a) it's supposed to be universal (b) connection to diachronic etc. Sec 1.2 of v14b does a lot about it judging itself, which looks more like we need to do more discussion of strategy vs judging itself, which I think perhaps we should not get into the weeds of yet...? }

\subsection{Responses}

\subsubsection{Limit the decision theory's scope}

	Perhaps the defender of a self-undermining decision theory will say it was never their intention that their theory should be used for these higher-order decisions. They might say they are offering a theory of first-order rational choice; not a fully general theory that covers any sort of decision, including these higher-order decisions between picking strategies. 
	
But that can't be right, for these theories are presented by their proponents as universal decision theories---they are intended to cover any sort of decision, providing we can determine credences and utilities and any other parameters that must be fixed. In their descriptions, it is not specified that they are to be applied only to a certain sort of decision, such as decisions between first-order actions. They are intended to be what above we called structural: that is, their recommendations don't depend on the content of the outcomes, but only on the list of credences of various utility outcomes. And, as we saw in our treatment of the Allais decisions above, for any higher-order decision between picking strategies under uncertainty about the decision problem you'll face, there is an extensionally equivalent decision between ordinary first-order actions. If the defender of a self-undermining decision theory were to respond in the way outlined, they'd have to deny that their decision theory is structural in this sense and they'd have to say why that is the case.
	
	

%Perhaps the defender of a self-undermining decision theory will say it was never their intention that their theory should be used for these higher-order decisions. They are offering a theory of first-order rational choice; not a fully general theory that covers any sort of decision, including these higher-order decisions between picking strategies. 
%But that can't be right, for these theories are presented by their proponents as universal decision theories. In their descriptions, it is not specified that they are to be applied only in specific cases. 
%%These arguments are truly formal in the sense that the content of the decisions in question is not relevant to our evaluation of their plausibility. If the theory is not supposed to apply to higher-order decisions as well as first-order decisions, the defender of the theory must explain why the axioms are true when they govern preferences between first-order acts, but not when they govern preferences between higher-order acts like picking strategies.
%\end{colored}


%\todooldinfo{Todo: select part of the buchak quote, from below. Or perhaps just don't bother.//!}
%{\color{cyan}
%	
%	\begin{quote}
%		an individual might be more risk-
%		avoidant when it comes to his health than when it comes to money.  \ldots So, says the criticism, an individual’s r-function should be
%		indexed to particular domains or stakes, and there need not be any specific relation-
%		ship between rhealth and rmoney, or between rlow stakes and rhigh stakes
%		.
%		
%		\ldots
%		
%		 in the end this debate should not take place at the level of which preferences seem
%		reasonable anyway: it should proceed via a discussion of whether the axioms that
%		underlie REU theory are rational requirements. 
%		
%		\ldots 
%		Finally, I might respond to the objector who wants to allow different
%		risk-attitudes in different domains by pointing out that some gambles will be hybrid
%		gambles, that might result in a health outcome but might result in a monetary out-
%		come—for example, the purchase of medical insurance—and the objector will have to
%		say how the agent values these.
%		
%		(\citet[p79-80]{buchak2014rr})
%	\end{quote}
%	
%	More content:
%	\begin{quote}
%		One might agree that ruling out risk
%		avoidance/inclination is the only thing EU theory gets wrong, but think that I have
%		overlooked an important fact: namely that rational individuals can have different
%		risk functions in different domains. For example, an individual might be more risk-
%		avoidant when it comes to his health than when it comes to money. Or he might have
%		different risk functions when the stakes are different: he might be more risk-avoidant
%		the higher the stakes are. So, says the criticism, an individual’s r-function should be
%		indexed to particular domains or stakes, and there need not be any specific relation-
%		ship between rhealth and rmoney, or between rlow stakes and rhigh stakes
%		. That we accept different
%		levels of risk in different situations is born out by empirical evidence: a study by
%		Rottenstreich and Hsee (2001) found that the shape of the weighting function in pros-
%		pect theory depends on the kind of decision being made. For example, even though
%		individuals preferred \$50 to a kiss with their favorite movie star, they tended to be
%		willing to pay more for a chance of the kiss than for an equivalent chance of \$50:
%		they were more risk-seeking with respect to the kiss. More generally, the weighting
%		function deviated more from linearity when the possible outcomes were more emo-
%		tionally laden.
%		
%		I do not have space to explore this possibility in detail, but there are a few ways
%		to respond to these suggestions\ldots I do grant that I am ruling
%		out certain preferences as irrational. For example, an REU maximizer cannot value a
%		gamble below its EU in large stakes and value a gamble above its EU in small stakes, if
%		the probabilities of the analogous outcomes are the same. So the second thing to say is
%		that in the end this debate should not take place at the level of which preferences seem
%		reasonable anyway: it should proceed via a discussion of whether the axioms that
%		underlie REU theory are rational requirements. I will present these in the next chapter,
%		but it is worth mentioning that the axiom that the proponent of this objection is likely
%		to challenge is Axiom (A1), the Ordering Axiom, which entails that all gambles are
%		comparable. If one thinks that risk-attitudes may be different in different domains,
%		one might be drawn to thinking that we cannot compare outcomes that fall within dif-
%		ferent domains; otherwise, it would seem odd to have different preference norms in
%		different domains. Finally, I might respond to the objector who wants to allow different
%		risk-attitudes in different domains by pointing out that some gambles will be hybrid
%		gambles, that might result in a health outcome but might result in a monetary out-
%		come—for example, the purchase of medical insurance—and the objector will have to
%		say how the agent values these.
%		(\citet[p79-80]{buchak2014rr})
%	\end{quote}
%}

%No! It's supposed to be universal!

%Their theories are supposed to be universal. Especially Buchak talks about this. So we should apply REU equally to the choice of how to choose as to the choice of what to do. But then it says something else is better. So we shouldn't apply REU universally? Or what? How to think about this? 

%\subsubsection{What does REU guide you to do?}




%How different is it to other cases of REU seeing that optimal strategies are different to optimal actions. And diachronic stuff, or sequential stuff etc?? 

%What should one actually do? Conflict!

%It deems itself rationally impermissible. That just, on the face of it, seems very bad!
%\begin{colored}{violet}
	
%\subsubsection{What is being judged}

%What we've used the decision theory to evaluate is picking functions. We then associated picking functions with the decision theory to draw the conclusion that the decision theory is self-undermining. 

%However, decision theories are only able to be associated with picking functions depending on further parameters -- 

%Decision theories typically specify certain parameters -- credences, utilities, and perhaps a risk-profile -- to determine a so-called choice function, which specifies for each decision problem, which actions from it are deemed impermissible. Once we have a choice function, we can specify picking functions that are compatible with it by just checking whether it picks an impermissible action.

%What we have then shown is that choice functions that have certain structure are undermining. 
%For example, endorse the Allais preferences and either are opinionated about the local decision, or a slight sweetening is sufficient to induce such a strict preference and that this is a small enough sweetening to leave the overall Allais preferences unaffected. 

%We have then said that this shows that the decision theory recommends using a different decision theory. But that is only the right diagnosis if we hold fixed the parameters of credences, utilities and risk-profile, assuming that they don't change once she knows which decision problem she'll be faced with. But alternatively one could hold fixed one's decision theory and make conclusions about the other parameters. 

%We can hold fixed Expected Utility Theory and use similar analysis to make judgements about credences. This is essentially done in the approach of Schervish and Levinstein, and one's credences must be probabilistic. 
%We might alternatively think that her credences can vary depending on which decision problem she is faced with, treating the decision and state as dependent. This is Brown's argument for updating credences by conditionalization, holding fixed Expected Utility Theory. 
%Good's Value of Information theorem holds fixed both Expected Utility Theory and conditionalization but considers whether the agent wants to gather or avoid the free evidence. 

%Similar analysis could be done by holding fixed a different decision theory, say Buchak's risk-weighted expected utility theory. 
%Showing that the agent sometimes wants to avoid the free evidence, by holding fixed the decision theory and updating by conditionalization \citep{buchak2017trbvi}.
%One might also consider holding fixed that she'll take the evidence but consider what credal update plan is such that the plan judged as optimal will lead to her making decisions in accordance with REU (holding fixed her risk function and utility function).\footnote{See CCM\&BS??}

%What this shows is that the target of the criticism of self-undermining choice functions depends on what parameters are held fixed.\todo{doesn't this all then underminine our conclusion that the decision theory is bad??!! RP: I don't think so. It could, of course, be taken to show that the credences or the way of updating is wrong, but we've plenty of other reasons for thinking the credences and the way of updating them is fine. You could certainly escape all this by introducing constraints on credences and alternative updating rules, but that would be very radical. }

%The same kind of argument has been used which instead hold fixed that she makes decisions in accordance with Expected Utility Theory and instead uses these results to judge her credences (Schervish/Levinstein). We could allow her credences to vary depending which decision she is faced with, treating the decision and state as dependent and then use this as an argumment evaluating credal update plans. This is essentially Brown's argument for updating credences by conditionalization by holding fixed EUT (see some considerations around it for REUT in REF).
%Someone WHO?? is considering varying utilities.
% Good's Value of Evidence cases instead hold fixed that she'll update by conditionalization and make a decision in accordance with expected utility theory, but judges whether she wants to gather the evidence or avoid it. 
% 
 


%\end{colored}


%\begin{colored}{gray}

%\subsubsection{Change the way of updating credences}
%	\todoinfo{CCM trying to condense...}
%	 \todo{v1 !!!}

	
%	We have assumed that your uncertainty about which decision problem you're faced with is probabilistically independent of he first-order state of the world, which is relevant for determining the payoffs of the actions in the problem. 
%	From this assumption, we concluded that when faced with the particular decision problem, say $D^*$, you'll use those same first-order credences to make your decision. 
%	This is justified on the basis of assuming that the agent updates by Bayesian conditionalisation, although the same feature arises from various other update methods such as Dempster-Shafer theory \cite[pp. 133-5]{weisberg2015uui}. It is however, an assumption that can be rejected. 
%	
%	\todo{some ref to Bernhard?}In such an undermining case, we have a situation where what the decision theory judges as the best strategy is different to what it recommends when you know which decision problem you'll be faced with. The option it recommends once faced with a particular decision problem, $D^*$ is different. Say that the overall strategy judged as optimal by the decision theory is $\s_{\text{Risky}}$ but that in $D^*$, the decision theory recommends the Safe option. 
%	This can be rationalised by the decision theory by instead saying that when she learns she's faced with this decision problem, she does not update by Bayesian conditionalisation but instead updates to some credence function which, using the specified decision theory, evaluates Safe to be preferable; for example she might become certain that the company will go bust, thus rationalising picking Safe. 
%	
%	One might question the rationale behind such an update method, but several arguments for Bayesian conditionalization rely on the assumption of expected utility theory \citep[sec7]{campbellmoore2020arae}. We might consider justifying a credal update method by its fruits in terms of guidance in decision making. One can argue for Bayesian conditionalization as it is the update method which you evaluate will lead to the highest utility if one updates in accordance with it and then chooses what to do in accordance with expected utility theory. This is essentially Brown's (\citeyear{brown1976ceu}) argument for conditionalization, which makes use of Good's (\citeyear{good1967pte}) Value of Information theorem. 
%	But if we replace the assumption of expected utility theory with alternative decision theories, say \citet{buchak2010irereg}, and apply the same argument, what the failures of the Value of Information theorem show, or the examples we have of self-undermining decision theories, is that  the theory evaluates a different decision theory or picking strategy to be adopted, or that it results in a different update method from Bayesian conditionalization. 
%	
%	Just like the evaluation of the decision theory itself, what we get, however, is a difference between a ``local'' evaluation and a ``global'' evaluation \citep{campbellmoore2022aurs}. The decision theory may judge that the best strategy for updating her credences is one which results in avoidance of the self-undermining phenomena, once she has actually learned her evidence, some alternative way of updating may look better, perhaps even conditionalization. 
%	As soon as we think she should update in accordance with this local evaluation which differs from the global one, then we are again in the situation of self-undermining decision theory. \todo{what do i mean}
%	
%	-----
%	\emph{delete below}
%	In this argument, we have essentially assumed Bayesian conditionalisation. 
%	We assumed that your probability over the higher-order states makes which decision you'll face probabilistically independent of the first-order state of the world. And we assumed that whichever decision problem you face, you will use those same first-order probabilities to make your decision. This assumption that updating on probabilistically independent information does not change your state of uncertainty is justified by Bayesian conditionalisation, as well as holding in various other update methods such as Dempster-Shafer theory \cite[pp. 133-5]{weisberg2015uui}. It is however, an assumption that can be rejected. 
%	
%	
%	A decision theory which evaluates some other picking strategy to be better than its own recommendations means that if you were to learn which decision problem you'll be faced with and then change your first order credences before deciding in accordance with the specified theory, y
%		
%	
%	Several standard arguments for conditionalization assume that rational agents maximise expected utility, rather than risk-weighted expected utility \citep[sec7]{campbellmoore2020arae}. When considering alternative decision theories, we might instead evaluate what the decision theory itself evaluates as the optimal credal update strategy. This could either be based on evaluating which update strategy is evaluated as leading to the best decisions (\citet{brown1976ceu}, making use of \citet{good1967pte} Value of Information), or by evaluating which is judged as leading to the most epistemic utility \citep{greaves2006jcc,campbellmoore2022aurs}. 
	
	


%}


%We argued that self-undermining decision theories result in a dilemma, offering conflicting advice about what to do when faced with a given decision, should you consult the optimal picking plan and follow its recommendations? Or should you apply the decision theory now you're facing the particular decision problem.

%\end{colored}



\begin{colored}
	{violet}
	\subsubsection{Change the way of updating credences}

There is another way the defender of these theories could argue against the charge of self-underminingness. They can note first that, by assuming decisions are probabilistically independent of states of the world, and assuming our credences in states of the world don't change when we learn which decision we face, we have essentially assumed Bayesian conditionalization. And then they can argue against conditionalization. As pointed out in \citet[16]{campbellmoore2020arae}, several standard arguments for conditionalization assume that rational agents maximise expected utility.

The picking strategy which the decision theory deems optimal is compatible with that very decision theory when we allow her credences to change when she learns which decision problem she faces. Instead of holding fixed her credences (by assuming probabilistic independence and conditionalization), the defender of the decision theory might instead argue for an alternative credal update strategy: the one which results in the optimal picking strategy when coupled with risk-weighted expected utility theory \citep[see also][]{campbellmoore2022aurs,brown1976ceu}.

Whilst we acknowledge this is a possible way out of the criticism, it is a significant bullet to bite. 


\end{colored}

\begin{colored}{red}
%\todoinfo{ORIGINAL}
%Another way the defender of these theories could argue against our objection against self-undermining theories is to argue that we have presupposed Bayesian conditionalization, and then argue against that norm. 

%We have assumed conditionalization because, when evaluating the strategies, we have constructed a probability over the higher-order states of the world---which specify both the first-order state of the world and the decision problem you'll face---by taking probabilities for the first-order states, probabilities for the decision problems, and assuming the decision you face is independent of the first-order state of the world. To see this, consider the relationship between $D^\mathrm{Allais}_\mathrm{local}$ and $D^\mathrm{Allais}_1$ and $D^\mathrm{Allais}_2$ above.
%We then assumed that once you face whichever decision problem, you will use those same first-order probabilities to make your decision. 
%This step has made use of Bayesian conditionalization. 
%Perhaps, if you update your credences in some alternative way, what the decision theory recommends in each decision theory might correspond exactly with the optimal strategy.


%{ It is worth noting that many theories insist that updating on irrelevant information does not change your state of uncertainty, not just traditional Bayesianism. For example, Dempster-Shafer theory models evidential irrelevance and updating in a much different way from the Bayesian one (using Dempster’s Rule of Combination). Nonetheless, Dempster-Shafer theory also makes this prediction \cite[pp. 133-5]{weisberg2015uui}.}

%{ Firstly, we do not assume anything so strong. We do indeed assume that propositions about which decision you face are both causally and evidentially irrelevant to the first-order state of the world. We also assume that if you update on some bit of information that is irrelevant to the state of the world, then your views about the choiceworthiness of actions whose outcomes only depend on the first-order state of the world will not change. Bayesians model evidential dependence as probabilistic independence. And updating by conditionalizing has the feature in question: conditioning on some bit of information that is probabilistically independent of the state of the world does not change your probabilities over worlds; and assuming that your views about the choiceworthiness of actions supervene on the relevant probabilities and utilities (which we hold fixed), those views will not change either. But many other models of uncertainty, updating and choice have this property as well. For example, Dempster-Shafer theory models evidential independence and updating in a different way (using Dempster’s Rule of Combination), but nonetheless behaves in the way described above \cite[pp. 133-5]{weisberg2015uui}.}



%{ But put that to the side. As pointed out in \citet[sec7]{campbellmoore2020arae}, several standard arguments for conditionalization assume that rational agents maximise expected utility, rather than risk-weighted expected utility. Perhaps the argument we have made here can be turned into an argument against Bayesian conditionalization.}


%Whilst we don't know of any defenders of these theories taking this line of argument, as pointed out in \citet[sec7]{campbellmoore2020arae}, several standard arguments for conditionalization assume that rational agents maximise expected rather than risk-weighted expected utility. And in fact, the argument we have made here can be turned into an argument against Bayesian conditionalization. 

%We could judge a credal update method by the utility of the action it requires you to select (using one's preferred decision theory) in each decision problem. The credal update method which you evaluate to be optimal will exactly be one where picking in accordance with the decision theory using the updated credences is just the picking strategy that the decision theory evaluates as optimal. The example we have discussed implies that it won't always be conditionalization, because making a decision with the first-order probabilities is not evaluated as the optimal picking strategy, so the update cannot return the original first-order probabilities despite the higher-order probabilities encoding the assumption that they are independent. 

%This approach is closely related to Brown's (\citeyear{brown1976ceu}) argument for conditionalization as an update rule, which makes use of Good's (\citeyear{good1967pte}) Value of Information theorem. But the Value of Information Theorem fails for these decision theories \citep{wakker1988neuai,buchak2010irereg}: sometimes you think you'll make better decisions if you ignore the evidence (or don't update your credences). 
%The epistemic variant of this argument is from \citet{greaves2006jcc}, and \citet{campbellmoore2022aurs} consider how it applies to agents who use Buchak's (\citeyear{buchak2014rr}) risk-weighted decision theory. 

%However, as \citet{campbellmoore2022aurs} stress, whilst this might provide an argument that the optimal update strategy is this particular way, this does not mean that the risk-sensitive agent is rationally required to adopt such an update procedure, unless one has a principle linking previous evaluations of strategies to rational behaviour. And this is in general controversial, and doubly so in the case of risk-sensitive theories.

%In that paper, \citet{campbellmoore2022aurs} also consider ``local'' versions of the epistemic arguments for update procedures, following \citet{leitgeb2010ojb2}, showing that they require the risk-sensitive agent to nonetheless update by conditionalization. \Citet{campbellmoore2022aurs} argue that this is actually what gives the rational requirements on updating, and so conclude that conditionalization is rationally required (and thus, the Value of Information challenge cannot be avoided this way, cf., \citep{campbellmoore2020arae}). The analogous questions can be also asked for the practical utility versions: whilst we are unsure what this results in, it is unlikely that they result in the same recommendations as the optimal update plan.\footnote{It differs from the epistemic versions because we used an $r$-proper scoring rule, but this is not what we'd get if we just use pragmatic utility. } And thus, whilst the agent might evaluate the optimal update plan to be one which avoids the undermining we have identified, it is not the update method which we think that they should actually implement. 


\end{colored}


%{\color{red}
%	
%	\bigskip -----
%
%
%ALTERNATIVE VERSION:
%
%
%
%Whilst we don't know of any defenders of these theories taking this line of argument, as pointed out in \citet[sec7]{campbellmoore2020arae}, several standard arguments for conditionalization assume that rational agents maximise expected rather than risk-weighted expected utility. And in fact, the argument we have made here can be turned into an argument against Bayesian conditionalization. 
%%As is familiar, such theories lead to cases where the agent will pay to avoid free evidence \citep{buchak2010irereg}, but such violations can immediately be turned into arguments against updating 
%
%\Citet{brown1976ceu} provides an argument for Bayesian conditionalization inspired by Good's \citeyear{good1967pte} value of information principle. Good's principle shows that you will always do better if you take into account the available evidence before making your decision. Or equivalently, that if you make a decision in accordance with your credences updated by conditionalization, then you expect that you'll do better than making them instead using your prior credences. Brown moreover notes that we can judge various ways of updating your credences by the utility of the action they guide you to make, and it turns out that conditionalization will always be best. 
%
%But this argument will fail for the defender of risk-weighted decision theory, or any theory accommodating the Allais preferences or violating the sure thing principle. Any failure of the Value of Information principle, as we have for these theories \citep{buchak2010irereg,wakker1988neuai}, can be seen as cases where making the decision using one's prior credences is evaluated as better than making it after updating by conditionalization, or put another way, that updating your credences by staying put is preferable to updating them by conditionalization. So the argument for updating by conditionalization fails, some other update rule is better. 
%
%\Citet{campbellmoore2022aurs} make this point and investigate how the arguments apply for update rules, focusing not on practical utility but on maximising epistemic utility, or accuracy. We can apply analogous investigation in the case of practical utilities. An extension of the argument of Brown, or in the epistemic case, \citet{greaves2006jcc} would be to evaluate various update strategies and adopt the update strategy which you risk-weightedly expect to lead you to the best decisions, if you then use them to make a decision in accordance with risk-weighted expected decision theory. We could apply an analogous argument here: you have $P$ a probability measure over $\Omega\times\D$; you should plan to update your credences when faced with the various decision problems, $D$, so that when you then make a decision using those new credences (in accordance with risk-weighted expected utility theory), an optimal strategy will be recommended. This update must not be conditionalization, but there is some way of updating your credences to satisfy this desiderata: if $\s$ is the optimal picking strategy, then for each $D$, adopt the credences such that using them in your decision theory tells you to pick $\s(D)$. 
%
%\Citet{campbellmoore2022aurs} investigate what such an update looks like in the epistemic case, and the same could be done in the practical case. 
%
%However, there remains the question of whether this is a good argument for the rational requirement to update her credences in accordance with the proposed update rule. The alternative update rule might be the optimal strategy, but why think one should implement optimal update strategies. This is exactly the sort of thing that will fail for such theories \citep[discussion in ][section 4.2]{campbellmoore2022aurs}. 
%
%}

%
%
%\bigskip 
%--------
%
%
%
%
%Here is one way to argue for a method of credal updating: you provide an update plan, i.e., numerical values you will adopt as 
%
%\Citet{campbellmoore2020arae} investigate, there are various ways to apply arguments for rational update methods
%
%
%
%
%Any theory that accommodates the Allais preferences, or more generally, fails the Sure Thing Principle, is liable to have a case where what 
%
%
%
%
%
%
% As we presented the higher-order decision between strategies $\s_1$ and $\s_2$, you determine the probabilities over the higher-order states of the world---which specify both the decision problem you'll face and the first-order state of the world---by taking probabilities for the first-order states, probabilities for the decision problems, and assuming the decision you face is independent of the first-order state of the world. 
% And then, once you face whichever decision problem you do face, you use those same probabilities for the first-order states to make your decision. Now, that is clearly the right thing to do if you update your probabilities by standard Bayesian conditionalizing. 
% That is, once you know which decision problem you face, you simply conditionalize on that information to give your new probabilities, and because the decision problems and the worlds are independent, learning what decision problem you face doesn't change your credences about the world. 
% 
% One way to avoid this conclusion and the dilemma it poses is to argue against Bayesian conditionalization. 
% 
% 
% 
% 
%The problem with that assumption is that many decision theories that permit the Allais preferences don't think you should plan to update by conditionalizing on new evidence. Indeed, the cases we're discussing demonstrate that.
%
%
%
%
%
%
%$$
%\begin{array}{r|cccc}
%	& \omega'_1\ \&\ D_1 &  \omega'_2\ \&\ D_1 &  \omega'_1\ \&\ D_2 &  \omega'_2\ \&\ D_2 \\
%	\hline
%	P &  \sfrac{1}{11} \times \sfrac{11}{100} & \sfrac{10}{11}\times\sfrac{11}{100} & \sfrac{1}{2}\times\sfrac{89}{100} & \sfrac{1}{2}\times\sfrac{89}{100} \\
%\end{array}
%$$
%
%And we use the decision theory to evaluate the optimal picking strategy. We then noticed that it differs from what the decision theory recommends when faced with $D_1$, as calculated using $p$. 
%
%The defender of these decision theories might point out that this has made use of the assumption that you are updating by conditionalization. 
%
%
%
%To judge the recommendations of the decision theory, we applied the decision theory at the higher-level taking account of one's uncertainty not only over the state of the world but also over what decision she'll be faced with. 
%
%We assumed that these are independent. However, it is Bayesian conditionalisation which allows us to go from the assumption that they are independeent.  
%
%	
%In the example we gave, we assumed that which decision theory the agent will be faced with is independent of what the world was like.  
%
%One way that the defender of such decision theories may respond to our challenge is to reject our use of conditionalization. 
%
%Whilst conditionalization didn't explicitly 
%
%
%
%
%
%
%
%
%We presented this as offering contradictory advice at the point at which you face the first-order decision. Should you decide in accordance with what the optimal plan tells you to do, or should you apply the decision theory in this case?
%
%
%
%
%
%
%
%As is familiar,  theories which exhibit the Allais preferences will sometimes recommend avoiding free evidence, and even paying to avoid evidence \citep{buchak2010irereg}. Such arguments can be seen as 
%
%
%\Citet[sec.7]{campbellmoore2020arae} note that the 

%}




%
%
%
%{\color{red} This was RPs. To be deleted???
%
%
%
%
%In response to the second argument against self-undermining decision theories from above---the argument that they give rise to irresolvable dilemmas for the agent who uses them---a defender might say that the dilemma is, in fact, easily resolvable: You should do what your decision theory tells you to do at the point at which you face the decision. After all, at that point, you have more evidence, for you know which decision problem you'll face; and the Principle of Total Evidence tells us to choose using the more informed credences.
%
%The problem with that response is that the standard decision-theoretic justification of this version of the Principle of Total Evidence---which originates with Janina \citet{hosiasson1931wdw} and Frank \citet{ramsey1990wvk}, and is noted by \citet{savage1954fs}---doesn't hold for theories that permit the Allais preferences. It is well-known that these theories will sometimes require you to pay not to receive certain evidence; or, if you can't help but receive it, they'll prefer you to choose using the credences you had before learning it  \citep{buchak2010irereg}.
%
%So that response doesn't work, but it does suggest an alternative response that might work better. As we presented the higher-order decision between strategies $\s_1$ and $\s_2$, you determine the probabilities over the higher-order states of the world---which specify both the decision problem you'll face and the first-order state of the world---by taking probabilities for the first-order states, probabilities for the decision problems, and assuming the decision you face is independent of the first-order state of the world. And then, once you face whichever decision problem you do face, you use those same probabilities for the first-order states to make your decision. Now, that is clearly the right thing to do if you update your probabilities by standard Bayesian conditionalizing. That is, once you know which decision problem you face, you simply conditionalize on that information to give your new probabilities, and because the decision problems and the worlds are independent, learning what decision problem you face doesn't change your credences about the world. 
%The problem with that assumption is that many decision theories that permit the Allais preferences don't think you should plan to update by conditionalizing on new evidence. Indeed, the cases we're discussing demonstrate that.
%
%Suppose you make a plan: if I learn I face decision problem $D_1$, I'll adopt these posteriors; if I learn I face $D_2$, I'll adopt those posteriors. And suppose you take the utility of this plan at one of the higher-order states of the world to be the utility of whichever act you'll choose using your decision theory and the posterior credences you'll have if you learn the evidence that's true at that state and update as your plan says you should. Then the plan that looks best from your situation of uncertainty about which decision problem you'll face is one that leads to posteriors that will lead you to choose in line with strategy $\s_1$, since that's the strategy you prefer from that vantage point.
%
%So it might look as if we have a response to the self-undermining challenge: it is based on an account of credal updating that the user of the Allais-permitting decision theory rejects. The problem is that all we've shown is that these decision theories say you shouldn't \emph{plan} to conditionalize; not that you shouldn't conditionalize. \citet{campbellmoore2022aurs} note a similar problem arises when you evaluate an updating plan not by the utility of the posteriors to which it gives rise, but by something like the accuracy of those posteriors. And they argue that, in that context, risk-weighted expected utility theory indeed tells you to plan to update other than by conditionalizing, indeed, that this immediately follows from the avoidance of evidence. They nonetheless argue that one should actually conditionalize when the evidence comes in. And so any defender of a self-undermining decision theory who wishes to use the response outlined here must say why that isn't true for their decision theory. For if it turns out that, once you learn what decision problem you actually face, you should in fact update by conditionalizing, even though you would have planned not to had you planned before the uncertainty was resolved, then the self-undermining worry remains.
%
%}


\subsubsection{Uncertainty about decisions}\label{sect:reu:other mu}
%\todooldinfo{The writing/wording needs improving here, but I am proposing that it is at least a good working draft of it and not thinking that big changes need making.}

In our argument that Allais-permitting decision theories are self-undermining, we used the preferences they permit over the Allais gambles to construct a \emph{particular} way that you might be uncertain about what decisions you'll face and showed that, \emph{in that case}, you judge some alternative picking strategy to be preferable. 
%{\color{violet}Or, more generally, we noted that one could use any particular failure of the Sure Thing Principle or the Independence Principle to generate a particular such challenge. }



Moreover, the particular way of being uncertain over decisions you'll be faced with was a rather odd one. We specified that the credence you'll face $D^{\mathrm{Allais}}_{\mathrm{local}}$ is $\sfrac{1}{11}$ and your credence that you'll get the constant amount is $\sfrac{89}{100}$, with what the constant amount is being chosen specifically to demonstrate the undermining feature. 
	
	Of course this might be the situation you're facing, where you happen to have these credences over the possible decisions. It might be just unfortunate coincidence, or it could be specifically constructed this way by a nefarious opponent who has set you up: they're going to toss a biased coin to determine what to offer you at the specified probabilities, having chosen these numbers specifically to demonstrate the inconsistency in your judgements. 
	
	However, typically one's uncertainty over which decision you'll face is spread over a wide class of possible decision problems. If your particular uncertainty is not one that generates the underminingness, is it really that problematic that were you to be in the particular uncertainty situation your decision theory were undermining, if in your actual situation it judges its own picking strategies as optimal?
	
	


%\begin{colored}{red}ORIGINAL:
%	%	But these are only generating a particular way of being uncertain over decisions under which the strategy undermines its own recommendations. 
%Moreover, the particular way of being uncertain over decisions you'll be faced with was a rather odd one. We specified that the credence you'll face $D_1$ is $\sfrac{1}{11}$ and your credence that you'll face $D_2$ is $\sfrac{89}{100}$. 
%%More generally, any strict failure of the Sure Thing Principle or Independence Axiom would generate such an example.
%%\todoold{RP: I had a go at changing: will this work?}can be used to generate such a self-undermining example: if your decision theory leads you to strictly prefer act $a$ to act $b$, but also strictly prefer the mixed act that gives you probability $p$ of getting act $b$ and probability $1-p$ of getting act $c$ to the mixed act that gives you probability $p$ of getting act $b$ and probability $1-p$ of getting act $c$, then we can immediately see that the decision theory is self-undermining. 
%We could motivate this uncertainty if, for example, you believe you are facing a nefarious opponent who has set you up: they're going to toss a biased coin then offer you $D_1$ or $D_2$ at the specified probabilities, having chosen these number specifically to demonstrate your inconsistent judgements. 
%
%But this is not how we're usually uncertain over what decisions we'll face. 
%What does this argument show about someone who is uncertain about what decision problems she'll face in an alternative, and more normal, way?\todo{CCM: why is it abnormal? Feels like it can be motivated to me... like in the doctor case. }
%{\color{violet} For example, a spread over a much wider range of possible decisions. }
%
%\end{colored}

This is analogous to a certain standard response to the Dutch book arguments: perhaps you just think it's unlikely you'll ever face such a nefarious bookie. A common response is to argue that the mere existence of a Dutch book already shows you are irrational because it shows your preferences are inconsistent in a particular way---they judge the same decision differently when it is presented in different way, perhaps.\footnote{Cf. \citep{armendt1993db,mahtani2014db}.} %The set-up in which a bookie approaches you and offers a string of bets merely dramatizes this inconsistency. 
{The set-up in which an opposing Gambler approaches you, buys and sells bets that you deem fair, and thereby saddles you with a sure loss merely dramatizes this inconsistency.} The argument doesn't assume you'll ever actually meet such a person. We might argue similarly that the mere existence of self-underminingness for some way of being uncertain about what decision you'll faced is already a challenge to the decision theory. Perhaps it shows that it is inconsistent in the same way the Dutch book argument shows non-probabilistic credences are inconsistent.

In the case of the Dutch book arguments, \Citet[Sec 6.2]{pettigrew2020dutch} argues the mere existence of a set of bets you'll accept individually that, taken together, lead to sure loss isn't sufficient to show you are  irrational. Instead, he asks what happens if you are uncertain about which decisions you'll face. Drawing on results from Mark \citet{schervish1989gm} and Ben \citet{levinstein2017pgeu}, he shows that, for very  many natural ways of being uncertain about the decisions you'll face, if you have non-probabilistic credences and face whatever decision you'll face with those credences, there are alternative probabilistic credences you might have had instead that guide your choices better.








%
%
%\todooldinfo{Can we include }
%
%We would like to be able to offer an alternative and more general argument which would apply to a whole host of ways of being uncertain about which decision problem you'll be faced with. 
%
%
%\Citet{pettigrew2020dutch} instead agrees that the mere existence isn't sufficient to show irrationality. Poor performance in a particular situation is not usually a reason to abandon a tool, since it might compensate by performing better in other situations. He instead develops a more general argument that probabilistic credences are better guides to action. We could similarly try to develop a more general  result applying to a whole host of ways of being uncertain about what decision you'll be faced with. 
%
%
%
%
%
%
%However, this is just a particular, unusual situation to be in. And poor performance in a particular situation is not usually a reason to abandon a tool, since it might compensate by performing well in other situations. Something similar may apply here. 
%
%
%
%
%In our argument that Allais-permitting decision theories are self-undermining, we used the preferences they permit over the Allais gambles to construct a \emph{particular} way that you might be uncertain about what decisions you'll face and showed that, \emph{in that case}, you judge some alternative picking strategy to be preferable. But the defender of such a theory might reply that this shows only that the theory has these troubling consequences for agents who are uncertain in that way. For all we've shown, the theory might be entirely unproblematic when you are uncertain in other ways about the decision problem you'll face. Perhaps, according to your uncertainty, the theory is self-recommending.
%
%{\color{violet}
%Moreover, the particular way of being uncertain required for the example was a particular constructed case using the Allais preferences, specifying that the credence you'll face $D_1$ is $\sfrac{1}{11}$ and your credence that you'll face $D_2$ is $\sfrac{89}{100}$. These would be the right credences to have in a particular scenario, where a neferious opponent sets you up with this problem: she'll offer you $D_1$ or $D_2$ at the specified probabilities, having chosen these number specifically to demonstrate your inconsistent judgements. However, this is just a particular, unusual situation to be in. And poor performance in a particular situation is not usually a reason to abandon a tool, since it might compensate by performing well in other situations. Something similar may apply here. 
%}
%{\color{red}
%This response is analogous to one offered against the Dutch book arguments: perhaps you just think it's unlikely that you'll ever face such a neferious bookie. A common response to this is to argue that the mere existence of such a Dutch Book exhibits an irrationality, although \citet{pettigrew2020dba} instead bolsters the argument by showing that quite generally non-probabilistic credence functions have alternatives which are guaranteed to act as a better guide to action than your current credences.
%
%We could similarly try to bolster our objection by showing a more general result. 
%}

%{\color{red}In response to this, we could try to bolster our objection by showing a more general result. }
We might here similarly try to bolster our objection by showing a more general result which would apply to a whole host of natural ways of being uncertain over which decision problem you'll face. 
Of course, we would have a stronger objection if we could show that for \emph{any} way of being uncertain over the decision problems you face, the theory is self-undermining. In fact, we'll never get something as general as that: after all, if your probability distribution places all of its probability on a single decision problem, then it will think of itself as permissible---and indeed, it will think of anything that disagrees with it as impermissible. But we might hope to be able to show that it is self-undermining for a much broader range of distributions over the possible decision theories than we currently have, thus arguing that for any `plausible' way of being uncertain over possible decisions, the theory is still undermining. 

%If a defender of REU tries to apply this response, there's pressure on them to say why one should be uncertain over decision problems win a way that's compatible with being self-recommending. For example, if they could show that REU is self-recommending when you think that all possible decision problems are possible; if your measure of them requires almost everywhere decisiveness\todoold{what's the name we'll use later?}, then we think that this would be an acceptable response. 

We don't have any general results in this area, but we do have some suggestive particular cases for a particular Allais-permitting theory. This is John Quiggin's (\citeyear{quiggin1982tau,quiggin1993geut}) rank-dependent utility theory, which is formulated for exogenous, objective probabilities, and Lara Buchak's (\citeyear{buchak2014rr}) risk-weighted expected utility theory, which is a formally equivalent theory formulated for endogenous, subjective probabilities.

To give your \emph{risk-weighted expected utility} for an act $a$, defined over the states in $\Omega$, we need a probability function $p$ over $\Omega$, and a risk function $r : [0, 1] \rightarrow [0, 1]$, which takes a probability and skews it---we assume $r$ is continuous, strictly increasing, and $r(0)= 0$ and $r(1) = 1$. Now, suppose $a$ is an act, and let $\U(a)$ be the random variable that gives the utility of $a$ at a given state. 
If $\Omega$ is finite, the risk-weighted expected utility of $a$ can be written as follows:\footnote{This is not Buchak's favoured formulation; instead it's closer to the usual formulation of rank dependent expected utility theory; see, for example \citep[ch.6]{wakker2010prospect}.  See also sec.6.9 for the continuous case, although note that Buchak's theory does not make use of a distinction between gains and losses \citep[see][p59]{buchak2014rr}. }%\todoold{double check ref}
%Then, if $\Omega$ is finite, the expected utility of $a$ can be written as follows:
%$$
%\Exp_p[\U(a)] = \sum_{u \in \Set{\U(a)(\omega)\given \omega \in \Omega}} (p(\U(a) \geq u) - p(\U(a) > u))\times u
%$$
%Applying the risk function to the probabilities in this formulation gives us the risk-weighted expected utility of $a$:
$$
\RExp_{p, r}[\U(a)] = \sum_{x \in \Set{\U(a)(\omega)\given \omega \in \Omega}} (r(p(\U(a) \geq x)) - r(p(\U(a) > x)))\times x
$$
And, more generally, if the utilities are bounded below by $l$ and above by $u$,
%$$
%\Exp_p[\U(a)] = \int^h_{l} p(\U(a) > u)\, du
%$$
%and
$$
\RExp_p[\U(a)] = \int^h_{u} r(p(\U(a) > x))\, dx
$$
Then risk-weighted expected utility theory tells you to maximize risk-weighted expected utility.


%To give your \emph{risk-weighted expected utility} for an act $a$, defined over the states in $\Omega$, we need a probability function $p$ over $\Omega$, and a risk function $r : [0, 1] \rightarrow [0, 1]$, which takes a probability and skews it---we assume $R$ is continuous, strictly increasing, and $r(0)= 0$ and $r(1) = 1$. Now, suppose $a$ is an act, and let $\U(a)$ be the random variable that gives the utility of $a$ at a given state. Then, if $\Omega$ is finite, the expected utility of $a$ can be written as follows:
%$$
%\Exp_p[\U(a)] = \sum_{u \in \Set{\U(a)(\omega)\given \omega \in \Omega}} (p(\U(a) \geq u) - p(\U(a) > u))\times u
%$$
%Applying the risk function to the probabilities in this formulation gives us the risk-weighted expected utility of $a$:
%$$
%\RExp_{p, r}[\U(a)] = \sum_{u \in \Set{\U(a)(\omega)\given \omega \in \Omega}} (r(p(\U(a) \geq u)) - r(p(\U(a) > u)))\times u
%$$
%And, more generally, if the utilities are bounded below by $l$ and above by $h$,
%$$
%\Exp_p[\U(a)] = \int^h_{l} p(\U(a) > u)\, du
%$$
%and
%$$
%\RExp_p[\U(a)] = \int^h_{l} r(p(\U(a) > u))\, du
%$$
%Then risk-weighted expected utility theory tells you to maximize risk-weighted expected utility.

%\todooldinfo{Todo: does Lara give this definition for the infinite case? I learned it from Kenny E.}


Now, suppose:
\begin{enumerate}[label={\normalfont (i)}]
\item there are just two first-order possibilities $\omega_1$ and $\omega_2$,
\item your credence function is $p$, with $p(\omega_1) = 0.1, 0.2, \ldots, 0.8$, or $0.9$ and $p(\omega_2) = 1-p(\omega_1)$;
\item your risk function is a power function $r_k(x) = x^k$, with $k = 0.5$, $0.6$, $\ldots$, $0.8$, $0.9$, $1.1$, $1.2$, $\ldots$, $1.9$, or $2$;
\item you know you'll face a choice between just two acts, but you don't know which two acts, and you place a measure $\mu$ over the possible decision problems that takes the utilities of the two acts at the two possibilities to be independent of one another and all distributed according to a beta distribution $\mathrm{Beta}(\alpha, \beta)$ with $\alpha = 1, 2, 3, 4$, or $5$ and $\beta = 1, 2, 3, 4$, or $5$.\end{enumerate}
Then, let $\s$ be any picking strategy compatible with REU when coupled with $p$ and $r_k$. Then there is an alternative picking strategy $\s^*$ such that REU with credences given by $p$ and $\mu$ and risk function $r_k$ strictly prefers $\s^*$ to $\s$. What's more, $\s^*$ is not compatible with REU with $p$ and $r_k$. And indeed, it's possible to find $\s^*$ so that it is compatible with REU with $p$ and $r_{k^*}$ for some $k^* \neq k$. That is, $\s^*$ is a picking strategy compatible with REU coupled with a different risk-averse risk function. So, uncertain which decision they'll face, someone using REU with this risk function would prefer that, when the uncertainty is resolved and they face a particular decision, they use REU with a slightly different risk function.\footnote{See the Mathematica notebook here for the tools to carry out these calculations [link to notebook from journal page as supplementary material].}%\todooldinfo{link to mathematica}
\todo{we'll need to remember to do that.}

%Second, suppose $0<k<1$. So your risk function $r_k(x) = x^k$ is risk-seeking. And now let $\s$ be any picking strategy compatible with REU with $p$ and $r_k$. Then there is an alternative picking strategy $\s^*$ such that REU with credences given by $p$ and $\mu$ and risk function $r_k$ strictly prefers $\s^*$ to $\s$. What's more, $\s^*$ is not compatible with REU with $p$ and $r_k$. And indeed, it's possible to find $\s^*$ so that it is compatible with REU with $p$ and $r_{k'}$, for some $k < k'$. That is, $\s^*$ is a picking strategy compatible with REU coupled with a less risk-seeking risk function. So, uncertain which decision they'll face, someone using REU and this risk function would prefer that, when the uncertainty is resolved and they face a particular decision, they use REU with a slightly less risk-inclined risk function.


%We have suggestive initial results for REU, but nothing more at this stage. Suppose there are just two states of the world. You have credence 0.3 in the first and 0.7 in the second. And your risk function is a power function $R(p) = p^k$ for some $k> 0$. Suppose your utilities are bounded and so the possible utility pay-offs of the different possible options lies in some range $[a, b]$. Suppose you know you'll face a binary decision---that is, a choice between just two alternatives. We might write such a decision problem as $(u_1, u_2; v_1, v_2)$, where $u_1$ is the utility of the first option at the first state of the world, and so on. And suppose we take the different values $u_1, u_2, v_1, v_2$ to be independent of one another and all distributed according to the same distribution. Then: if that distribution is the uniform distribution or any number of other beta distributions, then, for a range of $k > 1$, which make your risk function $R_k$ risk-averse, then there is another alternative, less risk-averse decision theory---in fact, REU with $R_{k'}$ for $k' < k$---that REU with $R_k$ would prefer to make the decision when it becomes clear which you'll face. And, for a range of $k < 1$, which make your risk function risk-seeking, then there is an alternative, less risk-seeking decision theory---in fact, REU with $R_k$ for $k < k'$---that REU with $R_k$ would prefer to make the decision. We list some of these results in the Appendix.




\section{Expected Utility Theory}\label{sect:eut}
In this section, we reassure ourselves that Savage-style expected utility theory is self-recommending; that is, if we assume act-state independence, expected utility theory endorses itself. We will need to be more detailed about the framework in order to present our result. 


 \emph{States } $\Omega$ is the set of possible states of the world. We'll assume there are only finitely many.

\emph{Uncertainty } The agent's uncertain beliefs about the world are represented by a single probability function $p$ over $\Omega$. 

\emph{Acts }
$\A$ is a non-empty set. It is the set of all possible acts. 



 \emph{Utilities } $\U$ is the agent's utility function. It takes each act $a$ in $\A$ and state $\omega$ in $\Omega$ and returns a utility value $\U(a)(\omega)\in\Re$. We will assume that utilities are bounded above and below. That is, there are $l,u\in\Re$ such that for all acts, $a\in\A$, $l\leq \U(a)(\omega)\leq u$. 
 %, and without loss of generality, for each act $a \in \A$ and state $\omega\in\Omega$, $0 \leq a(\omega) \leq 1$. We write $\A$ for the set of possible acts. \todoold{CCM: I still think we should have U and a separated. At least we *need* to do something like that to make sense of applying EU to strategies! since we need to specify the utility of a strategy..??}%An act is a function $a$ that specifies an outcome in each state $a(s)$. A utility function specifies the utility of that outcome $U(a(s))$.  %\todoold{should utilities be separated??}
%An act is a function $a$ that takes each state $s_i$ returns a utility $a(s_i)$, where $0 \leq a(s_i) \leq 1$. This needn't include all functions from $\S$ to $[0, 1]$, for we might take only certain acts to be possible.
%Note that we are essentially here holding the utility function fixed, for our purposes. 

\emph{Decision problems } A decision problem $D$ specifies a non-empty finite set of acts: the acts that are available in that decision problem. The set of all relevant decision problems is $\D$. %\todo{should we add ``re;evant''?}

(In fact, we could relax the assumption that $D$ specifies a finite set of acts and instead assume that the set of acts it specifies is compact relative to the utility function, that is, $\{\U(a)\given a \in D\}$ is a compact subset of $[l,u]^\Omega$. But we will continue to assume that decision problems are finite for ease of presentation.)

%That is, $\emptyset \neq D \subseteq \A$. %For example, you might be choosing whether to take an umbrella or not. 
%$\D$ is a set of decision problems. A decision problem $D$ is a set of acts. That is, $D \subseteq \A$. Again, this needn't include all subsets of $\A$, for we might take only certain decision problems to be possible.

%\emph{Choice function } A choice function, $\c$, specifies a non-empty subset of each decision problem, so that $\emptyset \neq \c(D) \subseteq D$, for each $D$ in $\D$. We might understand $\c(D)$ as the set of acts in $D$ that the choice function deems permissible or choiceworthy, or rather, those that it does not deem to be bad, or rejects. \todoold{should we add back in: ``Examples are $\EU_p$, $\REU_p^r$, $\EAd_\IP$ and $\Maximin_\IP$, as defined throughout the parper. ``}


{
\emph{Choice function } A choice function, $\c$, specifies a non-empty subset of each decision problem, so that $\emptyset \neq \c(D) \subseteq D$, for each $D$ in $\D$. If $a$ is \emph{not} in $\c(D)$ then $\c$ deems $a$ impermissible in $D$. Some authors go further and say that any $a$ in $\c(D)$ is rationally \emph{permissible} \citep[e.g.,][]{moss2015tse}. But others do not. They instead say that, if $a$ is in $\c(D)$, then $a$ is not deemed impermissible, but unless $a$ is the only act in $\c(D)$, it does not follow that $a$ is permissible or positively evaluated in any way \citep[e.g.,][]{debock2019iar}.}

%{ \emph{Choice function } A choice function, $\c$, specifies a non-empty subset of each decision problem, so that $\emptyset \neq \c(D) \subseteq D$, for each $D$ in $\D$. We will understand $\c(D)$ as the set of acts in $D$ that the choice function does not \emph{reject} or deem \emph{rationally impermissible to choose}. If $\c(D)$ contains only a single act, then that act is uniquely \emph{choiceworthy} or \emph{rationally permissible} in $\D$. But \emph{being rejected} is not the dual of \emph{being permissible}. Rather, we think of being rejected or impermissible in $D$ as a negative status. Similarly, we think of being choiceworthy or permissible in $D$ as a positive status. It is possible for an act to lack either status (not possible if the two notions are duals). More on this in sections 3 and 4.}






%So it is not the case that an option is rejected/impermissible just in case it is not positively permissible. This allows us to maintain rational permissibility---a positive status---agglomerates in the sense that if it is permissible to pick $a$ in $D_1$ and it is permissible to pick $b$ in $D_2$, then their conjunction is permissible, \emph{i.e.}, it is permissible to both pick $a$ in $D_1$ and $b$ in $D_2$




%But if $\c(D)$ contains many acts, that does \emph{not} imply that they are all equally choiceworthy, nor that they are all rationally permissible. Not rejected/impermissible in $D$ is a strictly weaker status than choiceworthy/permissible in $D$. Compare: in volleyball, it is not impermissible for any front row player on the serving team to stand with their hands up, even if they are bunched together. But it \emph{is} impermissible for \emph{all} front row players to stand with their hands up in that case---that is illegal screening. This would be a puzzle if not impermissible and permissible were equivalent. If it is positively permissible for \emph{any} front row player on the serving team to stand with their hands up, then it must be positively permissible for \emph{all} of them to do so.

%that were so, then it would be permissible for }




%this may either reflect the fact that all acts in $\D$ are equally choiceworthy and hence rationally permissible, or the fact that they are \emph{incomparable}. In the latter case, whether or not an act in $\c(D)$ is rationally permissible...}





\emph{Picking strategy } As above, a picking strategy, $\s$, specifies an act from each decision problem, so that $\s(D)\in D$, for each $D$ in $\D$. The set of all picking strategies is $\S$. 

A picking strategy picks for a choice function $\c$ if it never picks an option that $\c$ deems impermissible. That is: 
\begin{definition}\label{def:picks for}
	A picking strategy $\s$ \emph{picks for a choice function} $\c$, if for all decision problems, $D$, $\s(D) \in \c(D)$.
\end{definition}
%Such a picking strategy will never select an option which $\c$ deems impermissible. 

%\item\emph{Choice function} A choice function 
%\end{itemize}

Given a probability function $p$ defined over $\Omega$, the expected utility of an action $a$, by the lights of $p$, is given by \begin{equation}
	\Exp_p [\U(a)]:=\sum_{\omega\in\Omega}p(\omega)\U(a)(\omega)
\end{equation}
If one has (probabilistic) credences given by $p$, expected utility theory says that one should choose an act in $D$ that maximises $\Exp_p[\U(a)]$.\footnote{The version we present here is of the sort described by \citet{savage1954fs}, in which it is assumed that the acts are independent of the states of the world. This assumption is dropped in the evidential decision theory of \citet{jeffrey1965lod} and the causal decision theory of \citet{stalnaker1972ldl, gibbard1978ctk, joyce1999fcdt}.} That is, we define the choice function to which expected utility theory gives rise when coupled with probability function $p$ as follows:

\begin{definition}[Expected Utility Theory ($\EU_p$)]\label{def:EU}
	\[\EU_p(D):=\{a\in D\given \text{for all $a'\in D$, }\Exp_p[\U(a)]\geq \Exp_p[\U(a')]\}\]
\end{definition}
%\todooldinfo{In nu stuff I am using ``picks for'' terminology. Should that be used throughout??}
%\todooldinfo{should we introduce ``choice funciton'' here??}
%$\EU_p$ is an example of a so-called choice function. 
%\begin{itemize}
%	\item A \emph{Choice function}, $C$, specifies for each decision which acts are choiceworthy, or perhaps not rejected, that is $C:\D\to\A$ such that $C(D)\subseteq D$ and $C(D)\neq\emptyset$.
%	\item A strategy $\s$ is a $C$ strategy iff $\s(D)\in C(D)$ for each $D$. 
%\end{itemize}

%That is, $\s$ is an $\EU_p$ picking strategy iff $\s(D)$ is an action in $D$ which maximises $\Exp_p(a)$. In fact, there can be multiple $\EU_p$ strategies when there are different options which both maximise expected utility.\todoold{does this need more discussion?}

So a picking strategy $\s$ picks for $\EU_p$ iff $\s(D)$ maximizes expected utility by the lights of $p$.
Since we have assumed that each $D$ is finite, or compact, $\Exp_p[\U(a)]$ obtains its maximum in $D$; so $\EU_p(D)\neq\emptyset$. 

Now we want to judge the expected utility of a picking strategy itself---we want to ask whether the picking strategies that always pick an act that maximizes expected utility from any decision problem themselves maximize expected utility when you're uncertain which decision problem you'll face. This requires us to fix not only $p$, which gives your credences over $\Omega$, but also your credences over the decision problems you might face, given by some probability measure $\mu$ over $\D$. We will assume these are independent. %So we can separate your credence that you'll be faced with decision $D$ and that the world is thus-or-so, $\omega$, into a probability measure $\mu$ over $\D$ and a probability measure $p$ over $\Omega$. 
{So your credences over the joint space, $\Omega\times\D$, are given by the product measure $p\times\mu$. That is, your credence that you are in world $\omega$ and will be faced with some decision problem in the (measurable) set of decision problems $E$ is given by $(p\times\mu)(\omega,E)=p(\omega)\times\mu(E)$.} 
We will moreover always assume that $\mu$ is countably additive. 


We can now simply apply our notion of expected utility with the credences over $\Omega\times\D$ given by %$p\times \mu$. 
{ $p\times\mu$.} We judge a picking strategy by the utility of the acts it picks, and so we define $\U(\s)(\omega,D):=\U(\s(D))(\omega)$. We can then apply our definition of expected utility and get that any picking strategy that picks for $\EU_p$ maximizes expected utility by the lights of %$p\times \mu$. 
{ $p\times\mu$.}
\begin{proposition}\label{thm:eu-self-rec}
	For any $p$, $\mu$, if $\s$ picks for $\EU_p$, then, for any picking strategy $\s'$ in $\S$,
	$$\Exp_{p\times\mu}[\U(\s)]\geq\Exp_{p\times\mu}[\U(\s')].$$
	That is, if $\s$ picks for $\EU_p$, then $\s\in\EU_{p\times\mu}(\Strategies)$. 
	\end{proposition}
(Recall: 
%$\s\in\EU_{p\times \mu}$ is the choice function to which $\EU$ gives rise when coupled with \todo{I don't think we've defined $\EU$ by itself... what's going on here!}%$p\times \mu$. 
%{ $p\times\mu$}, and 
$\Strategies$ is the set of picking strategies. $\EU_{p\times \mu}(\Strategies)$ is the set of those picking strategies that maximize expected utility by the lights of %$p\times \mu$. 
{ $p\times\mu$}, as in \cref{def:EU}.)


This shows that expected utility theory is not self-undermining in the way the Allais-permitting decision theories considered in the previous section are self-undermining. Expected utility picking strategies are themselves maximisers of expected utility. 

What's more, they are the only picking strategies which maximise expected utility. Or at least, the picking strategies which maximise expected utility are those that look like an $\EU_p$ strategy from $\mu$'s perspective. %For example, if $\mu$ is certain you won't be faced with the decision problem $D$, then it doesn't matter whether the strategy chooses in an expected utility way for $D$. 

\begin{definition}\label{def:mu surely picks for}
If $\c$ is a choice function and $\s$ is a picking strategy, then $\s$ \emph{$\mu$-surely picks for $\c$} iff $\mu\{D \in \D \given \s(D)\in \c(D)\}=1$
\end{definition}
%That is, $\mu$ is certain that, for each decision problem $D$, $\s$ picks an act from $D$ that lies in $\c(D)$.
{That is, $\s$ $\mu$-surely picks for $\c$ just in case $\mu$ is certain you'll face a decision problem where what $\s$ picks is compatible with $\c$, \emph{i.e.}, $\s(D)\in\c(D)$. That is, $\mu$ is sure that $\s$ does not pick an option that $\c$ rejects. }
%{ That is, $\s$ $\mu$-surely picks for $\c$ just in case you are certain (probability 1 according to $\mu$) that you will face a decision problem $D$ where $s$ picks an option from the choice set $\c(D)$ (\emph{i.e.} a non-rejected option).}

\begin{proposition}\label{thm:eu-uniquely-optimal}
	For any $p$ and $\mu$, if $\s$ $\mu$-surely picks for $\EU_p$, while $\s'$ does not, then $$\Exp_{p\times\mu}[\U(\s)] > \Exp_{p\times\mu}[\U(\s')].$$
	
	So, if $\s$ does not $\mu$-surely pick for $\EU_p$, then $\s\notin\EU_{p\times\mu}(\Strategies)$. 
\end{proposition}

We thus have that $\s \in \EU_{p\times \mu}(\Strategies)$ iff $\s$ $\mu$-surely picks for $\EU_p$.

The proofs of all the results in the paper are in the appendix.

It is worth noting that the reasoning that delivers these results only holds when we have assumed that the state of the world is independent of the act chosen, i.e., where we are using Savage's (\citeyear{savage1954fs}) version of expected utility theory, in which the states of the world are independent of the acts chosen. %It is a further question whether the causal or evidential versions of expected utility theory are self-undermining in the current sense. We leave that for future work.%\footnote{In his discussion of evidential and causal decision theory, from which we borrow his use of the terminology `self-recommending', Brian \citet{skyrms1982cdt} gives a similar sort of argument against both of those rival theories. He imagines that you know the sequence of decisions that you will face, and you ask your decision theory whether it would recommend committing to using itself as the theory with which you face those decisions. And he notes that there are sequences for which causal decision theory would not recommend itself, and there are sequences for which evidential decision theory would not recommend itself. This is analogous to the money-pump arguments we mentioned earlier, since it shows that there is some sequence of choices such that causal decision theory does not permit you to use causal decision theory to make them, and there is some sequence of choices such that evidential decision theory does not permit you to use evidential decision theory to make them. But it is still an open question whether, when there is uncertainty about the decision you'll make, one of these theories would not permit you to use itself when making whatever decision you face.}

% \todooldinfo{Not clear whether we need this big footnote.}

%As well as assuming act-state independence, we've also assumed decision-state independence: that is, we've assumed that, from the point of view of your credences over $\Omega \times \D$, the decision you face and the state of the world are independent of one another. We've done this because this is the assumption we made in the case of the Allais- or risk-permitting theories above and we'll make again for the Ellsberg- or ambiguity-permitting theories below. But, in fact, analogues of \Cref{thm:eu-self-rec,thm:eu-uniquely-optimal} hold even if we don't assume this. If we assume that, when the uncertainty about the decision you face is resolved, and you face that decision, you face it using expected utility theory with the credences you obtain from your credences over $\Omega \times \D$ by updating on the decision problem you face, then the picking strategy that results maximizes expected utility from the point of view of your credences over $\Omega \times \D$.


\subsection{Decision-State Dependence}\label{sect:decdep}

As well as assuming act-state independence, we've also assumed decision-state independence: that is, we've assumed that, from the point of view of your credences over $\Omega \times \D$, the decision you face and the state of the world are independent of one another, given by $\pb=p\times\mu$. But, in fact, analogues of \Cref{thm:eu-self-rec,thm:eu-uniquely-optimal} hold even if we don't assume this. 

In any decision problem, we must bring one's probability $\pb$ up to speed on the problem that you face (by conditionalizing on that information), and then use expected utility theory with this updated credence function to determine what to select. 
%The strategies which are in $\EU_\pb(\S)$ are those that select according to expected utility theory, using the probability function over $\Omega$ which is generated by conditionalising on knowing which decision problem it is that you're facing, assuming that this is well-defined. 
%If we assume that, when the uncertainty about the decision you face is resolved, and you face a particular decision problem, you will use expected utility theory with the credences you obtain by updating on the decision problem you face, then the picking strategy that results maximizes expected utility from the point of view of your credences over $\Omega \times \D$.
The details of this are developed in \cref{sect:appendix:decdep}.

\begin{definition}\label{def:cond}
	If $\pb$ is a probability over $\Omega\times \D$; we specify a choice function: 
	$\EU_{\pb(\cdot|-)}$ given by $\EU_{\pb(\cdot|-)}(D)=\EU_{\pb(\cdot|D)}(D)$, when this is well-defined.\todooldinfo{check specification and link to appendix}
	That is, $\EU_{\pb(\cdot|-)}(D):=\{a\in D\given \text{for all $a'\in D$, }\Exp_{\pb(\cdot|D)}[\U(a)]\geq \Exp_{\pb(\cdot|D)}[\U(a')]\}$\todo{Should this ``that is'' go into footnote or be deleted?}
%	 that is, $a\in\EU_{\pb(\cdot|-)}(D)$ iff $\Exp_{\pb(\cdot|D)}[\U(a)]

$\pb_\Decs$ is the marginal probability measure on $\Decs$ generated by $\pb$, that is $\pb_\D(E)=\pb(\Omega\times E)$ for $E\subseteq\Decs$.\todo{measurable?}

We say that %$\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ iff $\pb\{\seq{\omega,D}\given\omega\in\Omega,\, \s(D)\in\EU_{B(\cdot|D)}(D)\}=1$; that is the marginal $\pb_\D\{D\given \s(D)\in\EU_{B(\cdot|D)}(D) \}=1$. 
a picking strategy, $\s$, \emph{$\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$} iff \[\pb_\Decs\{D\given \s(D)\in EU_{\pb(\cdot|-)}(D)\}=1\]
\end{definition}
When $\pb$ has the form $p\times\mu$, then $\pb_\Decs$ is just $\mu$ and for every $D$, the conditional $\pb(\cdot|D)$ is just given by $p$. These are thus exactly our original notions of $\EU_p$ and $\s$ $\mu$-surely picking for $\EU_p$. 
\todooldinfo{Check this notation throughout}

We can then show the more general version of \cref{thm:eu-self-rec,thm:eu-uniquely-optimal}:
\begin{proposition}\label{thm:eu-dep}
	$\s\in\EU_\pb(\Strategies)$ iff $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$.
\end{proposition}


%It is also important that we assumed that you think that the decision you'll face is independent of the state of the world. It's clear that these results will fail without this assumption. Suppose you might face the decision whether or not to take a bet on whether it is raining or not; if you do, you maximize expected utility by taking the bet; but suppose I arrange things so that you face that decision only if it isn't raining. Then of course the best picking strategy will tell you to not to take the bet. More generally, if you might face a decision problem, and your decision theory recommends an act that does not dominate another available one, i.e., when there is some state in which the other one is better, then if you're only going to face that decision problem in that possibility, then, while uncertain which decision problem you'll face, your decision theory will prefer a picking strategy that picks the other act. 
%If you're only going to face $D_1$ in world $\omega_1$ and $D_2$ in $\omega_2$, 
% \todooldinfo{check and improve the wording there}






\section{Decision theories for imprecise credences}

There is another range of decision theories that diverge from expected utility theories: those theories which accommodate ambiguity and imprecision. In the decision theories considered so far, we represent an individual as assigning precise credences to the various states of the world. But some think we do better to model individuals as having imprecise credences instead \citep{walley1991srip, bradley2016ip}. There are many ways to do this, but one of the most well-known represents an individual's doxastic state not by a single credence function, which assigns to each state of the world a single numerical measure of their confidence in that state, but by a set of such functions. We call this set your \emph{credal set}. It is a set $\IP$ of probability measures over the states of the world, $\Omega$.\footnote{{It is standard in the imprecise probability literature to reserve the term ``credal set'' for convex sets of probability measures. We do not assume convexity here.}}


Many decision theories have been proposed for an agent whose uncertain beliefs are represented in this way. We discuss three prominent ones: $\Gamma$-Maximin, E-Admissibility and Maximality. %\todoold{that ok?}
%E-Admissibility says that an act is permissible if it maximizes expected utility by the lights of at least one member of your credal set. \todoold{reference?} $\Gamma$-Maximin is more restrictive than E-Admissibility: it tells you how to choose amongst the options that E-Admissibility deems permissible. Maximality is more permissive than E-Admissibility: it will only rule out an action if there's an alternative which is better according to every member of your credal set. 



\subsection{$\Gamma$-maximin}\label{sect:gamma}
%Some decision theories go beyond E-Admissibility and permit only some subset of the E-Admissible acts. Any such theory will be susceptible to the same self-undermining concerns that we saw affecting E-admissibility above: there will be strategies that pick for the choice function given by that theory that the theory itself deems impermissible. The situation may in fact be worse for such theories because for E-Admissibility there are guaranteed to be some strategies that pick for E-Admissibility that E-Admissibility also permits: strategies that successfully coordinate across decisions. But for more restrictive theories these coordinated strategies may be ruled out by the theory, and we might have the result that all the strategies that pick for the theory's choice function are deemed impermissible by the theory. 

%A prominent example of a theory that is more restrictive than E-Admissibility is $\Gamma$-Maximin. 

To illustrate $\Gamma$-Maximin, consider an example that is often used to motivate it, namely, the Ellsberg paradox \citep{ellsberg1961rasa}:

\label{sect:Ellsberg}
\begin{quote}
 An urn contains 90 balls. You know that 30 of them are red, and the remaining 60 are black and yellow, but you don’t know how many are black and how many are yellow. I am about to draw a ball from the urn.
\end{quote}
If the states of the world are \emph{Red} (I draw a red ball), \emph{Black} (I draw black), and \emph{Yellow} (I draw yellow), you might naturally take your credal set to be $$\IP = \Set{p \given p(\emph{Red}) = \sfrac{1}{3}\ \&\ p(\emph{Black})+p(\emph{Yellow}) = \sfrac{2}{3}}.$$
Now consider the following two possible decision problems, $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$:
$$
\begin{array}{r|ccc}
D^{\mathrm{Ellsberg}}_1 & \emph{Red} & \emph{Black} &\emph{Yellow} \Bstrut \\\hline \Tstrut\IP & \sfrac{1}{3} & x&\sfrac{2}{3}-x %\multicolumn{2}{c}{\sfrac{2}{3}}
\Bstrut \\\hline \hline\Tstrut 
\text{1E} & \text{\pounds 10} & \text{\pounds 0}  & \text{\pounds 0} \\
\text{1F} & \text{\pounds 1} & \text{\pounds 11}  & \text{\pounds 1} 
\end{array}
\hspace{10mm}
\begin{array}{r|ccc}
D^{\mathrm{Ellsberg}}_2 & \emph{Red} & \emph{Black} &\emph{Yellow} \Bstrut \\\hline \Tstrut\IP & \sfrac{1}{3} &x&\sfrac{2}{3}-x % \multicolumn{2}{c}{\sfrac{2}{3}}
\Bstrut \\\hline \hline\Tstrut 
\text{2E} & \text{\pounds 11} & \text{\pounds 1}  & \text{\pounds 11} \\
\text{2F} & \text{\pounds 0} & \text{\pounds 10}  & \text{\pounds 10} 
\end{array}
$$
Faced with these decisions, people often report the Ellsberg preferences: they will choose 1E from $D^{\mathrm{Ellsberg}}_1$, and 2F from $D^{\mathrm{Ellsberg}}_2$.\footnote{In fact, we have added a small constant to the usual versions of 1F and 2E, reflecting the fact that people strictly prefer the usual version of 1E over the usual version of 1F, and so are willing to pay a penalty for making that choice; we've taken that penalty to be \pounds 1, but our point remains however small you make it.} And indeed that is exactly what $\Gamma$-Maximin demands. It says that, faced with a particular decision problem, you should pick one of the acts whose minimum expected utility by the lights of the probability functions in $\IP$ is maximal: in $D^{\mathrm{Ellsberg}}_1$, 1E uniquely maximizes minimum expected utility; and in $D^{\mathrm{Ellsberg}}_2$, 2F does that.

\begin{definition}[$\Gamma$-Maximin$_\IP$ ($\Maximin_\IP$) ]
$$
\Maximin_\IP(D) = \left \{ a \in D \mid (\forall a' \in D)\left [\min_{p \in \IP} \Exp_p[\U(a')] \leq \min_{p \in \IP} \Exp_p[\U(a)] \right ] \right \}
$$
(This should only be applied when these minima exist, e.g., when $\IP$ is a closed set.)
\end{definition}

So $\s$ picks for $\Maximin_\IP$ iff for every $D\in\D$, $\s(D)\in\Maximin_\IP(D)$. In this case, the only picking strategy that picks for $\Maximin_\IP$ must pick 1E from $D^{\mathrm{Ellsberg}}_1$ and 2F from $D^{\mathrm{Ellsberg}}_2$. We call this strategy $\s_E$; the strategy corresponding to the Ellsberg preferences. Such a strategy is incompatible with expected utility theory: it does not pick for $\EU_p$ for any $p\in\IP$.\footnote{\label{ftnte:Ellsberg not EU}This is because, to have $\Exp_p[\U(\text{1E)}]\geq \Exp_p[\U(\text{1F)}]$, it must be that $x\leq \sfrac{7}{30}$; and, to have  $\Exp_p[\U(\text{2F)}]\geq \Exp_p[\U(\text{2E)}]$, it must be that $x\geq \sfrac{13}{30}$; and these are jointly incompatible. } Indeed, this fact accounts for Ellsberg's use of the case: like Allais, he wished to provide an example of intuitively rational preferences that could not be captured by expected utility theory. 

Now we will use the theory itself to judge picking strategies. To do this, we need to describe the agent's uncertainty not only over what the world is like, but also over which decision problem she'll face. 
Suppose that in fact you've got precise probabilities over what decision you'll face, and you think it's $50\%$ likely you'll face $D^{\mathrm{Ellsberg}}_1$ and $50\%$ likely you'll face $D^{\mathrm{Ellsberg}}_2$. 
So we are representing your uncertainty as a set, $\IB$, of (higher-order) probabilities over both $\Omega$ and $\D$, each of which makes the state of the world independent of the decision you're faced with. 
%So we are considering $\IB=\{p\times \mu^*\given p\in\IP\}$, 
{That is, your credal set is given by $\IB=\{p\times\mu^* \given p\in\IP\}$, where $\mu^*$ is this probability over $\D$, and $\IP$ is the credal set as described in the Ellsberg case.}

Observe, then, that $\Exp_{\mu^*}\U(\s_{\text{1E,2F}})(\omega)=5$ for each $\omega$ in $\Set{\emph{Red}, \emph{Black}, \emph{Yellow}}$, but $\Exp_{\mu^*}\U(\s_{\text{1F,2E}})(\omega)=6$ for each $\omega$ in $\Set{\emph{Red}, \emph{Black}, \emph{Yellow}}$. So, for any $p$ with $p\times \mu^*\in \IB$,  $\EU_{p\times\mu^*}\U(\s_{\text{1E,2F}})=5$ and  $\EU_{p\times\mu^*}\U(\s_{\text{1F,2E}})=6$; so $\s_E=\s_{\text{1E,2F}}\notin\Maximin_{\IB}(\S)$. 


	This example is closely related to another phenomenon: Dutch book type challenges or paradoxes of sequential choice, which can be constructed against agents on the basis of such examples \citep{seidenfeld2004contrast,elga2010sp}. \todooldinfo{REFS!!!!}
	For instance, we might consider how the agent will choose in $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$, individually, and then combine these choices and observe that the result is dominated---1F-and-2E dominates 1E-and-2F. One response is simply to reject the package principle. However, there is another version of the examples in which they are presented diachronically: first evaluate $D^{\mathrm{Ellsberg}}_1$, then $D^{\mathrm{Ellsberg}}_2$. And again we can note that 1F-then-2E dominates 1E-then-2F. And we can note that $\Gamma$-Maximin would still have you choose 2E when faced with $D^{\mathrm{Ellsberg}}_2$, even if you know you've already chosen 1F when faced with $D^{\mathrm{Ellsberg}}_1$. But some will deny that sure loss as a result of decisions at different times indicates irrationality.
		What this example highlights is the close connection between the analysis of this paper and existing challenges and discussions for these theories. Any such Dutch book or sequential choice challenge can be seen as a particular instance where one is unsure which decision problem you'll be faced with, taking them each as equally likely, and evaluating strategies. However, it is a slightly different philosophical question.

%However, such examples can always be used to construct instances of undermining evaluations of strategies. Suppose $D^{\mathrm{Ellsberg}}_1, \ldots, D_k$ is a sequence of decision problems and your decision theory demands you pick $a_i$ from $D_i$; but suppose further than, if you were to pick $b_i$ from $D_i$ you'd be better off for sure: that is, the total utility of $a_1$-and-\ldots-and-$a_k$ is less than the total utility of $b_1$-and-\ldots-and-$b_k$  at every state of the world. Then, if you are equally confident you'll face each of $D^{\mathrm{Ellsberg}}_1, \ldots, D_n$, then, at each world, the expected utility of a strategy that picks $a_i$ when faced with $D_i$ is less than the expected utility of a strategy that picks $b_i$ when faced with $D_i$. 


%
%Now we will use the theory itself to judge picking strategies. To do this, we need to describe the agents uncertainty not only over what the world is like, but also which decision problem she'll be faced with. 
%We will still assume that you take which decision problem you're faced with to be independent of the state of the world, so we will represent your uncertainty with a set, $\IB$, of pairs of probabilities $p\times \mu$ where $p$ is a probability function over $\Omega$ and $\mu$ is a probability function over $\D$.
%Then we can ask, from the point of view of $\IB$, and using $\Gamma$-Maximin as our decision theory, whether the Ellsberg picking strategy $\s_E$ is permissible. That is, is $\s_E$ in $\Maximin_B(\S)$, where $\Maximin_B$ is the choice function to which $\Gamma$-Maximin gives rise when it is coupled with the credal set $\IB$ over $\Omega \times \D$, and $\Strategies$ is the set of possible picking strategies? 
%
%Suppose $\IB$ is such that for any $p\times \mu\in \IB$, $p\in\IP$, i.e., $p(\emph{Red}) = \sfrac{1}{3}\ \&\ p(\emph{Black})+p(\emph{Yellow}) = \sfrac{2}{3}$; 
%and $\mu$ assigns positive credence to facing both $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$. 
%Then the answer is no, $\s_E\notin\Maximin_B(\S)$. 
%To see this, we use the fact that $\Gamma$-Maximin is a more restrictive decision theory than E-Admissibility, so that any $\Gamma$-Maximin strategy must be optimal according to some probability in $\IB$ \todoold{add ref!!}. That is, there must be some $p\times \mu\in \IB$ for which $\s$ maximises $\EU_{p\times\mu}[\U(\s)]$; but as we saw from \cref{thm:eu-uniquely-optimal}, the only such strategies are those which $\mu$-surely pick for $\EU_p$ for some $p\times \mu\in \IB$. But, so long as $\mu$ gave non-zero credence to each of $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$, then $\s_E$ also does not $\mu$-surely pick for any $\EU_p$ with $p\in\IP$. \todoold{check this, and maybe prove it}
%
%So, at least in the Ellsberg case, where we are uncertain whether we'll face $D^{\mathrm{Ellsberg}}_1$ or $D^{\mathrm{Ellsberg}}_2$, $\Gamma$-Maximin is self-undermining. The only picking strategy that picks for that decision theory with $\IP$---namely, $\s_E$---is not permitted by that decision theory with $\IB$.


{
%\subsubsection{Uncertainty about decisions}
\label{sect:gamma:other mu}

Moreover, there is a further question of particular interest in our analysis which goes beyond showing the existence of cases of uncertainty for which the decision theory is undermining, as the Ellsberg case does, or using any instance of the sequential choice or Dutch Book challenges. As discussed in \cref{sect:reu:other mu}, we want a general result saying that for a wide class of ways of being uncertain about what decision you'll be faced with, $\Gamma$-Maximin is undermining.
% The example we have illustrated with the Ellsberg case, or more generally pointed to using any existing Dutch book or sequential choice challenge, merely gives some way of being uncertain over which decision problem you'll be faced with where $\Gamma$-Maximin undermines its own recommendations. 
%So far, we have only shown that in this particular way of being uncertain about what decisions you'll face, $\Gamma$-Maximin is self-undermining, declaring the only strategy that picks for it impermissible. As in our discussion in \cref{sect:reu:other mu}, we would like to be able to improve on the results by giving a more general result, which says that, for a wide class of ways of being uncertain, $\Gamma$-Maximin is similarly undermining. 

We are able to provide a general result for a wide class of ways of being uncertain over which decision problem you'll be faced with, although not for all. We will discuss the details of this in \cref{sect:nu:Max}. 

%\todooldinfo{check the link to the general stuff!!}

%Of course, if one's credal set is precise, then $\Gamma$-Maximin is just $\EU_p$, and we get self-recommendation. So the result will only hold when the credal set is imprecise. One also needs to have uncertainty about which decision problem you'll be faced with otherwise there's no
}
%\todooldinfo{work needs doing!}

%We can then calculate which acts maximize expected utility by the lights of a probability function in $\IP$ (\cref{tab:Ellsberg EU recommendations}).

%\begin{table}[ht]
%	\[
%	\begin{array}{lcc}
%		\toprule
%		\text{Constraint on } p(\emph{Yellow}) & \EU_p(D^{\mathrm{Ellsberg}}_1) & \EU_p(D^{\mathrm{Ellsberg}}_2) \\
%		\midrule
%		p(\emph{Yellow}) < \tfrac{7}{30} & \{1B'\} & \{2B\} \\[1mm]
%		p(\emph{Yellow}) = \tfrac{7}{30} & \{1B'\} & \{2A,\,2B\} \\[1mm]
%		\tfrac{7}{30} < p(\emph{Yellow}) < \tfrac{13}{30} & \{1B'\} & \{2A\} \\[1mm]
%		p(\emph{Yellow}) = \tfrac{13}{30} & \{1A,\,1B'\} & \{2A'\} \\[1mm]
%		p(\emph{Yellow}) > \tfrac{13}{30} & \{1A\} & \{2A'\} \\
%		\bottomrule
%	\end{array}
%	\]
%\caption{EU\(_p\) recommendations for decisions \(D^{\mathrm{Ellsberg}}_1\) and \(D^{\mathrm{Ellsberg}}_2\).\label{tab:Ellsberg EU recommendations}}

%\end{table}

%So, faced with $D^{\mathrm{Ellsberg}}_1$, either 1A or 1B is E-Admissible; and faced with $D^{\mathrm{Ellsberg}}_2$, either 2A or 2B is E-Admissible. And so any picking strategy picks for E-Admissibility in this case. However, the strategy that picks $1A$ from $D^{\mathrm{Ellsberg}}_1$ and $2B$ from $D^{\mathrm{Ellsberg}}_2$ picks for E-Admissibility, but it does not maximize expected utility by the lights of any $\langle p, \mu \rangle$ in $\IB$. That is, it is not in $\EAd_\IB(\S)$. 

%There are, however, strategies that pick for E-Admissibility and that E-Admissibility deems permissible: that is, $\s$ %picks for $\EAd_\IP$, and $\s$ is in $\EAd_\IB(\S)$. For example, the strategy that picks $1A$ from $D^{\mathrm{Ellsberg}}_1$ and $2A$ from $D^{\mathrm{Ellsberg}}_2$. 

%So E-admissibility deems some of its strategies permissible and others not. In fact, for every E-Admissible act in a fixed decision $D$, there is an E-Admissibility strategy which is E-Admissible and which selects that option in $D$. What might be ruled out, however, is certain combinations. For the strategy to be E-Admissible, it requires coordination across the various decision problems. 






%\begin{definition}[$\Gamma$-Maximin$_\IP$ ($\Maximin_\IP$) ] 
%$$
%\Maximin_\IP(D) = \left \{ a \in D \mid (\forall a' \in D)\left [\min_{p \in \IP} \Exp_p[\U(a)'] \leq \min_{p \in \IP} \Exp_p[\U(a)] \right ] \right \}
%$$
%(This should only be applied when these minima exist, e.g., when $\IP$ is a closed convex set.)\todoold{closed? closed and convex?}
%\end{definition}
%In the Ellsberg case as described above, this requires one to select $1A$ in $D^{\mathrm{Ellsberg}}_1$ and $2B$ in $D^{\mathrm{Ellsberg}}_2$; that is the only strategy recommended by $\Maximin_\IP$ is one that is not E-Admissible. It is thus also not itself acceptable according to Maximin. That is, there is a unique $\Maximin_\IP$ strategy $\s$, and this is not in $\Maximin_{\IB}(\S)$. Maximin undermines its own recommendations, evaluating its unique picking strategy to be impermissible. 

%\todooldinfo{Do we have a general result saying that for any $\mu$, Gamma-Maximin undermines all the compatible strategies?? RP: my sense is we can't get this, because there will be some string of decision problems whose $\min_{p \in P}$ is given by a single p in P.}

\subsection{E-Admissibility and Maximality}\label{sect:e-admiss}\label{sect:Max}

%Two alternative decision theories are E-Admissibility and Maximality. When coupled with a credal set $\IP$, E-Admissibility rejects an act $a$ from a decision problem $D$ when, for any $p$ in $\IP$, there is some $a'$ in $D$ that $p$ expects to do better than $a$, while Maximality rejects an act $a$ from $D$ when there is some $a'$ in $D$ that every $p$ in $\IP$ expects to do better than $a$. %that the E-Admissible acts, that is, those that are evaluated as optimal according to at least one probability function in the credal set, $\IP$. Maximality with $\IP$ permits That is:	%\todoold{move between options and actions??? }

{ Two alternative decision theories are E-Admissibility and Maximality. When coupled with a credal set $\IP$, E-Admissibility rejects an act $a$ from a decision problem $D$ when, for any $p$ in $\IP$, there is some $a'$ in $D$ that $p$ expects to do better than $a$. In that case, every $p$ in $\IP$ expects \emph{some} other option to be better than $a$, even though there may be no \emph{single} option that they all agree to be better. Maximality rejects an act $a$ from $D$ when there is some $a'$ in $D$ that every $p$ in $\IP$ expects to do better than $a$, \emph{i.e.}, when all $p$ in $\IP$ agree on a \emph{single} option which they expect to be better than $a$. If an act is rejected according to Maximality, then it is also rejected according to E-Admissibility, but not vice versa.}

\begin{definition}
 $$\EAd_\IP(D) = \Set{a \in D \given (\exists p \in \IP)(\forall a' \in D)(\Exp_p[\U(a)]\geq\Exp_p[\U(a')])}
	$$\end{definition}
	 $\s$ picks for $\EAd_\IP$ iff for every $D\in\D$, $\s(D)\in\EAd_\IP(D)$.
	

	
	\begin{definition}
	$$\Maximality_\IP(D) = \Set{a \in D \given (\forall a'\in D)(\exists p\in\IP)(\Exp_p[\U(a)] \geq\Exp_p[\U(a')])}$$
\end{definition}
	 $\s$ picks for $\Maximality_\IP$ iff for every $D\in\D$, $\s(D)\in\Maximality_\IP(D)$.


Let's treat E-Admissibility first. Suppose you are uncertain which decision problem you'll face. And suppose, as above, we represent your uncertainty over $\Omega \times \D$ with a credal set $\IB$. 
% {\color{orange}an set $\IB$ of probabilities over $\Omega\times\D$, each taking $\Omega$ and $\D$ to be probabilistically independent. This is a very strong notion of independence, called Strong Independence. We make use of it 
% }
 Then we can ask, from the point of view of $\IB$, and using E-Admissibility as our decision theory, whether a picking strategy that picks for $\EAd_\IP$ is permissible. And it turns out that, unlike for $\Gamma$-Maximin, there always is such a strategy. Indeed, if you simply take $p$ from $\IP$, and take a picking strategy $\s$ that picks for $\EU_p$, then $\s$ also picks for $\EAd_\IP$, and $\s$ is E-Admissible, as evaluated by $\IB$---that is, $\s$ is in $\EAd_\IB(\Strategies)$; at least if there is some $\pb=p\times\mu\in\IB$. The following is a corollary of \Cref{thm:eu-self-rec,thm:eu-uniquely-optimal}:
 \begin{proposition}\label{thm:ead-suff-indep}
 		If there is some $p\times \mu\in \IB$ such that $\s$ $\mu$-surely picks for $\EU_p$, then $\s\in\EAd_\IB(\Strategies)$. 
 	
 	Thus, if there is some $p\in\IP$ with some $\mu$ such that $p\times\mu\in \IB$, then there is some $\s$, namely any $\s$ which picks for $\EU_p$, which picks for $\EAd_\IP$ and which is in $\EAd_\IB(\Strategies)$. 
 \end{proposition}
 
 There are a number of conditions which guarantee the existence of some $p\times\mu\in\IB$ for any $p\in\IP$, and thus ensure that every E-Admissible action in a decision problem is part of a picking strategy which is E-Admissible.
 For example, suppose that you have no views whatsoever about which decisions you will face,  or the evidential value of information about which decisions you will face. 
 In that case, your credal set $\IB$ over $\Omega\times\D$ is given by the \emph{natural extension} of $\IP$ to this space, which is the largest (least informative) set of probabilities that extend the probabilities in $\IP$ to $\Omega \times \D$. 
 This is sufficient to guarantee that for every $p\in\IP$ there is some $\mu$ on $\D$ such that $p\times\mu\in \IB$.
 

 
Alternatively, suppose that you have a bit of information both about the world and which decision problem you will face. Your uncertainty about the world is given by the credal set $\IP$ over $\Omega$. Your uncertainty about the which decision you will face is given by the credal set $\ID$ over $\D$. Suppose also that you treat information about which decisions you will face as \emph{irrelevant} to which state of the world you are in. 

In the precise setting, irrelevance is a univocal, symmetric notion: for any joint distribution $\pb$ over $\Omega \times \D$, $\D$ is \emph{stochastically independent} of (and hence irrelevant to) $\Omega$ according to $\pb$ just in case $\pb(\omega\in A\given D\in E)=\pb(\omega\in A)$ whenever $\pb(D\in E)>0$.\footnote{For any $A\subseteq\Omega$, $\omega\in A:=\left\{\left<\omega,D\right>\in\Omega\times\D\given \omega\in A\right\}$. Likewise, for any $E\subseteq\D$, $D\in E:=\left\{\left<\omega,D\right>\in\Omega\times\D\given D\in E\right\}$.}
\todooldinfo{ftnte defining marginal again. Check consistency throughtou paper. } 
But in the imprecise setting, irrelevance fractures into a variety of distinct, not necessarily symmetric notions.\footnote{A short survey of independence notions for imprecise probability: complete independence for sets of probabilities (\cite{seidenfeld2007ci,cozman2012}); independence in selection for lower previsions (\cite{campos1995}); strong independence for lower previsions and sets of desirable gambles (\cite{cooman:2012:indnatexdesirs}); epistemic independence (value and subset) for sets of desirable gambles (\cite{Moral2005b}); epistemic h-independence for lower previsions and credal sets (\cite{debock2015:phdthesis}); S-independence for choice functions (\cite{debock2021:S-independence}).}

Consider a case where $\IP$ and $\ID$ are closed and convex and you treat $\D$ as \emph{epistemically irrelevant} to $\Omega$, in the sense of \cite{walley1991srip}. This means roughly that learning information about which decision problem you face does not change your maximum buy price for any ``worldly'' gamble, \emph{i.e.}, any gamble whose payout depends only on $\Omega$. Suppose that $\IP$, $\ID$ and this judgment of epistemic irrelevance jointly capture the totality of your views. In that case, your credal set $\IB$ over $\Omega \times \D$ is given by the \emph{irrelevant natural extension}  \citep[see][Thm 13]{cooman2012}%\todo{shouldn't it be `see'??}.
This is the largest (least informative) set $\IB$ of probabilities $\pb$ over $\Omega \times \D$ that marginalize to $\IP$ and $\ID$ and satisfy the following inequality constraints: for any gamble $g:\Omega\rightarrow\mathbb{R}$ and any $B\subseteq\D$ with $b(D\in B)>0$
\[
\inf\left\{\Exp_p[g]\given p\in\IP\right\}\leq\Exp_\pb[g^+]\leq\sup\left\{\Exp_p[g]\given p\in\IP\right\}
\]
and
\[
\inf\left\{\Exp_p[g]\given p\in\IP\right\}\leq\Exp_\pb[g^+\given D\in B]\leq\sup\left\{\Exp_p[g]\given p\in\IP\right\}
\]
where $g^+:\Omega\times\D\rightarrow\mathbb{R}$ is the ``cylindrical extension'' of $g$ defined by $g^+(\omega,D)=g(\omega)$ for all $\omega\in\Omega$ and $D\in\D$. As many authors have noted, individual probabilities $\pb$ in the irrelevant natural extension $\IB$ will not in general treat $\D$ as irrelevant to $\Omega$ (\emph{cf.} \cite[pp. 96-7]{debock2019iar}). Nonetheless, $\IB$ itself will do so, in the sense described above. Moreover, $\IB$ will contain any $\pb$ that treats $\D$ as stochastically independent of $\Omega$. This is sufficient to guarantee that the condition of \cref{thm:ead-suff-indep} holds.

Rather than treating $\D$ as epistemically irrelevant to $\Omega$, you might treat $\D$ and $\Omega$ as \emph{completely independent}, in the sense of \cite{seidenfeld2007ci,cozman2012}, \emph{i.e.}, $\D$ and $\Omega$ are stochastically independent according to every $\pb\in\IB$. This is a more stringent notion of irrelevance than epistemic irrelevance (and is also symmetric). If $\IP$ and $\ID$ capture your opinions about $\Omega$ and $\D$, respectively, you judge $\D$ and $\Omega$ as completely independent, and nothing more (this captures the totality of your views), then your credal set $\IB$ over $\Omega \times \D$ is the largest (least informative) set $\IB$ of probabilities $\pb$ over $\Omega \times \D$ that marginalize to $\IP$ and $\ID$ and satisfies completely independent, \emph{i.e.}, $\IB=\left\{p\times\mu\given p\in\IP, \mu\in\ID\right\}$. This is also sufficient to guarantee that the condition of \cref{thm:ead-suff-indep} holds.



%\footnote{More carefully, we suppose that $\IB$ is a set of probability measures $\pb$ over $\Omega\times\D$ which are absolutely continuous with respect to $u\times\lambda$, where $u$ is the uniform distribution on $\Omega$ and $\lambda$ is the restriction of the Lebesgue measure to $\D$. Let $f^\pb:\Omega\times\D\rightarrow\mathbb{R}_{\geq0}$ be a Radon-Nikodym derivative (or density) of $\pb$, so that for any measurable $A\subseteq\Omega\times\D$
%\[
%\pb(A)=\sum_{\omega\in\Omega}\int_{A_\omega} f^\pb(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)
%\]
%where, for any $\omega\in\Omega$, $A_\omega:=\left\{\left<\omega^*,D^*\right>\in A\given \omega=\omega^*\right\}$. To say that $\IB$ marginalizes to $\IP$ is just to say that \[
%\IP=\left\{\int_\D f^b(\cdot,D) \frac{1}{|\Omega|}\diff\lambda(D)\given b\in\IB\right\}
%\]
%}
 
 
%{\color{olive}\begin{proposition}\label{thm:ead-suff}
%	If $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ for some $\pb\in \IB$ then $\s\in\EAd_\IB(\Strategies)$. 
%	
%	Thus, there is some $\s$ which picks for $\EAd_{\IB(\cdot|-)}$ and which is in $\EAd_\IB(\S)$. 
%%	
%%	Thus, if there is some $p\in\IP$ with some $p\times\mu\in \IB$, then there is some $\s$, namely any $\s$ which picks for $\EU_p$, which picks for $\EAd_\IP$ and which is in $\EAd_\IB(\Strategies)$. 
% %		Suppose $\IB\restriction \Omega=\IP$ and making $\Omega$ and $\D$ independent.}
%		%If $p\in\IP$ with some $p\times \mu\in \IB$ and $\s$ picks for $\EU_p$, then $\s$ picks for $\EAd_\IP$ and $\s$ is in $\EAd_\IB(\S)$. 
%%		If $\s$ $\mu$-surely picks for $\EU_p$ for some $p\times\mu\in \IB$, then $\s$ is in $\EAd_\IB(\S)$.
%%		
%%		Suppose $p$ is in $\IP$ with some $p\times\mu\in \IB$. Then if $\s$ picks for $\EU_p()
%%		 then $\s$ picks for $\EAd_\IP$ and 
%%{	If $p\in\IP$ with some $p\times \mu\in \IB$ and $\s$ picks for $\EU_p$, then $\s$ picks for $\EAd_\IP$ and $\s$ is in $\EAd_\IB(\S)$. }
%%If $\s$ picks for $\EU_p$, for some $p\times \mu\in \IB$, then $\s$ picks for $\EAd_\IP$ and $\s$ is in $\EAd_\IB(\Strategies)$.\todooldinfo{Now I've fiddled with the independence, we can't quite say this!!! And same for Max }
%%\todooldinfo{is this a good way to write the thm?}
%\todooldinfo{JK: Observe that $p\times\mu$ is in the natural extension of $\IP$. So there is some joint distribution consistent with $\IP$ relative to which $\s$ picks for $\EAd_\IP$.}
%\end{proposition}
%}
The upshot is that E-Admissibility is not self-undermining in the same way that $\Gamma$-Maximin is self-undermining. So long as $\IP$ and $\IB$ are appropriately related,\todo{reread} then there are strategies that pick for it that it does not deem impermissible.


Do we obtain a converse to \cref{thm:ead-suff-indep}?  Are these the only E-Admissible strategies? \todooldinfo{I rewrote the start of this. JK to check. }
A strategy is E-Admissible iff there is some $\pb\in\IB$ which expects it to be optimal.
We might hope to be able to apply \cref{thm:eu-uniquely-optimal} to get that it is only these strategies which are E-Admissible. For this, we need to assume that every $\pb$ in $\IB$ has the form $p\times\mu$, i.e., that you treat $\D$ as completely irrelevant to $\Omega$:

%{\color{red} It is worth emphasising, though, that the strength of your irrelevance judgments (whether you make a judgment of epistemic irrelevance or complete independence, for example) makes a big difference to which strategies are permissible by the lights of E-Admissibility. If you treat $\D$ as completely irrelevant to $\Omega$, then we can use \cref{thm:eu-self-rec,thm:eu-uniquely-optimal} to get the following: 

\begin{proposition}\label{thm:ead-equiv[indep]} Suppose $\D$ and $\Omega$ are completely independent in $\IB$. Then every $\pb\in\IB$ has the form $p\times\mu$. In that case, $\s\in\EAd_\IB(\Strategies)$ iff there is some $p\times \mu\in\IB$ where $\s$ $\mu$-surely picks for $\EU_p$. 
%	If $\s$ does not $\mu$-surely pick for $\EU_p$, for any $p\times \mu\in \IB$, then $\s$ is not in $\EAd_\IB(\Strategies)$.
\end{proposition}

So the rather strong judgment of complete independence has rather strong implications for your views about picking strategies. The only strategies that are permissible by the lights of E-Admissibility in this case are ones that pick for expected utility theory, \emph{i.e.}, always pick options that maximize $p$-expected utility, for some $p\times \mu\in\IB$.



\begin{comment}
{\color{violet}
In fact, these are pretty much the only strategies that are permissible by the lights of E-Admissibility when coupled with $\IB$. Immediate from the definition of E-Admissibility, we have that a strategy is in $\EAd_\IB(\S)$ just if it is in $\EU_B(\S)$ for some $B\in\IB$. We can thus make use of \cref{thm:eu-self-rec,thm:eu-uniquely-optimal} to get the following, which applies when we have a strong independence. (In \cref{sect:EAd-dep} we will describe how this assumption can be dropped, but we first analyse it under this assumption.) 

\begin{proposition}\label{thm:ead-equiv[indep]}Suppose every $\pb\in\IB$ has the form $p\times\mu$.
	
	Then, $\s\in\EAd_\IB(\Strategies)$ iff $\s$ $\mu$-surely picks for $\EU_p$ for some $p\times \mu\in\IB$. 
%	If $\s$ does not $\mu$-surely pick for $\EU_p$, for any $p\times \mu\in \IB$, then $\s$ is not in $\EAd_\IB(\Strategies)$.
\end{proposition}}
\end{comment}



For example, in the Ellsberg case (\cref{sect:Ellsberg}), $\EAd_\IP(D^{\mathrm{Ellsberg}}_1)=\{\text{1E,1F}\}$ and $\EAd_\IP(D^{\mathrm{Ellsberg}}_2)=\{\text{2E,2F}\}$; so every strategy picks for $\EAd_\IP$. 
However, the $\s_{\text{1E,2F}}$ strategy, which is the empirically observed strategy, does not pick for any $\EU_p$: it is not rationalisable by expected utility theory.\footnote{See \cref{ftnte:Ellsberg not EU}.} 
If every $\pb\in\IB$ has the form $p\times\mu$ with each $\mu$ giving positive probability to facing both of the decisions in the Ellsberg case, then it also does not $\mu$-surely pick for $\EU_p$ for any $p\times\mu\in\IB$; and thus, is not in $\EAd_\IB(\S)$, despite picking for $\EAd_\IP$. However, the other strategies also pick for $\EAd_\IP$ and they pick for $\EU_p$ for some $p$,%\todo{is this phrasing used elswhere... strategies ``are EUp''???} 
and thus in $\EAd_\IB(\S)$, for example $\s_{\text{1E,2E}}$. 



This is a more general feature: there are always some strategies which pick for $\EAd_\IP$ but which are rejected by $\EAd_\IB(\Strategies)$, at least if $\EAd_\IP$ is not sufficiently precise, i.e., that it does not $\mu$-surely look like $\EU_p$, or a restriction thereof, for any $p\times\mu\in\IB$. 
\begin{proposition}\label{thm:ead-existsimpermissibel[indep]}

{Suppose $\D$ and $\Omega$ are completely independent in $\IB$. Then every $\pb\in\IB$ has the form $p\times\mu$. Suppose further that for every $p\times \mu\in \IB$, $\mu\{D\given \EAd_\IP(D)\subseteq \EU_p(D)\}\neq 1$. That is, for all $p\times \mu\in \IB$, $\mu\{D\given \text{there is $p'\in\IP$ with }\EU_{p'}(D)\not\subseteq\EU_p(D)\}>0$. }

%Suppose every $\pb\in\IB$ has the form $p\times\mu$.
	
	%Suppose that for every $p\times \mu\in \IB$, $\mu\{D\given \EAd_\IP(D)\subseteq \EU_p(D)\}\neq 1$. That is, for all $p\times \mu\in \IB$, $\mu\{D\given \text{there is $p'\in\IP$ with }\EU_{p'}(D)\not\subseteq\EU_p(D)\}>0$. 
	
	Then there is some $\s$ which picks for $\EAd_\IP$ but which is not in $\EAd_\IB(\Strategies)$.
\end{proposition}


%{\color{red}E-Admissibility rejects some picking strategies which are compatible with its own choice set. But in every decision problem you're faced with, those acts which are E-Admissible are exactly those that arise from E-Admissible picking strategies. What this reveals is that...}
%{\color{blue}
%	E-Admissibility rejects some picking strategies which are compatible with its own choice set. But in every decision problem you're faced with, those acts which are E-Admissible are exactly those that arise from E-Admissible picking strategies, unlike in \cref{sect:reu:phil-discussion}, although E-Admissible picking strategies require coordinating across the decision problems so that they could arise as expected utility strategies for some probability in one's credal set. 
%	This feels a bit like Consumer Bulletin advising you that it is fine to hire a personal shopper who invariably comes back with Consumer Bulletin best buys, but only if they are \emph{guaranteed} to pick Consumer Bulletin best buys that are also Consumer Reports best buys. This is not quite self-undermining, but close, you might think.
%}

 E-Admissibility rejects some picking strategies which are compatible with its own choice set. %\todo{this is repeating statement above the propn... is that ok?}
  \Cref{thm:ead-equiv[dep]} shows that E-Admissible picking strategies require coordinating across the decision problems so that they could arise as expected utility strategies for some probability in one's credal set. Still, in every decision problem you're faced with, those acts which are E-Admissible are exactly those that arise from E-Admissible picking strategies, unlike in \cref{sect:reu:phil-discussion}.



Does this make E-Admissibility self-undermining? Not exactly. But it does mean that, by their own lights, an E-Admissibility decision-maker must pick from their choice set \emph{as if} their credal set represented some true, precise probability which they are simply not in a position to identify. This is close to what \cite{levi99} referred to as \emph{imprecise} rather than \emph{indeterminate} probabilities. And it may not sit well with contemporary proponents of E-Admissibility.



%One might see this result as a problem for E-Admissibility. If every $\pb$ in $\IB$ agrees that a given picking strategy could \emph{possibly} saddle you with an E-Admissibile but non-$\EU$ option in \emph{some} decision problem or other, then E-Admissibility will reject that strategy. The only picking strategies it does not reject are ones that are certain, according to some $\pb$ in $\IB$, to yield an $\EU$ option in every decision problem. 
%	This feels a bit like Consumer Bulletin advising you that it is fine to hire a personal shopper who invariably comes back with Consumer Bulletin best buys, but only if they are \emph{guaranteed} to pick Consumer Bulletin best buys that are also Consumer Reports best buys. This is not quite self-undermining, but close, you might think.
	
	
	On the other hand, one might not see this as a concern for E-Admissibility. %Firstly, as \cref{thm:ead-equiv[dep]} makes clear, there are many picking strategies that E-Admissibility does not reject. In the Consumer Bulletin analogy, it is as if there are many other magazines that your personal shopper could cross-reference, not just Consumer Reports. Secondly, and more to the point, 
%{\color{red}What all of this shows, one might think, is just that E-Admissibility sees value in coordinating how you resolve incomparability.}
%{\color{violet}
It just shows that E-Admissibility sees value in coordinating how you resolve incomparability.
Take a simple example.
	
	
	$$
	\begin{array}{r|cc}
		D^{\mathrm{coord}}_1 & X & \neg X  \Bstrut \\\hline \Tstrut		p & x & 1-x \Bstrut \\\hline \hline\Tstrut 
		B & \text{\pounds 10} & \text{\pounds 10}   \\
		1C & \text{\pounds 0} & \text{\pounds 20} 
	\end{array}\hspace{20mm}
	\begin{array}{r|cc}
		D^{\mathrm{coord}}_2 & X & \neg X  \Bstrut \\\hline \Tstrut		p & x & 1-x \Bstrut \\\hline \hline\Tstrut 
		B & \text{\pounds 10} & \text{\pounds 10}   \\
		2C & \text{\pounds 20} & \text{\pounds 0} 
	\end{array}
	$$
	
	Any $\EU$-maximizer will coordinate their choices in $D^{\mathrm{coord}}_1$ and $D^{\mathrm{coord}}_2$ in the following sense: they will (assuming utility linear in GBP) choose $B$ (reject $1C$) in $D^{\mathrm{coord}}_1$ just in case they choose $2C$ (reject $B$) in $D^{\mathrm{coord}}_2$; likewise, they will choose $1C$ (reject $B$) in $D^{\mathrm{coord}}_1$ just in case they choose $B$ (reject $2C$) in $D^{\mathrm{coord}}_2$. 
% \todoold{unless $p=0.5$!!!}	
	
	Suppose $\IP = \{p_1, p_2\}$, where $p_1$ expects $B$ to be strictly better than $1C$, while $p_2$ expects $B$ to be strictly worse than $2C$. In that case, $\EAd_\IP(D^{\mathrm{coord}}_1)=\{B,1C\}$ and $\EAd_\IP(D^{\mathrm{coord}}_2)=\{B,2C\}$. You find both options in both options \emph{incomparable}, \emph{i.e.}, not rejected but also not indifferent, or equally good. Just as each of $p_1$ and $p_2$ coordinates their choices in $D^{\mathrm{coord}}_1$ and $D^{\mathrm{coord}}_2$, so too does E-Admissibility, advising you to coordinate how you resolve incomparability in a picking strategy. You ought to pick $B$ in $D^{\mathrm{coord}}_1$ just in case you pick $2C$ in $D^{\mathrm{coord}}_2$. Likewise, you ought to pick $1C$ in $D^{\mathrm{coord}}_1$ just in case you pick $B$ in $D^{\mathrm{coord}}_2$. 

	
	
	Now, you might doubt that there is \emph{really} any value in this sort of ``modal coordination.'' (Recall, you will actually only face one of $D^{\mathrm{coord}}_1$ or $D^{\mathrm{coord}}_2$. You are not coordinating across time.) But the fact that E-Admissibility \emph{sees} value in coordinating how you resolve incomparability does not render it self-undermining.
	
%	You might also be perplexed by the sheer quantity of strategies that pick for E-Admissibility but are nonetheless rejected by E-Admissibility. Here it is important to remember that picking for E-Admissibility is simply a matter of being guaranteed to select options that are \emph{not rejected} by E-Admissibility. The utility of an option, $\U(a)$, is a gamble on $\Omega$. The utility of picking strategy, in contrast, $\U(\s)$, is a gamble on $\Omega\times\D$---a larger, refined sample space. Reasons for rejection that are not visible at one scale, or level of resolution, might nonetheless become apparent at others. The fact that E-Admissibility identifies some reasons for rejection at the scale of picking strategies---reasons grounded in the (putative) value of coordination---does not conflict in any way with more pervasive non-rejection at the scale of actions or options, nor is it altogether surprising. 

Moreover, while E-Admissibility's lust for coordination does require decision-makers to \emph{pick as if} their probabilities were imprecise rather than indeterminate, in Levi's sense, this does not mean that they \emph{actually are} imprecise rather than indeterminate. Their credal set need not \emph{actually} represent some true, precise probability which they are unable to identify. This is reflected in their rejection judgments. They often find options genuinely incomparable---not rejected, but not indifferent. No agent with precise probabilities would do so. They are committed to \emph{picking as if} they have some true, precise probability because they value coordination in resolving incomparability.

The utility of an action, $\U(a)$, is a gamble on $\Omega$. The utility of picking strategy, in contrast, $\U(\s)$, is a gamble on $\Omega\times\D$---a larger, refined sample space. Reasons for rejection that apply to one scale, or level of resolution, might not apply at others. E-Admissibility identifies some reasons for rejection at the scale of picking strategies---reasons grounded in the (putative) value of coordination---that are not reasons for rejection at the scale of actions or options. 


%{\color{blue}E-Admissibility rejects some picking strategies which are compatible with its own choice set. But in every decision problem you're faced with, those acts which are E-Admissible are exactly those those that arise from E-Admissible picking strategies, unlike in \cref{sect:reu:phil-discussion}, although E-Admissible picking strategies require coordinating across the decision problems so that they could arise as expected utility strategies for some probability in one's credal set. }

%{\color{blue}What these show is that for a strategy to be evaluated as E-Admissible it requires coordination across different decision problems. For example, in the coord case, is fine for one's strategy to pick either act in $D^{\mathrm{coord}}_1$ so long as it picks the corresponding act in $D^{\mathrm{coord}}_2$. It does not permit arbitrary picking strategies which are compatible with 
%If one takes E-Admissibility as a choice function which actively deems any act to be permissible, then it is not clear how such coordination can be imposed. But 
%}
%The role of the picking strategies 
%Does this result in any conflicting guidance at the point of decision making, paralleling that of \cref{sect:reu:phil-discussion}? No, because any E-Admissible act is one that arises from an E-Admissible strategy.\
%However, if being E-Admissib
%
%It is not, however, clear how the coordination is to be imposed. If I'm actually facing a decision problem $D$, any E-Admissible act is not rejected, although were I to be facing an alternative $D'$ I should plan to pick correspondingly. Decision theory doesn't usually talk about this ``were I to be facing another decision problem''. 
%
%This can be seen as a justification for why we shouldn't deem acts that are not rejected to be actively permissible, as that seems to judge 
%If decision theories are understood simply as specifications of which acts are permissible in various decision problems, it is not clear how this coordination can be encoded. What does E-Admissibility deem permissible to pick in these decision problems? In the Ellsberg case, neither act is impermissible in $D^{\mathrm{Ellsberg}}_1$, so long as one would pick the corresponding act in $D^{\mathrm{Ellsberg}}_2$. This requires some additional structure in the decs
% unlike in the closely related, for example, paradoxes of sequential choice where one's selections can restrict future 
%It is fine to select either act when faced with $D^{\mathrm{Ellsberg}}_1$ so long as, were one to be faced with $D^{\mathrm{Ellsberg}}_2$ one would pick the corresponding action. This is not a way that decision theories are usually presented. Instead it is simply given as a specification of which options are impermissible in each decision problem, i.e., given by a choice function. What this shows us is that the decision theory has to encode more information. 
%Since we are evaluating strategies as a means of evaluating the decision theory, we might then ask how the decision theory encodes this requirement to coordinate, as they are usually presented simply as giving guidance in the individual decision problems. To build on this idea, we will change to using the decision theory to evaluate not the picking strategies but to evaluate the decision theory itself. 
\todooldinfo{sort out here!!!! I }

	\subsubsection{Maximality}
	
	
Since Maximality is a more permissive decision theory than E-Admissibility, \cref{thm:ead-suff-indep} entails:


 \begin{proposition}\label{thm:max-suff}
	If $\s$ $\mu$-surely picks for $\EU_p$ for some $p\times \mu\in \IB$ with $p\in\IP$, then $\s$ picks for $\Maximality_\IP$ and $\s$ is in $\Maximality_\IB(\Strategies)$.
%	
%	If $\s$ picks for $\EU_{\pb(\cdot|-)}$ for some $\pb\in\IB$, then $\s$ picks for $\Maximality_{\IB(\cdot|-)}$ and $\s\in \Maximality_\IB(\Strategies)$. 
	%and is not in $\EAd_\IB(\Strategies)$.
\end{proposition}

\begin{comment}
Since Maximality is a more permissive decision theory than E-Admissibility, \cref{thm:ead-equiv[indep]} entails:



\begin{proposition}\label{thm:max-suff}
	If $\s$ picks for $\EU_p$, for some $p$ in $\IP$ with some $p\times \mu\in \IB$, then $\s$ picks for $\Maximality_\IP$ and $\s$ is in $\Maximality_\IB(\Strategies)$.
%	
%	If $\s$ picks for $\EU_{\pb(\cdot|-)}$ for some $\pb\in\IB$, then $\s$ picks for $\Maximality_{\IB(\cdot|-)}$ and $\s\in \Maximality_\IB(\Strategies)$. 
	%and is not in $\EAd_\IB(\Strategies)$.
\end{proposition}
%Here, we extended maximality to apply to dependence in the analogous way: conditionalise the probabilities on which decision problem you're faced with. 
\end{comment}

And so, like E-Admissibility, Maximality is not self-undermining in the way that $\Gamma$-Maximin is self-undermining. There are always strategies that pick for it that it does not deem impermissible.

Unlike for E-Admissibility, we do not get the converse result (even under the assumption of complete independence). There can sometimes be some strategies which are not ruled out by Maximality but which nonetheless do not pick for any $\EU_p$. This is because a strategy is only ruled out as impermissible if there's a \emph{single} alternative which is preferable according to \emph{every} $\pb\in \IB$. 
This happens, for example, in the Ellsberg case if one's probability over which decision problem you think you'll face is sufficiently imprecise or if it's precise and pretty confident about which one you will face. 
%, as when $\mu$ does assign high enough confidence to facing $D^{\mathrm{Ellsberg}}_1$, then it evaluates any strategy as permissible. 

If, however, you think it's precise and equally likely that you'll face each of $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$, so your credal set is given by $\IB=\{p\times\mu^*\given p\in\IP\}$, where $\mu^*(D^{\mathrm{Ellsberg}}_1)=\mu^*(D^{\mathrm{Ellsberg}}_2)=0.5$, then as we observed in \cref{sect:gamma}, $\Exp_{\mu^*}\U(\s_{\text{1E,2F}})(\omega)=5$ for each $\omega$ and $\Exp_{\mu^*}\U(\s_{\text{2E,1F}})(\omega)=6$; so then for every probability $p$, $\Exp_{p\times\mu^*}\U(\s_{\text{1E,2F}})=5$ and $\Exp_{p\times\mu^*}\U(\s_{\text{2E,1F}})=6$, so  $\s_{\text{1E,2F}}\notin\Maximality_\IB(\Strategies)$. 

We will be able to show that if your credence over which decision problem you'll be faced with is precise, and also has a further property, that it requires almost everywhere decisiveness,\footnote{The example using the Ellsberg case does not have this property, which requires many decision problems to be possible. If, for example, we had selected $\mu^*(D^{\mathrm{Ellsberg}}_1)=0.1$, then one can check that no strategies are ruled out by Maximality.} then we will be able to show that the only strategies that Maximality does not rule as impermissible are the expected utility strategies. We will discuss this and give the details in \cref{sect:nu:Max}.

%We will be able to nonetheless get results for this case if we assume that you happen to be precise about which decision problem you'll be faced with, and this measure over the decisions has a particular property (requiring almost everywhere decisiveness); which we will discuss this in \cref{sect:nu:Max}.



	\subsubsection{Decision-State Dependence}\label{sect:EAd-dep}
To avoid the various independence assumptions that we employed in \cref{sect:e-admiss}, we will now generalize some of our earlier results to the decision-state dependent case, as we did in the precise setting in \cref{sect:decdep}. To do this, in any decision problem, we must bring $\IB$ up to speed on the problem that you face (by updating on that information via pointwise conditionalization), and then use the updated credal set to determine which options to reject. 
\todoold{We already talked about this earlier! }
%\todo{Do we need to say it's pintwise conditionalizaiton?}

%To avoid the strong independence we can present version of this result using the credal state over $\Omega\times\D$ which might allow for dependence. To do this, one must first conditionalise the probabilities by the decision problem you're faced with, and then select amongst the available ones. 

\begin{definition}
	$$\EAd_{\IB(\cdot|-)}(D) = \Set{a \in D \given (\exists \pb \in \IB)(\forall a' \in D)(\Exp_{\pb(\cdot|D)}[\U(a)]\geq\Exp_{\pb(\cdot|D)}[\U(a')])}
	$$\end{definition}
	That is, $\EAd_{\IB(\cdot|-)}(D)=\bigcup_{\pb\in\IB}\EU_{\pb(\cdot|-)}(D)$. We thus have, as a consequence of \cref{thm:eu-dep}: 
	\begin{proposition}\label{thm:ead-equiv[dep]}
		$\s\in\EAd_\IB(\Strategies)$ iff for some $\pb$ in $\IB$, $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$.
	\end{proposition}
	Thus there exists some $\s$ which picks for $\EAd_{\IB(\cdot|-)}$ which is not rejected by $\EAd_\IB(\Strategies)$; but there will also be some which are not, at least if $\EAd_{\IB(\cdot|-)}$ is not a restriction of some precise choice function. 
		\begin{proposition}\label{thm:ead-existence[dep]}
Suppose that for every $\pb\in\IB$, $\pb_\D\{D\given \EAd_{\IB(\cdot|-)}(D)\subseteq\EU_{\pb(\cdot|-)}\}<1$. That is, for all $\pb\in \IB$, $\pb_\D\{D\given \text{there is $\pb'\in\IB$ with }\EU_{\pb'(\cdot|-)}(D)\not\subseteq\EU_{\pb(\cdot|-)}(D)\}>0$. 
	
	Then there is some $\s$ which picks for $\EAd_{\IB(\cdot|-)}$ that is rejected by $\EAd_\IB(\Strategies)$.
%	Then there is some $\s$ which picks for $\EAd_\IP$ but which is not in $\EAd_\IB(\Strategies)$.\todooldinfo{This thm is new, check it and see if instead I should write it with IPxM}
\end{proposition}

Similarly, for Maximality, we can define: 
\begin{definition}
	$$\Maximality_{\IB(\cdot|-)}(D) = \Set{a \in D \given (\forall a' \in D)(\exists p\in\IP)(\Exp_{\pb(\cdot|D)}[\U(a)]\geq\Exp_{\pb(\cdot|D)}[\U(a')])}
	$$\end{definition}
	Since $\s\in\EAd_\IB(\S)$ implies $\s\in\Maximality_\IB(\S)$, we obtain, as a consequence of \cref{thm:ead-equiv[dep]}:
	\begin{proposition}\label{thm:max-suff[dep]}
	If for some $\pb$ in $\IB$, $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$, then $\s\in\Maximality_\IB(\Strategies)$. 
\end{proposition}And thus, there is always some $\s$ which picks for $\Maximality_{\IB(\cdot|-)}$ which is itself not rejected according to $\Maximality_\IB(\Strategies)$. However, again, we do not have %the converse result 
an analogue of \cref{thm:ead-existence[dep]} unless we impose additional particular restrictions on $\IB$ (\cref{sect:nu:Max}).





%{\color{red} DELETE???
%
%
%
%However, the analogous follow-up result requires some further assumptions. In particular, we need to assume that your credence over which decision{\color{orange} you'll be faced with is given by a single, \emph{precise} probability, $\mu^*$. That is, where $\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$. }And we place a further condition on $\mu^*$:
%\begin{definition}\label{def:suff spread}
%	$\mu^*$ \emph{requires almost everywhere decisiveness} iff for all probabilities $p$,$$\mu^*\Set{D\given \EU_p(D)\text{ is a singleton}}=1.$$ 
%\end{definition}
%That is, for each probability function $p$, $\mu^*$ is certain that you'll face a decision problem in which only one act maximizes expected utility. That is, the set of decision problems in which there are ties for expected utility has measure 0.
%%Just considering a single proposition, this will hold, for example, if you might be faced with various decision problems indexed by $t\in[0,1]$: are you willing to pay $\$t$ for a bet paying out $\$1$ if $p$ and $\$0$ if $\neg p$, where $\mu^*$ is a measure assigning strictly positive weight to every non-degenerate interval $[x,y]$. 
%%Suppose you have any coherent rejection function over $\Omega$. 
%
%%Wald gives us a very strong result linking being Bayes to be 
%
%Then we have the following result.
%\begin{proposition}\label{thm:max-nec}
%If $\mu^*$ requires almost everywhere decisiveness and countably additive, and $\s$ does not $\mu^*$-surely pick for $\EU_p$, for some $p$ in $\IP$, then $\s$ is not in $\Maximality_\IB(\Strategies)$.
%\end{proposition}
%This follows from a version of Wald's Complete Class Theorem, which shows that, if $\s$ does not $\mu^*$-surely pick for $\EU_p$, for some $p$ in $\IP$, then there is an alternative strategy $\s^*$ such that $\EU_{p\times\mu^*}[\U(\s)] < \EU_{p\times\mu^*}[\U(\s^*)]$, for each $p$ in $\IP$.
%}
%This will (somehow) result in a conflict with the notion of being a coherent rejection function, at least one which satisfies (a), (b) and (c). 


%\begin{corollary}
%	If $\s$ is not $\mu^*$-surely an $\EU_p$ strategy for any $p\in\IP$ then $\s\notin \Maximality_{\IP\times \{\mu^*\}}(\S)$.\todoold{is this legitimate way to write it??}
%\end{corollary}

%\todooldinfo{Qu for JK: can we get a version written for rejn fns generally instead of the credal set kind of version. }
%
%
%\todooldinfo{Do we have condition under which some Max strategies disagrees with EUp? }

%This is again similar to the case as for E-Admissibility: some Maximality strategies are Maximal (in particular, all the $\EU_p$ ones), whilst some are not (all the others). The difference to our discussion of E-Admissibility was the stronger assumption that we needed: that $\mu$ was in fact precise. 


%\subsection{The restriction to precise $\mu$}\label{sect:maximality:imprecise mu}

%Recall \cref{sect:reu:other mu} where we discussed how problematic our results were, since they only showed undermining given particular $\mu$. Something similar is happening here: we have only shown that they theory is undermining on particular ways of being uncertain over which decision she'll be faced with, in particular that she has precise probabilities, and they are uncertain over a whole range of decisions. This is a wide range of uncertainty and so we find the fact that it is undermining in these cases to be a worry. Of course the imprecise probability defender can say that she doesn't have precise probabilities over what decisions she'll be faced with, but nonetheless we feel it is a worry. 

%\todooldinfo{RP needs to look at this. How does the argument ehre differ from the REU one? }


\section{The utility of using a decision theory}\label{sect:nu}


%\todooldinfo{Both RP and JK to check this section [black]! I think I've got a proposal here, but would like to know if you think it's ok! I think the development of the options is right....?}
%\todooldinfo{I'm still actually unsure if this stuff should go underneath the maximality part. If so, they should all be subsections of a single IP section, not the way I've got here. That might make more sense anyway...}


%\begin{colored}{red}\todoold{get in there somewhere!}
%	The E-Admissibility challenge is very fundamental when viewed this way: however you evaluate $\U(C,D,\omega)$, each $p$ thinks it'd be best to be $\EU_p$, so must be $\EU_p$!
%\end{colored}

%
%\begin{colored}{red}
%	In this paper so far, we have asked how a decision theory evaluates the picking strategies that are compatible with what the theory recommends for each decision. But there are other things we could be doing within this topic to determine whether a decision theory undermines itself. 
%	
%	If our decision theory gives $C(D)$ containing various options, what should one do? 
%	
%	\begin{enumerate}
%		\item Evaluate the utility of the choice function itself. It should be guided by the idea of its value being what it leads you to. We should specify a numerical value for $\U(C,D,\omega)$, ensuring its between $\U(a,\omega)=a(\omega)$ for all $a\in C(D)$. 
%		\begin{itemize}
%			\item Clearly if $\U(C,D,\omega):=\sup\{a(\omega)\given a\in C(D)\}$ then it is indeed valuable to be imprecise. In fact, then one should be maximally imprecise in all decisions, $C(D)=D$. 
%			\item CAN WE MOTIVATE THE NU VERSION THIS WAY???\todooldinfo{???}
%		\end{itemize} 
%		\item 
%		\item We restricted attention to deterministic picking strategies, picking a unique action for each decision. Some indeterministic, or probabilistic, picking strategies could be considered. These randomise over the various options that are not ruled out. 
%		\item You can select a choice function $C$, as a free choice, but then the question is how valuable that is for you, what will that lead you to choose? you might simply have uncertain opinions about what you'll pick if you're using choice function $C$. You could be precise
%	\end{enumerate}
%\end{colored}
%
%
%
%\begin{colored}{red}
%	In this paper so far, we have asked how a decision theory evaluates the picking strategies that are compatible with what the theory recommends for each decision. This was supposed to be motivated by asking whether the decision theory undermines its own recommendations.
%	
%	
%But our original question was whether the decision theories undermine themselves, which would involve evaluating the recommendations of the decision theory themselves rather than any particular picking function compatible with those recommendations.
%A closely related original question was whether the decision theory thinks that some other decision theory will lead the agent to better get to her goals of love/money/utility, which would require us to 
%
%
%But our original question was whether the decision theory undermines itself. Perhaps we should try to apply the decision theory to itself rather than picking strategies compatible with its recommendations. 
%
%
%\end{colored}


Up to this point, we have asked how a decision theory evaluates the picking strategies that pick for the choice function to which that decision theory gives rise. This is one way to answer the question of whether the decision theory undermines its own recommendations, and we've seen that Allais-permitting decision theories fare poorly, as does $\Gamma$-Maximin both of which rule out as impermissible the strategy which they require; while E-Admissibility and Maximality fare better, as some compatible strategies are evaluated as acceptable (although not all). \todoold{check that}

However, other approaches are available too. We are interested in judging a decision theory as a means to your ends, and we have been using the proposed decision theory itself to do the judging, for it is, after all, a theory of which means to your ends are rational. Judging picking strategies that pick for the choice function that a decision theory produces furnish us with an straightforward approach to this question because they determine what the outcomes are: given a decision problem and a state of the world, the utility of a picking strategy is the utility, at that state of the world, of the act it picks from the decision problem. Since decisions theories don't always have definitive guidance on what to pick when faced with a decision, we considered various strategies compatible with its recommendations, which we called, the strategies which pick for it. 
%But all of the decision theories we've been considering, but especially E-Admissibility and Maximality, don't always tell you exactly what you should do when faced with a decision; instead they rule out various options as impermissible, and often leave more than one remaining. As a result, there are many different picking strategies that pick for them. 
How else might we evaluate what a decision theory will lead you to do when there are various strategies %\todo{should it be 'strategies'?}
it leaves open?

We propose that you might have a precise probability over those permissible acts---what we'll call a probabilistic picking strategy---and you might take the utility of this probabilistic picking strategy at a state of the world to be its expected utility at that world. There are two reasons you might think this is the right way to evaluate a decision theory: 

Firstly, you might think that, once your decision theory gives you its choice set, you will pick by applying some randomisation method, such as tossing a coin or rolling a die. Perhaps you think that we are freely selecting amongst various randomisation methods as well as the choice functions to which your decision theory gives rise, or perhaps you think that when selecting a choice function, it simply comes with a specified randomisation method. %This approach gives some pretty concrete guidance about how to turn a collection of available acts into a selection of what to do.% \todooldinfo{More about randomisation discussion??}


Secondly, you might think that, once your decision theory gives its choice set, you don't know what happens next except that, in the end you do in fact pick a particular act from that set. We then want to represent your uncertainty about how you'll end up picking when you've adopted a particular decision theory whose choice set is not a singleton. And it might just be that your uncertainty over how you'll pick is best represented by a precise probability. (In \cref{IPpicking}, we will extend this to the case where your uncertainty over how you'll pick is imprecise.)

%In either of these, what we have is for each choice function $C$ and decision $D$, a probability function $\prpickstrat^C_D$ over $C(D)$, the options from $D$ which are not ruled out by $C$ in this decision problem. We can then consider $\U(C,D,\omega)=\Exp_{\prpickstrat^C_D} a(\omega)$. 

%\todoold{I'd like your judgements on whether these are the things you were thinking of here.}

Before discussing some alternatives, we will now show that under either of these ways of thinking about judging the outcomes of adopting a decision theory, all our previous claims carry over, and in fact in some cases even get worse since the strategies that align with expected utility theory arise from extremal picking strategies which we might want to rule out under this way of thinking. 


%Either way, what we formally have, is, for each decision problem $D$ and picking strategy $C$, some probability $\prpickstrat$ over the options which are not ruled out by $C$. Then we have $\U(C,D,\omega)$ as $\Exp_{\prpickstrat^C_D}\U(a,D,\omega)$; which we think of as your expected utility of having adopted the choice function $C$. 




\subsection{Probabilistic picking strategies}


We begin by extending our definitions:
\begin{definition}\label{def:nu stuff}\ 
	\begin{itemize}
		%\item %A \emph{choice function}, $C$, specifies for each decision which acts are choiceworthy, or perhaps not rejected, that is $C:\D\to\A$ such that $C(D)\subseteq D$ and $C(D)\neq\emptyset$. Examples are $\EU_p$, $\REU_p^r$, $\EAd_\IP$ and $\Maximin_\IP$, as defined throughout the parper. 
%		\item A  \emph{(deterministic) $C$ picking strategy} specifies an action compatible with $C$ for each $D$, that is where $\s(D)\in C(D)$
		\item %\todoold{name??? ``picking strategy''? ``probabilistic picking strategy''? ``picking expectation''}
		A \emph{probabilistic picking strategy} $\prpickstrat$, specifies, for each decision problem, $D\in\D$, a probability function $\prpickstrat_D$ over $D$, i.e., over the acts available in the decision problem $D$.
		% $\prpickstrat$ is a function from $\D$ to probability functions over $\A$ which 
		\item For a choice function $\c$,  $\prpickstrat$ \emph{picks for $\c$} iff for all $D\in\D$,  $\prpickstrat_D(\c(D))=1$, i.e., it is certain that what it picks will be compatible with $C$'s recommendations.
		\item For a choice function $\c$, $\prpickstrat$ \emph{$\mu$-surely picks for $\c$}, if $\mu\{D\given \prpickstrat_D(\c(D))=1\}=1$.
 	\end{itemize}
\end{definition}
Observe that in the special case where $\prpickstrat$ is extremal---that is, when for every $D$ it assigns all its probabilistic weight to an individual member of $D$---then we recover our original notion of a picking strategy. We will call these the \emph{deterministic} picking strategies. 

We add a further definition which is relevant for probabilistic picking strategies: 
\begin{definition}
%{\color{red}For a choice function $\c$,  \emph{$\prpickstrat$ is regular for $\c$}, if, for each $D$ in $\D$: $\prpickstrat_D(a) > 0$ iff $a \in \c(D)$.}
%{\color{violet}
For a choice function $\c$,  \emph{$\prpickstrat$ is regular for $\c$}, if it picks for $\c$ and for every $D\in\D$ and $a\in\c(D)$, $\prpickstrat_D(a)>0$. %\todo{not sure if it was quantifier order, but I fould old defn hard to read. }}
\end{definition}


%We will apply the choice function to evaluate strategies, or more generally, $\prpickstrat$ too. \todooldinfo{JK to help!! is there a systematic thing to say about extending. See the Max section...???}

For a deterministic picking strategy, $\s$, we simply took its utility to be the utility of the act it requires you to pick: given a state of the world $\omega$ and a decision problem $D$, $\U(\s)(\omega,D):=\U(\s(D))(\omega)$, the utility of the act $\s(D)$ at $\omega$. For $\prpickstrat$, we take its utility to be the \emph{expected} utility of the act it lead you to pick: given a state of the world $\omega$ and a decision problem $D$, $$\U(\prpickstrat)(\omega,D):=\sum_{a \in D} \prpickstrat_D(a)\U(a)(\omega).$$
\begin{comment}
\begin{colored}{violet}
	If we are allowing $D$ to be infinite (although compact), we should in fact take an integral, $\U(\prpickstrat)(\omega,D):=\int_{a \in D} \U(a)(\omega)\,\prpickstrat_D(\diff a).$\todoold{should we explicitly talk about finitely additive integrals?}
%	All our results merely require that $\prpickstrat$ is finitely additive so in fact, we should be taking a Daniell integral, REFS.  $\U(\prpickstrat)(\omega,D):=\int_{a \in D} \U(a)(\omega)\,\prpickstrat_D(\diff a).$
\end{colored}
\end{comment}
% = \Exp_{\prpickstrat_D}[\U(\omega)].$$ %We then have
%$$\EU_{p\times\mu}[\U(\prpickstrat)]=\EU_{p\times\mu}\Exp_{\prpickstrat_D}[\U(\omega)]$$%\todooldinfo{I'm not sure about the right notation}


We have thus far been assuming that decision problems $D$ are non-empty finite sets of acts. If we were to allow $D$ to be infinite (although compact), then we should have $\U(\prpickstrat)(\omega,D):=\int_{D} \U(a)(\omega)\; \prpickstrat_D(\diff a)$.

In the next few sections, we note how our earlier results concerning deterministic
%\todo{did say ``non-probabilistic''}
picking strategies generalize to probabilistic picking strategies.
\subsubsection{Expected Utility Theory}
We will judge whether the decision theory evaluates a probabilistic picking strategy $\prpickstrat$ to be impermissible. This will depend on the range of alternatives available. That is, we will be judging whether $\prpickstrat$ is an impermissible picking strategy from a set of picking strategies, $\PrPickStrategies$. 
There are various natural proposals for what is considered in $\PrPickStrategies$ depending on one's interpretation and applications of our results. 
When $\PrPickStrategies$ consists just of extremal picking strategies, it is equivalent to the set of deterministic picking strategies, $\S$, which we considered in the first half of the paper. 
If you think you get to pick by randomisation, and can select any randomisation process, then $\PrPickStrategies$ will be the collection of all probabilistic picking strategies.
If you think we are just evaluating choice functions, and each one just comes along with a single randomisation process (for example, a uniform distribution over its choice set),%\todo{I added the `uniform'},
then $\PrPickStrategies$ will have a particular $\prpickstrat^\c$ for each $\c$, where $\prpickstrat^\c$ picks for $\c$. 
If you instead are just uncertain over how you'll pick when using a choice function $\c$, and assume that this is a matter governed by a precise probability, then again we'll have a $\prpickstrat^\c$ representing your probabilistic uncertainty over how you'll pick once you've selected a choice function $\c$ and are faced with a decision $D$.

Our results will all hold under various choices of $\PrPickStrategies$, so long as it has a particular feature: 
%We will just need a structural requirement on $\prpickstrategies$ for our results to hold, which will be satisfied in these various interpretations: 
\begin{definition}\label{def:EU-complete}
	A set of probabilistic picking strategies, $\PrPickStrategies$, is \emph{$\EU$-complete} if, for every probability $p$ over $\Omega$, there is some $\prpickstrat$ in $\PrPickStrategies$ such that $\prpickstrat$ picks for $\EU_p$.
	%	
	%A set of probabilistic picking strategies is \emph{$\EU$-complete (for $\IB$)} if, for each $p\times \mu\in \IB$, there is $\prpickstrat$ in $\prpickstrategies$ such that $\prpickstrat$ $\mu$-surely picks for $\EU_p$.
\end{definition}
%In fact, for our results we will only require that it is sufficiently $\EU$-complete for $\IB$, meaning that for each $p\times \mu\in \IB$ there is some $\prpickstrat\in\prpickstrategies$ such that $\prpickstrat$ $\mu$-surely picks for $\EU_p$. However, we will continue to use the general $\EU$-completeness for ease of exposition. 

In fact, for all the results, one only needs that $\PrPickStrategies$ is sufficiently $\EU$-complete in that it contains a strategy which is $\mu$-surely an $\EU_p$ strategy for relevant $p$ and $\mu$. We don't think that such weakenings are significantly interesting, so we simply impose $\EU$-completeness for ease, noticing that it applies both when $\PrPickStrategies$ is the collection of all deterministic strategies and when $\PrPickStrategies$ is the collection of all probabilistic picking strategies. 

\Cref{thm:eu-self-rec,thm:eu-uniquely-optimal} extend to this setting.
Suppose $p$ is a probability over $\Omega$ and $\mu$ is a probability measure over $\D$.
\begin{proposition}\label{thm:eu-nu-nec-suff}
If $\PrPickStrategies$ is $\EU$-complete, we have:
$$\prpickstrat \in \EU_{p\times \mu}(\PrPickStrategies) \Leftrightarrow \prpickstrat \text{ $\mu$-surely picks for $\EU_p$.}$$
and, more generally, 
$$\prpickstrat \in \EU_{\pb}(\PrPickStrategies) \Leftrightarrow \prpickstrat \text{ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$.}$$
\end{proposition}


\subsubsection{E-Admissibility}
\Cref{thm:ead-equiv[dep],thm:ead-existence[dep],thm:ead-suff-indep,thm:ead-existsimpermissibel[indep]} also generalise to the probabilistic picking strategy setting. 

%{\color{violet}For the remainder, we will assume that $\IB$ makes $\Omega$ and $\D$ strongly independent, i.e., every $B\in\IB$ has the form $p\times\mu$ for some probability $p$ over $\Omega$ and $\mu$ over $\D$.}

Since something is in $\EAd_\IB$ iff for some $\pb$ in $\IB$ it is in $\EU_\pb$, we get as an immediate consequence of \cref{thm:eu-nu-nec-suff}:
\begin{proposition}\label{thm:ead-nu-nec-suff}
If $\prpickstrat$ $\mu$-surely picks for $\EU_p$ for some $p\times \mu\in \IB$, then $\prpickstrat\in\EAd_\IB(\PrPickStrategies)$. More generally, if for some $\pb\in\IB$, $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$, then $\prpickstrat\in\EAd_\IB(\PrPickStrategies)$. 

Moreover, these are the only members of $\EAd_\IB$, at least assuming that $\PrPickStrategies$ is $\EU$-complete: 
$$\prpickstrat \in \EAd_\IB(\PrPickStrategies) \Leftrightarrow \text{for some $\pb$ in $\IB$, $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$.}$$
%If $\IB$ is strongly complete, then: $\prpickstrat \in \EAd_\IB(\prpickstrategies) \Leftrightarrow \prpickstrat \text{ $\mu$-surely picks for $\EU_p$, for some $p\times \mu$ in $\IB$.}
%\end{align}

%If $\prpickstrategies$ is $\EU$-complete, then $$\prpickstrat \in \EAd_\IB(\prpickstrategies) \Leftrightarrow \prpickstrat \text{ $\mu$-surely picks for $\EU_p$, for some $p\times \mu$ in $\IB$.}$$
\end{proposition}
%In fact, the former result does not depend on any features of $\IB$ such as the strong independence. 

%\begin{proposition}\label{thm:ead-nu-reg-nec}
%Suppose $\c$ is a choice function and, for every $p\times \mu\in \IB$,  $\mu\{D\given \c(D)\subseteq \EU_p(D)\}< 1$. Then, if $\prpickstrat$ is a regular picking strategy for $\c$, then $\prpickstrat\notin \EAd_\IB(\prpickstrategies)$. \todooldinfo{let's amend this to just being an EAd version! Do we know how this would look???}
%\end{proposition}

	\begin{proposition}\label{thm:ead-nu-reg-nec}
		Suppose that $\PrPickStrategies$ is $\EU$-complete. 
		
Suppose that for every $\pb\in\IB$, $\pb_\D\{D\given \EAd_{\IB(\cdot|-)}(D)\subseteq\EU_{\pb(\cdot|-)}\}<1$. That is, for all $\pb\in \IB$, $\pb_\D\{D\given \text{there is $\pb'\in\IB$ with }\EU_{\pb'(\cdot|-)}(D)\not\subseteq\EU_{\pb(\cdot|-)}(D)\}>0$. 
		
%	Suppose that for every $p\times \mu\in \IB$, $\mu\{D\given \EAd_\IP(D)\subseteq \EU_p(D)\}\neq 1$. That is, for all $p\times \mu\in \IB$, $\mu\{D\given \text{there is $p'\in\IP$ with }\EU_{p'}(D)\not\subseteq\EU_p(D)\}>0$. 
		
	Then, if $\prpickstrat$ is a regular picking strategy for $\EAd_{\IB(\cdot|-)}$, then $\prpickstrat\notin \EAd_\IB(\PrPickStrategies)$.
	\end{proposition}
	



%The antecedent of the final part of the theorem ensures that $\IP$ is actually imprecise and that you think you'll be faced with a decision where this matters. In which case, then the $\prpickstrat$ will not be an $\EU_p$ picking strategy for any $p\in\IP$, and moreover, will not be $\mu$-surely an $\EU_p$ picking strategy for any $p\times \mu\in \IB$. 


%Let's see the final part of this theorem at work in a very simple example. 
%Consider the Ellsberg setup, \cref{sect:Ellsberg} and suppose that you're in fact certain that you'll face decision problem $D^{\mathrm{Ellsberg}}_1=\{\text{1A, 1B}\}$. And you have a credal set $\{\}
%Suppose you're certain you'll face a decision problem $D = \{a_1, a_2\}$. So $\mu(D) = 1$. Suppose $\IP = \{p_1, p_2\}$, where $p_1$ expects $a_1$ to be strictly better than $a_2$, while $p_2$ expects $a_2$ to be strictly better than $a_1$. So E-Admissibility with $\IP$ says that both $a_1$ and $a_2$ are choiceworthy. Then for any picking strategy $\prpickstrat$ such that $\prpickstrat_D$ gives positive probability to $a_1$ and $a_2$, $p_1$ doesn't expect it to be best, and nor does $p_2$. So, by E-Admissibility, that picking strategy is not rationally permissible.


Let's see this at work. Consider again the Ellsberg setup, \cref{sect:Ellsberg}. Recall that $\EAd_\IP(D^{\mathrm{Ellsberg}}_1)=\{\text{1E,1F}\}$ and $\EAd_\IP(D^{\mathrm{Ellsberg}}_2)=\{\text{2E,2F}\}$. 
Every probability in $\IP$ rules out at least one of the E-Admissible options as impermissible. For example, any $p(\emph{Black})>\nicefrac{7}{30}$ rules 1E as excluded, i.e., not in $\EU_p$, but there's some positive chance that $\prpickstrat_{D^{\mathrm{Ellsberg}}_1}$ picks 1E, by the assumption that it is regular for $\EAd_\IP$. 
Thus, $\prpickstrat$ does not pick for $\EU_p$. It also does not even $\mu$-surely pick for $\EU_p$ if we assume that each $\mu$ assigns positive probability to both $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$. It is thus not E-Admissible. 

In fact, if $\IP$ is allowed to be non-convex, we get cases where $\prpickstrat$ will be judged as impermissible even when you know what decision problem you'll be faced with.\footnote{These examples are avoided when $\IP$ is convex as then there will be  a probability $p^*\in\IP$ which is indifferent between the two actions, and thus, $\prpickstrat\in\EU_{p^*}(\PrPickStrategies)$.} %\todo{ref: Bayes for p not defined. C: changed it.}
Suppose you're certain you'll face a decision problem $D = \{a_1, a_2\}$. So $\mu(D) = 1$. And suppose $\IP = \{p_1, p_2\}$, where $p_1$ expects $a_1$ to be strictly better than $a_2$, while $p_2$ expects $a_2$ to be strictly better than $a_1$. So E-Admissibility with $\IP$ says that neither $a_1$ or $a_2$ are rejected. Then for any regular picking strategy for $\EAd_\IP$, $\prpickstrat_D$ will give positive probability to both $a_1$ and $a_2$. $p_1$ doesn't expect it to be best, and nor does $p_2$. So, by E-Admissibility, $\prpickstrat$ is not rationally permissible.
\todoold{double check wording of above. And make it match JKs eg}


%And the same will happen for any $\IB$ such that, for each $\seq{p, \mu} \in \IB$, $\mu\{D\given \EAd_\IP(D)\subseteq \EU_p(D)\}< 1$.



\begin{comment}
So, at least when we consider regular probabilistic picking strategies, things have become pretty bad for E-Admissibility: given some reasonable constraints on your uncertainty about the decision problem you'll face, the regular probabilistic picking strategies that pick for E-Admissibility are not themselves E-Admissible. And so, in this sense, E-Admissibility is self-undermining.

{
	There is a more fundamental point here: however one is going to judge choice functions, each probability, $p$, will think that $\EU_p$ is optimal; and so E-Admissibility will only judge expected utility theory as acceptable. When we considered (deterministic) strategies, we were accepting $\EU$ strategies as E-Admissibility strategies, so had some E-Admissibility strategies evaluated as E-Admissible. However, if we are judging the choice function by assigning a single evaluation of it, taking the imprecision into account, it will distinguish it from expected utility theory,  and thus all probability functions will think something else is better, namely being $\EU_p$ for the $p$ doing the evaluation; and thus, E-Admissibility is not itself E-Admissible. 
	}

\end{comment}
%\todooldinfo{Was there more in v14b to be brought back??}



	
	
		One motivation for introducing probabilistic picking strategies, and in particular regular probabilistic picking strategies, was to judge an agent's decision theory as a means to her ends. We wished to give a particular judgement of how good it would be to adopt a given decision theory, rather than simply leaving open a whole range of picking strategies, which represent a range of different ways of implementing that theory.
	If this is how we are trying to judge E-Admissibility, then E-Admissibility is self-undermining.
	For example, if one picks amongst the non-rejected options by randomisation, with a regular randomisation device, then E-Admissibility deems it impermissible. 
	The defender of E-Admissibility will argue that one shouldn't select by randomisation, and also shouldn't have uncertainty over how one picks in a way which amounts to randomisation. 
	Instead, the defender of E-Admissibility will highlight that it sees value in coordinating how you resolve incomparability. Randomisation, or anything that amounts to that, just won't do. 
	%\todooldinfo{yes, this was JKs stuff. I am still spiritially trying to say what I had said. }
	
	The point is a general one.  If you try to give any way of scoring, or measuring the utility of choice functions or decision rules at each world and at each decision problem, then you can only avoid being ruled out as impermissible by the lights of E-Admissibility if your rule is equivalent to expected utility theory. 
% \todooldinfo{this new}


\subsubsection{Maximality}\label{sect:nu:Max}


As in \cref{sect:Max}, since Maximality is more permissive than E-Admissibility, all $\EU_p$ strategies are evaluated as acceptable according to Maximality. We thus have, as a corollary to \cref{thm:eu-nu-nec-suff}, and extending \cref{thm:max-suff}:
\begin{proposition}\label{thm:max-nu-suff}
		If for some $p\times\mu\in \IB$, $\prpickstrat$ $\mu$-surely picks for $\EU_{p}$, then $\prpickstrat\in\Maximality_\IB(\PrPickStrategies)$. 
		
		More generally, if for some $\pb\in \IB$, $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$, then $\prpickstrat\in\Maximality_\IB(\PrPickStrategies)$. 
\end{proposition}

For E-Admissibility, we were able to show that it was only these strategies which were judged by the decision theory as acceptable. This result does not immediately apply to Maximality in a similar way because for a strategy to be deemed impermissible, the various probabilities $\pb\in \IB$ have to agree on a particular alternative as better.

However, if we impose an additional restriction on $\IB$ we can obtain an analogous result: suppose your credence over which decision you'll be faced with is given by a single, \emph{precise} probability, $\mu^*$, and that $\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$. All our results equally apply when $\IB$ is the convex hull of this, as taking a convex hull doesn't affect which acts are rejected by Maximality. But since $\IP$ being convex implies that this $\{p\times\mu^*\given p\in\IP\}$ is convex (since $\mu^*$ is fixed), we do not bother with presenting this strengthening. 
\todoold{check!!!}


We will also place a further condition on $\mu^*$:
\begin{definition}\label{def:suff spread}
	$\mu^*$ \emph{requires almost everywhere decisiveness} iff for all probabilities $p$,$$\mu^*\Set{D\given \EU_p(D)\text{ is a singleton}}=1.$$ 
\end{definition}
That is, for each probability function $p$, $\mu^*$ is certain that you'll face a decision problem in which only one act maximizes expected utility. That is, the set of decision problems in which there are ties for expected utility has measure 0.
Just considering a single proposition, this will hold, for example, if you might be faced with various decision problems indexed by $t\in[0,1]$: are you willing to pay $\pounds t$ for a bet paying out $\pounds 1$ if $p$ and $\pounds 0$ if $\neg p$, where $\mu^*$ is a measure assigning strictly positive weight to every non-degenerate interval $[x,y]$. 
%Suppose you have any coherent rejection function over $\Omega$. 

%Wald gives us a very strong result linking being Bayes to be 

Then we have the following result.%
\footnote{{The assumptions on $\mu^*$ are not required if one instead assumes that $\PrPickStrategies$ is convex, which is motivated when one considers randomisations as available options. If $D$ was allowed to be infinite, one should also ensure that $\PrPickStrategies$ is closed, 
%		However, one should usually only apply these decision theories to compact sets, otherwise all options can be rejected, so we should ensure that $\prpickstrategies$ is also closed,
		 for which one needs to allow merely finitely additive randomisations \citep{schervish2020finite}.}}
\begin{proposition}\label{thm:max-nu}Suppose $\PrPickStrategies$ is $\EU$-complete. Suppose that 
	$\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$ and $\mu^*$ requires almost everywhere decisiveness.
	Then if $\prpickstrat \in \Maximality_\IB(\PrPickStrategies)$, then there is some probability $p$ such that $\prpickstrat$ $\mu^*$-surely picks for $\EU_p$.\todo{did we know that $p$ in $\IP$??}
	\todoold{I removed the converse because I don't know if the probability has to be in the original set, not sure if that follows from the Wald..???}
\end{proposition}
This follows from a version of Wald's Complete Class Theorem. We will be able to show that if is not in $\EU_{p\times\mu^*}$ for any $p$, then there is some alternative $\prpickstrat'$, in fact, an $\prpickstrat'$ which picks for some $\EU_p$, where $\Exp_{\mu^*}\U(\prpickstrat')(\omega)>\Exp_{\mu^*}\U(\prpickstrat)(\omega)$ for all $\omega$, and thus for all probabilistic $p$, $\Exp_p[\Exp_{\mu^*}\U(\prpickstrat')]>\Exp_p[\Exp_{\mu^*}\U(\prpickstrat)]$, i.e., $\Exp_{p\times \mu^*}\U(\prpickstrat')>\Exp_{p\times\mu^*}\U(\prpickstrat)$; and thus, $\prpickstrat\notin\Maximality_\IB(\PrPickStrategies)$. 
	
%	\todooldinfo{Either we incldue a ref to SSK's finite randomisation stuff or we drop the comment that $\prpickstrategies$ is convex. }
	
%	The assumptions on $\mu^*$ are not required if one instead assumes that $\prpickstrategies$ is convex, which is motivated when one considers randomisations as available options. However, one should usually only apply these decision theories to compact sets, otherwise all options can be rejected, so we should ensure that $\prpickstrategies$ is also closed, for which one needs to allow merely finitely additive randomisations. \todoold{ref to SSK ``what finadd''}
	

	
%	 If $\prpickstrat$ is not Bayes\todooldinfo{here!}
%	If $\prpickstrat$ does not $\mu^*$-surely pick for $\EU_p$ for any probability $p$, then by \cref{thm:eu-nu-nec-suff}, $\prpickstrat\notin \EU_{p\times\mu^*}(\prpickstrategies)$. We can then use a version of Wald's Complete Class Theorem to show that there is some $\prpickstrat'$ where $\Exp_{\mu^*}\U(\prpickstrat')(\omega)>\Exp_{\mu^*}\U(\prpickstrat)(\omega)$ for all $\omega$, and thus for all $p$, $\EU_{p\times\mu^*}\U(\prpickstrat')>\EU_{p\times\mu^*}\U(\prpickstrat)$, so $\prpickstrat\notin\Maximality_\IB(\prpickstrategies)$. 
%	What we can show is that if $\prpickstrat$ is such that there is no probability function with $\Exp_{p}\Exp_{\mu^*}\U(\prpickstrat)$
%	 which shows that non-Bayes options are dominated. We 
%%	Using this, we show that if $\prpickstrat$ is not Bayes for any 
%	if $\s$ does not $\mu^*$-surely pick for $\EU_p$, for some $p$ in $\IP$, then there is an alternative strategy $\s^*$ such that $\Exp_{\mu^*}[\U(\s)(\omega)] < \Exp_{\mu^*}[\U(\s^*)(\omega)]$, for each $\omega\in \Omega$; and thus also, $\Exp_{p\times\mu^*}[\U(\s)] < \Exp_{p\times \mu^*}[\U(\s^*)]$ for all $p\in\IP$ and thus $\s$ not in $\Maximality_\IB(\prpickstrategies)$. }

The deterministic picking strategies discussed in the earlier part of the paper, $\S$, form a class which is EU-complete as we assumed that all picking strategies were in it; and thus we obtain the result that we hinted at in \cref{sect:Max} that if we have such a $\IB$, then it is only $\EU_p$ strategies which are in $\Maximality_\IB(\S)$.



Just as we got a more challenging result for E-Admissibility when we restricted attention to \emph{regular} picking strategies, as these won't look like $\EU_p$ strategies, similarly we get a more challenging result for Maximality when we restrict to regular picking strategies because such regular picking strategies will not $\mu^*$ surely pick for any $\EU_p$, unless Maximality just collapses to being $\EU_p$ for some $p$, or at least $\mu^*$-surely does so.
%We get a more interesting challenge for Maximality, though, if we restrict attention to regular picking strategies. This is relevant when we are trying to use probabilistic picking strategies to evaluate the utility of using the decision theory. 

\begin{proposition}\label{thm:max-nu-reg-nec}
	Suppose $\PrPickStrategies$ is $\EU$-complete.  
Suppose that $\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$ and $\mu^*$ requires almost everywhere decisiveness.

	Suppose that for every probability $p$, $\mu^*\{D\given \Maximality_\IP(D)\subseteq\EU_p(D)\}< 1$.\footnote{Equivalently, that for every probability $p$ there is a non-negligible set of decision problems for which there is some $a\in D$ which is not an $\EU_p$ act but which is Maximal: $\mu^*\{D\given \exists a\in D\left[(\exists b{\in} D \, \Exp_{p}(a)<\Exp_{p}(b))\text{ and }\forall b{\in} D\, \exists p'{\in}\IP \,\Exp_{p'}(b)\leq \Exp_{p'}(a)\right]\}>0$}\todo{this might actaully need to say every p rather than every p in P, no??? It's supposed to say, it's not an EU strategy, so that we can use the fact that it does not mu surely pick for any EUp???!!!}
	
	Then, if $\prpickstrat$ is regular picking strategy for $\Maximality$ then $\prpickstrat\notin \Maximality_\IB(\PrPickStrategies)$.
\end{proposition}


	The lesson from this result is that you should not pick amongst the options that are not ruled out by applying a (regular) randomisation device, or have uncertainty over how you'll pick in a way which amounts to randomisation; (at least when your opinions over which decision problem you'll be faced with is precise and require almost everywhere decisiveness). 
	Just as with E-Admissibility, the defender of Maximality will argue that this is the right answer. In cases like this, you should coordinate how you resolve incomparability. The are reasons for rejection at the scale of picking strategies---reasons grounded in the (putative) value of coordination---that are not reasons for rejection at the scale of actions or options. 
	
	%	One's account of imprecise decision making has to say something more sophisticated about how one should pick.
\todoold{I rewrote this from JKs comment}
%{\color{red}The same sorts of considerations that potentially diffuse this challenge for E-Admissibility also diffuse it for Maximality.}



%Our core result is that if $\prpickstrat$

%{\color{violet}This isn't unique to Maximality. It also doesn't depend on any strong independence condition. What we actually show is that if $\prpickstrat$ is not Bayes, then there is $\prpickstrat'$ such that $\Exp_{\mu^*}\U(\prpickstrat)(\omega)<\Exp_{\mu^*}\U(\prpickstrat')(\omega)$ for all $\omega$. So if $\c$ is any choice function allowing some non-$\EU_p$ strategies, then any regular $\prpickstrat$ gives some positive probability to these non-$\EU_p$ options; which are thus not Bayes and thus dominated in expectation by some alternative. }
%
% It will hold for all {
%	\color{orange} manner of choice functions. 
%
%	For a choice function $\c$, let $\c_{\mu^*}$ be an extension of $\c$ to $\Omega\times \D$ which treats $\D$ as being governed by the precise probability $\mu^*$ and respects dominance, i.e., if $\Exp_{\mu^*}\U(\prpickstrat)(\omega)<\Exp_{\mu^*}\U(\prpickstrat')(\omega)$ for all $\omega\in\Omega$, then $\prpickstrat\notin \c_{\mu^*}(\prpickstrategies)$. 
%	}
%	\todooldinfo{Qu again for JK if this formulation is right}
%
%\begin{proposition}\label{thm:chiocefns-nu-reg-nec}	
%		Suppose $\prpickstrategies$ is $\EU$-complete.  
%	Suppose that $\mu^*$ requires almost everywhere decisiveness and is countably additive.
%	
%	Suppose $\c$ is a choice function and, for every $p\in\IP$, $\mu\{D\given \c(D)\subseteq\EU_p(D)\}< 1$. 
%	
%	Then, if $\prpickstrat$ is regular picking strategy for $\c$ then $\prpickstrat\notin \c_{\mu^*}(\prpickstrategies)$.\todoold{check this}
%\end{proposition}
%
%This will apply to $\Gamma$-Maximin:
%\begin{proposition}\label{thm:gamma}
%	Suppose $\prpickstrategies$ is $\EU$-complete.  
%Suppose that $\mu^*$ requires almost everywhere decisiveness and is countably additive.
%
%Suppose that for every $p\in\IP$, $\mu^*\{D\given \Maximin_\IP(D)\subseteq\EU_p(D)\}< 1$. 
%
%Then, if $\prpickstrat$ is regular picking strategy for $\Maximin_\IP$ then $\prpickstrat\notin \Maximality_\IB(\prpickstrategies)$, where 	$\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$. 
%\end{proposition}



Since $\Gamma$-Maximin is a more restrictive theory than Maximality, for a strategy to be $\Gamma$-Maximin there must be some probability function $p$  \todo{why in IP???}for which the strategy $\mu^*$ surely picks for
% \todo{eg here should we replace?? I did so... changed order of quantifier}
$\EU_p$, although such strategies may nonetheless be impermissible according to $\Gamma$-Maximin, as we saw, for example, in the Ellsberg case (\cref{sect:gamma}) where the only strategy compatible with $\Gamma$-Maximin was judged impermissible by the theory itself. 
	\begin{proposition}\label{thm:gamma-nu}Suppose $\PrPickStrategies$ is $\EU$-complete. Suppose that 
		$\IB$ has the form $\{p\times \mu^*\given p\in \IP\}$ and $\mu^*$ requires almost everywhere decisiveness. Then if $\prpickstrat \in \Maximin_\IB(\PrPickStrategies)$, there is some probability $p$ where $\prpickstrat$ $\mu^*$-surely picks for $\EU_p$.
		
		If also for every probability $p$, $\mu^*\{D\given \Maximin_\IP(D)\subseteq \EU_p(D)\}<1$, then there is some $\prpickstrat$ which picks for $\Maximin_\IP$ and which is not in $\Maximin_\IB(\PrPickStrategies)$ (in fact, any $\prpickstrat$ which picks for $\EU_p$ with $p\in\IP$ will do).  And if $\prpickstrat$ is regular for $\Maximin_\IP$ then $\prpickstrat\notin \Maximin_\IB(\PrPickStrategies)$. \todo{double check thms... I removed ``in IP''!}
	\end{proposition}
	
%	Unlike in the case for Maximality, though, we do not have the corresponding existence, and often those picking functions which $\mu$-surely pick for $\EU_p$ are nonetheless ruled impermissible by $\Gamma$-Maximin, as we saw, for example, in the Ellsberg case (\cref{sect:gamma}). 
	


%Since these results only assume that $\prpickstrategies$ is $\EU$-complete, it applies also when $\prpickstrategies$ is $\S$, the deterministic strategies discussed in the first half of the paper.\todoold{check if this is already stated elsewhere}

\todoold{The idea of N being closed and convex has now totally been deleted. }
\begin{comment}
	If you take $\prpickstrategies$ to consist of all probabilistic picking strategies, then we in fact do not need the requirement that $\mu^*$ requires almost everywhere decisiveness (or countable additivity). 
The conclusions of \cref{thm:max-nu,thm:max-nu-reg-nec} hold without the assumption that $\mu^*$ requires almost everywhere decisiveness or countably additive if we instead assume that $\prpickstrategies$ is convex, i.e., that it contains any (finite) mixtures of members of $\prpickstrategies$. 
\begin{proposition}Suppose that $\prpickstrategies$ is $\EU$-complete and convex.\todooldinfo{Actually we should be careful: if $\prpickstrategies$ is convex but not closed then it is non-compact and our decision theories aren't even defined for it!!!}
	
	Let $\IB$ have the form $\{p\times \mu^*\given p\in\IP\}$
	
	If for every $p\in\IP$, $\mu^*\{D\given \Maximality_\IP(D)\subseteq\EU_p(D)\}< 1$ and $\prpickstrat$ is a regular picking strategy for $\Maximality_\IP$, then $\prpickstrat\notin\Maximality_\IB(\prpickstrategies)$.
%	
%	And, more generally, 
%	If for every $p\in\IP$, $\mu^*\{D\given \c(D)\subseteq\EU_p(D)\}< 1$ and $\prpickstrat$ is a regular picking strategy for $\c$, then $\prpickstrat\notin\c_{\mu^*}(\prpickstrategies)$.
%	
%
\end{proposition}
\end{comment}

%We in fact also have a result that does not assume that $\mu^*$ requires almost everywhere decisiveness or countably additive, although we still do need the assumption that you think which decision you're faced with is captured with a precise probability. In this case, instead of assuming that $\prpickstrategies$ is $\EU$-complete, we assume that it is convex, that is, whenever it contains two picking strategies, it contains any mixture of them. 
%
%\begin{proposition}
%	The conclusions of \cref{thm:max-nu,thm:max-nu-reg-nec} hold without the assumption that $\mu^*$ requires almost everywhere decisiveness or countably additive if we instead assume that $\prpickstrategies$ is convex. 
%\end{proposition}
%
%This version of the result might be the most relevant if you conceive of picking strategies as combining choice rules and randomization procedures. That is, the choice rules narrow down the set of permissible acts, and then the picking strategies assign chances to those permissible acts. In this case, it's plausible that the set of randomization procedures you might be able to use is convex.



	\paragraph{Uncertainty about decisions}
	Unlike for the E-Admissibility case, we have had to assume something particular about your uncertainty concerning the decision you'll face: we assume you have precise probabilities over the possible decision problems, and those probabilities are broad enough to ensure that they require almost everywhere decisiveness. It is, however, quite general, applying in a much broader range of cases than particular uncertainty generated from specific cases like the Ellsberg or Allais cases (see \cref{sect:reu:other mu}).
	
%	There may be some Maximality picking strategies that do not amount to picking in accordance 
	
%	Like in \cref{sect:reu:other mu}, these results hold when we assume something about your uncertainty concerning the decision you'll face: we assume you have precise probabilities over the possible decision problems, and those probabilities are broad enough to ensure that they require almost everywhere decisiveness. 
	It seems troubling enough that, should you acquire sufficient evidence to become uncertain about the decision problems you'll face in a way that is represented by precise probabilities, you would have to abandon the decision theory or the picking strategy you're using. 


	
	\subsection{Alternatives}
%	We proposed using probabilistic picking strategies, $\prpickstrat$ in evaluating a decision theory. 
	
	\subsubsection{Imprecise picking strategies}\label{sect:IPpicking}
	So far, we've used decision theories to judge deterministic picking strategies, $\s$, and probabilistic picking strategies, $\prpickstrat$. 
	But perhaps the proponent of imprecise probabilities thinks the way you pick is better represented by imprecise probabilities, indeed, a set of probabilistic picking strategies. 
%	Perhaps, it is the set of all strategies that pick for the choice function, $\c$. 
	

	Since E-Admissibility requires coordinating, and rejects all strategies except for those which are equivalent to $\EU_p$ strategies, perhaps it is the set of all these $\EU_p$ strategies that E-Admissibility recommends. Can we apply E-Admissibility itself to judge this proposal? 
	
	
	To do this, we might extend E-Admissibility so that it judges what we might call imprecise acts, where we represent an imprecise act as a set of acts. So the imprecise acts available in decision problem $D$ is any $\mathbb{A}\subseteq D$. For instance, the set of all the $\EU_p$ strategies is such an imprecise act in the decision problem containing all the possible strategies.

	
%	A formally equivalent account arises if one thinks of assigning the utility of an imprecise choice function as the set of the utilities of all the compatible actions. 
%	This might also be proposed as an account of how to specify the utility of a choice function: it is an imprecise matter, the set of the utilities of all the options that are not ruled out. 


%		We can 
%	These are imprecise acts, and it is imprecise what utility they lead to. 
	%How might we apply our theories of imprecise probability to judge such imprecise acts, where it is imprecise what utility they lead to? 	We can understand an imprecise act as a set of acts. 
	For precise acts, E-Admissibility rejects an act when, for every $p$ in $\IP$, there is some alternative act $a'$ that $p$ expects to do better. When extending E-Admissibility to judge imprecise acts, we have to ask what it means for $p$ to expect an imprecise act $\mathbb{A}'$ to do better than $\mathbb{A}$. 
	
	A first suggestion is to say that $p$ expects $\mathbb{A}'$ to do better than $\mathbb{A}$ when, for every $a\in\mathbb{A}$ and $a'\in\mathbb{A}'$, $\Exp_p[\U(a')]>\Exp_p[\U(a)]$. This is a very hard condition to meet, so very few imprecise acts will be ruled out as impermissible on this basis. This can already rule as impermissible any imprecise picking strategy each of whose members is a regular picking strategy for $\EAd_\IP$, but it does not deem impermissible the imprecise act consisting of all picking strategies or all $\EU_p$ strategies, our motivating idea for imprecise picking strategies. 
	
	Alternatively, one might say that $p$ expects $\mathbb{A}'$ to do better than $\mathbb{A}$ when, for every $a\in\mathbb{A}$ and $a'\in\mathbb{A}'$, $\Exp_p[\U(a')]\geq\Exp_p[\U(a)]$, and there is some $a\in\mathbb{A}$ such that for all $a'\in\mathbb{A}'$, $\Exp_p[\U(a')]>\Exp_p[\U(a)]$.\footnote{Or perhaps this second disjunct should say that there is some $a\in\mathbb{A}$ and $a'\in\mathbb{A}'$ with $\Exp_p[\U(a')]>\Exp_p[\U(a)]$, but since our application of interest satisfies the slightly stronger property, we merely impose that. }
	This condition generates a version of E-Admissibility for imprecise acts which rules as impermissible the set of all picking strategies for $\EAd_\IP$. 
	
	Using this criterion, we rule out the imprecise picking strategy which consists of the set of all $\EU_p$ strategies, unless they all $\mu$-surely pick for $\EU_p$, for a single $\mu\times p\in\IB$. 
%	Using this, we rule out any imprecise picking strategy whose members do not all $\mu$-surely pick for $\EU_p$, for a single $p\times\mu\in\IB$.
%	Using this, we get a result paralleling that for precise picking strategies: to avoid being ruled out as impermissible, one's strategy needs to $\mu$-surely be an $\EU_p$ strategy for some $p\times\mu\in\IB$. (Or equivalently for the conditional version.)
	Suppose $\imprecpickstrat$ is a set of picking strategies, and there is no $p \times \mu$ in $\IB$ such that all strategies in $\imprecpickstrat$ $\mu$-surely pick for $\EU_p$; then each probability $\pb=p\times \mu$ evaluates the precise picking strategy $\{\prpickstrat^p\}$, where $\prpickstrat^p$ picks for $\EU_p$, to be better, in this sense: for every $\prpickstrat\in\imprecpickstrat$, $\Exp_p[\U(\prpickstrat)]\leq \Exp_p[\U(\prpickstrat^p)]$, and there is some $\prpickstrat\in\imprecpickstrat$ with $\Exp_p[\U(\prpickstrat)]<\Exp_p[\U(\prpickstrat^p)]$. 
%	 
%	 	It is, however, already strong enough to rule out an imprecise picking strategy for $\EAd_\IP$ which is regular, i.e., where every $\prpickstrat\in\imprecpickstrat$ is a regular picking strategy for $\EAd_\IP$. This is because, for each $\prpickstrat\in\imprecpickstrat$, $\Exp_{p\times\mu}[\U(\prpickstrat)]\leq\Exp_{p\times\mu}[\U(\prpickstrat^p)]$, where $\prpickstrat^p$ picks for $\EU_p$, and there is some $\
%	 	and thus, $\pb=p\times\mu$ evaluates the precise picking strategy $\{\prpickstrat^p\}$ to be better, in this sense; so $\imprecpickstrat$ is deemed impermissible by this extension of E-Admissibility.  
	
	%\todooldinfo{check that again}



%	These are imprecise acts, and it is imprecise what utility they lead to. How might we apply our theories of imprecise probability to judge such acts?
%	
%	We understand an imprecise act as a set of precise acts. So the imprecise acts available in decision problem $D$ is any $\mathbb{A}\subseteq D$. Suppose that $\mathbb{A}$ has the following feature: for every probability $p$ in one's credal set $\IP$, there is some alternative $\mathbb{A}'$, such that $\Exp_p[\U(a)]<\Exp_p[\U(a')]$ for all $a\in\mathbb{A}$ and $a'\in\mathbb{A}'$. Then, we propose that an extension of E-Admissibility to judge imprecise acts would deem $\mathbb{A}$ as impermissible. This is a very hard condition to meet, so only a few imprecise acts are deemed impermissible on this basis.
%	
%	It is already strong enough to rule out any set of regular probabilistic picking strategies for $\EAd_\IP$. 
%%	In our case, a set of probabilistic picking strategies, $\imprecpickstrat$, is not deemed impermissible in this way iff there exists some $\prpickstrat\in\imprecpickstrat$ which $\mu$-surely picks for some $\EU_p$, for some $p \times \mu$ in $\IB$. But this is already strong enough to rule out any imprecise picking strategies which only contain regular probabilistic picking strategies. 
%	This is because each probability $p\times\mu$ in $\IB$ will evaluate every such regular $\prpickstrat\in\imprecpickstrat$ to be worse than a $\EU_p$ strategy, $\prpickstrat^p$ (by \cref{thm:ead-nu-reg-nec}). So for each $\pb$ in $\IB$, there is an alternative $\imprecpickstrat_p=\{\prpickstrat^p\}$, where $\prpickstrat^p$ picks for $\EU_p$ which is preferable. 
%	
%	However, we won't apply when we try to judge E-Admissibility by looking at the set of all picking strategies compatible with its recommendations, as some non-regular strategies are compatible, which is a natural application of the idea of imprecise picking strategies. 
%	
%	We can offer a slight strengthening, which we feel is still in the spirit of E-Admissibility. If $\mathbb{A}$ has the following feature, then it should be judged as impermissible: for every probability $p$ in $\IP$, there is some $\mathbb{A}'$ such that for all $a\in\mathbb{A}$ and $a'\in\mathbb{A}'$, $\Exp_p[\U(a)]\leq\Exp_p[\U(a')]$ and there exists some $a\in\mathbb{A}$ such that for all $a'\in\mathbb{A}'$, $\Exp_p[\U(a)]<\Exp_p[\U(a')]$. 
%	
%	With this strengthening, we get a result paralleling that for precise picking strategies: to avoid being ruled out as impermissible, one's strategy needs to $\mu$-surely be an $\EU_p$ strategy for some $\mu\times p\in\IB$. 
%%	
%	If $\imprecpickstrat$ is a set of picking strategies, and there is no $p \times \mu$ in $\IB$ such that all strategies in $\imprecpickstrat$ $\mu$-surely pick for $\EU_p$, then $\imprecpickstrat$ is judged  impermissible by this slightly strengthened account. This is because each probability $p\times \mu$ evaluates the precise picking strategy $\{\prpickstrat^p\}$, where $\prpickstrat^p$ picks for $\EU_p$, to weakly dominate it, i.e., every $\prpickstrat\in\imprecpickstrat$, $\Exp_p[\U(\prpickstrat)]\leq \Exp_p[\U(\prpickstrat^p)]$, and there is some $\prpickstrat\in\imprecpickstrat$ with $\Exp_p[\U(\prpickstrat)]<\Exp_p[\U(\prpickstrat^p)]$. 
%	
%%	So, if one extends E-Admissibility to judge imprecise acts in the proposed way then we get a result paralleling that in for precise probabilistic picking strategies: it must essentially be an $\EU_p$ strategy. The only E-Admissible imprecise picking strategies are those that are just a set of strategies all of which $\mu$-surely pick for $\EU_p$, for a single $p\times\mu\in\IB$. 
%	
%%	We would not expect to be able to obtain similar results for Maximality
%%	We would not expect the analogue results to apply for Maximality because our results show for each $\prpickstrat$ there's a $\prpickstrat'$ which is better according to every probability, but there's no guarantee of a single $\prpickstrat'$ which is better than all the $\prpickstrat\in\imprecpickstrat$. 
%
%%We would not expect to obtain similar results for Maximality. Even if we assume that one's probability over which decision she'll be faced with is precise, we don't have 
%%We would not expect to obtain similar challenges for extending Maximality, as we can only show that each $\prpickstrat$ has a $\prpickstrat'$ which is preferable, but there's no guarantee. 
We should not expect to obtain similar results for Maximality.
Consider an imprecise picking strategy consisting of the set of all compatible picking strategies, or even all those which are coordinated and pick for $\EU_p$ for some $p$ in one's credal set $\IP$. Each probability in $\IP$ agrees that there is a precise picking strategy that is better, but they disagree about which this precise picking strategy is: for which $p$ should it pick for $\EU_p$?
Maximality thus avoids the charge of evaluating the imprecise picking strategy as impermissible. There can be imprecise picking strategies that are Maximal and which do not all pick for a single $\EU_p$, unlike for E-Admissibility.


%We nonetheless are unsure about the 

%When we assume that one's probability over which decision she'll be faced with is precise, we showed that for every $\prpickstrat$ which is not $\mu^*$-surely an $\EU_p$ strategy we get some $\prpickstrat'$ which every probability $p$ agrees is preferable. But we can't guarantee that given a set of $\prpickstrat$s, $\imprecpickstrat$, we get any agreement on which $\prpickstrat'$ are preferable.


	
	
%	{\color{green}\todoold{RP: added this as a very rough summary of what CCM and I discussed at the whiteboard yesterday.}How might we evaluate them? Suppose $\Uset$ is a utility function that takes an act $a$ and returns not a single utility profile $\U = \langle u_1, \ldots, u_n\rangle$, where $u_i$ gives the utility of $a$ at state $\omega_i$, but a set of such utility profiles. How might we state E-Admissibility for such a conception of utility? Given a credal set $\IP$, we want to say that an act $a$ in a decision problem $D$ is rejected or impermissible if, for every probability $p$ in $\IP$, there is $a'$ in $D$ such that $p$ deems $a'$ better than $a$. But what do we mean when we say $p$ deems $a'$ better than $a$, when the utilities of both acts are represented by sets of utility profiles? One very permissive proposal---that is, a proposal that will lead E-Admissibility more rarely to reject acts---is that $p$ deems $a'$ better than $a$ when it deems $a'$ determinately better than $a$, that is, when for all utility profiles $\U$ in $\Uset(a)$ and all utility profiles $\U'$ in $\Uset(a')$, $\Exp_p[\U'] > \Exp_p[\U]$. In this case, we write $a' \succ_{p, \Uset} a$. So
%	$$\EAd_\IP(D) = \Set{a \in D \given (\exists p \in \IP)(\forall a' \in D)[a' \not \succ_{p, \Uset} a']}$$ 
%	Let's now apply this understanding to our case. Let $\PowN$ be the set of all sets of probabilistic picking strategies, and let $\prpickstrategies$ be one such set of these strategies. Then $\prpickstrategies$ is in $\EAd_\IB(\PowN)$ iff there is $\prpickstrat$ in $\prpickstrategies$ that $\mu$-surely picks for $\EU_p$. After all, in that case, for any alternative $\prpickstrategies'$ and any $\prpickstrat'$ in $\prpickstrategies'$, $\Exp_{p\times \mu}[\prpickstrat] \geq \Exp_{p\times \mu}[\prpickstrat']$. So, by \cref{thm:ead-nu-reg-nec}, if $\prpickstrategies$ contains only regular probabilistic picking strategies that pick for $\EAd_\IP$, then $\prpickstrategies$ is not in $\EAd_\IB(\PowN)$.
%	What of Maximality? Here the natural version says that an act $a$ in a decision problem $D$ should be rejected if there is $a'$ in $D$ that every $p$ in $\IP$ deems better than $a$. That is:
%	$$\Maximality_\IP(D) = \Set{a \in D \given (\forall a' \in D)(\exists p \in \IP)[a' \not \succ_{p, \Uset} a']}$$
%	In this case, we don't get the self-undermining result we have for E-Admissibility. For some sets $\prpickstrategies$ of probabilistic picking strategies that pick for $\Maximality_\IP$, there will be no alternative set $\prpickstrategies'$ that is deemed determinately better by the lights of each probability function in $\IB$.\todoold{RP: do we know this is true?}
%	}
%	


	So, whilst this approach is a challenge for E-Admissibility, it is an option for Maximality.  
	So, if picking strategies may be imprecise acts, represented by sets of precise probabilistic picking strategies, then Maximality sometimes evades the charge of being self-undermining. However, it is hard to see how to implement an imprecise picking strategy. Perhaps one could choose by tossing a coin about whose bias you have imprecise probabilities, or perhaps one should just have imprecise probabilistic uncertainty about how one will pick. But to maintain this response, one had better not gain additional information sufficient to make one precise. 
%	what mechanism one could use for picking between options your choice function does not reject that doesn't amount to randomisation. Moreover, 
	

%\todooldinfo{I deleted the Max response! Because I no longer get it...!!!}
	
	
	
%	
%\subsection{Imprecise probabilistic picking strategies}\label{sect:imprecise nu}
%
%
%\todoold{check this sec again}
%	
%So far, we've used decision theories to judge deterministic picking strategies, $\s$ and probabilistic picking strategies, $\prpickstrat$. 
%%So far, we've asked what decision theories say about the deterministic or non-probabilistic picking strategies that pick for their choice functions, and what they say about the probabilistic picking strategies that do so. 
%But perhaps the proponent of imprecise probabilities thinks the way you pick is better represented by a imprecise probabilities, indeed, a set of probabilistic picking strategies. What do E-Admissibility and Maximality then say about these?
%
%The first thing to do is to define the utility of a set of probabilistic picking strategies. Given a state of the world $\omega$ and a decision problem $\D$, what is the utility of a set $\prpickstrategies$ of such strategies? The natural thing is to say that the utility is itself imprecise. That is, it is the utility of $\prpickstrategies$ is represented by the following set of utility functions $\Uset_\prpickstrategies = \Set{\U(\prpickstrat) \given \prpickstrat \in \prpickstrategies}$.
%
%The problem with this is that, as we've formulated them, E-Admissibility and Maximality are defined for precise utility functions. However, it's straightforward to extend them to this case.
%
%\hspace{5mm} $\EAd_{\IP,\Uset}(D) =  \Set{a \in D \given (\exists p \in \IP, \U \in \Uset)(\forall a' \in D)(\Exp_p[\U(a)]\geq\Exp_p[\U(a')])}$
%%
%%That is $a\in\EAd_{\IP,\Uset}(D)$ iff $a\in\EU_\IP(D)$ for some $\U$
%
%\hspace{5mm}  $\Maximality_{\IP, \Uset}(D) = \Set{a \in D \given (\forall a'\in D)(\exists p\in\IP, \U \in \Uset)(\Exp_p[\U(a)] \geq\Exp_p[\U(a')])}$
%
%In this context, we can extend the results about E-Admissibility. Applying our definition, we have that $\prpickstrategies\in\EAd_\IB(\PowN)$ iff there is some $\prpickstrat\in\prpickstrategies$ and $p\times \mu\in \IB$ such that 
%
%
%Let $\PowN$ be a set of sets of picking strategies which is either the set of all deterministic picking strategies, or the set of all probabilistic picking strategies. 
%\begin{enumerate}[{\normalfont (i)}]
%\item A set $\prpickstrategies$ of probabilistic picking strategies is in $\EAd_\IB(\PowN)$ iff there is $\langle p, \mu \rangle$ in $\IB$ and $\prpickstrat$ in $\prpickstrategies$ such that $\prpickstrat$ $\mu$-surely picks for $\EU_p$. (This is an extension of \cref{thm:ead-nu-nec-suff}.)
%\item Suppose each $\prpickstrat$ in $\prpickstrategies$ is regular. Then, by \cref{thm:ead-nu-reg-nec} there is no $\prpickstrat$ which is $\prpickstrategies$ is not in $\EAd_\IB(\PowN)$, where $\IB$ is as in that theorem.
%%\item Suppose there is some $\prpickstrat$ which does not $\mu$-surely pick for $\EU_p$ for any $p\times \mu\in \IB$, then 
%%
%%\item Suppose each $\prpickstrat$ in $\prpickstrategies$ is (a) regular and (b) $\mu^*$-surely picks for $\EAd_\IP$; where $\mu^*$ is precise, requires almost everywhere decisiveness, and countably additive, then $\prpickstrategies$ is not in $\EAd_\IB(\PowN)$. 
%%(This is a corollary of \cref{thm:ead-nu-reg-nec}.)
%%\item Suppose that $N$ 
%\end{enumerate}
%
% However, we don't get any result analogous to \cref{thm:max-nu,thm:max-nu-reg-nec}. In those theorems, we showed that, if $\mu^*$ is precise, requires almost everywhere decisiveness, and countably additive, then, for any $\prpickstrat$ that doesn't $\mu^*$-surely pick for $\EU_p$, for some $p$, there is an alternative $\prpickstrat'$ that is better than $\prpickstrat$ in expectation. And this is sufficient to give us the two analogous results for E-Admissibility just stated. But it is not sufficient to give the analogous results for Maximality, because there is no guarantee that it is the same $\prpickstrat'$ that is better in expectation than each $\prpickstrat$ in $\prpickstrategies$.
%
%
%So, the advocate of Maximality might respond to our results by saying that all it shows is that you shouldn't pick by randomising; or, if you do, then you shouldn't learn which picking strategy represents your randomisation, and so shouldn't attain a precise probability over the decision problems. 
%	
%Two responses to this: first, it's hard to see what mechanism one could use for picking between options your choice function deems permissible that doesn't amount to randomisation; second, it seems troubling that you would have to abandon the decision theory you're using or the picking strategy you're using upon learning what it is. 
%
%
%
%
%%\begin{colored}{red}
%%
%%	
%%	Perhaps the advocate of imprecise probabilities thinks it is misguided to seek a precise measure of the utility of a choice function when that choice function arises from an imprecise decision theory. What might they say instead? Well, given a choice function $C$, say that $C^*$ is a precisification of $C$ if, for all $D$, $C^*(D) = \{a\} \subseteq C(D)$. That is, $C^*$ picks a particular act from each choice set. Then we might say that the utility of a choice function $C$ is the set of utilities of its precisifications. 
%%	
%%	Of course, by doing so, we make it so that we simply cannot apply the imprecise decision rule to itself: for the imprecise decision rule relies on there being determine precise utilities for the acts between which it adjudicates; and this strategy removes that.\todoold{CCM: and if we extend the Max and E-admiss to apply to utilitiesi n the natural way too, what happens? Do our results hold? I think it does for E-admiss but not for Max? Can we say that...}
%%	
%%	This would at least give a principled reason for saying that the decision theory cannot be applied to itself. But it ignores the fact that, while a choice function can be non-committal between two different acts, declaring both choiceworthy, an agent faced with a decision problem must choose only one. And so, if we really are to judge decision theories by extent to which they get us what we want, we must look at what they actually lead us to choose. And, for that, we need a picking strategy and from that we obtain a precise account of the utility of the decision theory.
%%\end{colored}
%
%
%
%
\subsubsection{Utility of a choice function not given by a picking strategy}

Perhaps one should specify the utility of a choice function in an alternative way, specifying $\U(\c,D,\omega)$ for the various $D$ and $\omega$. 
%Perhaps the utility of adopting a choice function $\c$ isn't governed by randomisation. 
Even if we impose that $\U(\c,D,\omega)$ should be a mixture of $\U(a,\omega)$ for $a\in\c(D)$ our results do not follow. For example, if we specify that $\U(\c,D,\omega)=\sup\{\U(a,\omega)\given a\in \c(D)\}$, then our results clearly do not hold, in fact, then one should be maximally imprecise in every decision problem, setting $\c(D)=D$. 
%An alternative defence is to argue that there's no representation in terms of a mixture of utilities of the non-rejected options. If for example $\U(\c,D,\omega)=\sup\{\U(a,\omega)\given a\in \c(D)\}$, then our results would not hold, in fact, then one should be maximally imprecise in every decision, setting $\c(D)=D$. 
This, however, needs more justification. Why should it be evaluated this way?
Why would this be the right judgement of what utility I will get if I adopt the choice function $\c$?
Perhaps an approach which says that when there's an imprecise decision set one should do something else to make the decision, for example to consult an expert \citep{de2014efficient}; although we wonder then why the option to consult an expert or gather more evidence wasn't available in the setting up of the decision problem. 
%A defence by Jasper etc\todoold{what?} say that when there's an imprecise decision set one should do something else, for example gather more evidence. We're not sure however, whether that's that helpful. Why wasn't the gathering more evidence option an option that was available in the first setting up of the decision problem? 




%\begin{colored}{orange}
%There is a different defence that the imprecise defender may argue for which we think would be successful, which will result in the feature that there's no representation in terms of a mixture of utilities of the choiceworthy options. If for example $\U(C,D,\omega)=\sup\{U(a,\omega)\given a\in C(D)\}$, then our results would not hold, in fact, then one should be maximally imprecise in every decision, setting $C(D)=D$. This, however, needs more justification, why should it be evaluated this way. A defence by Jasper etc\todoold{what?} say that when there's an imprecise decision set one should do something else, for example gather more evidence. We're not sure however, whether that's that helpful. Why wasn't the gathering more evidence option an option that was available in the first setting up of the decision problem? 
%\end{colored}
%
%\todooldinfo{Add stuff! JK, but also the stuff we were discussing!} 
%
%
%
%
%%\subsubsection{Picking strategies vs judging choice theories}



\section{Conclusion}


We proposed to use a decision theory to judge itself, or to judge strategies compatible with its recommendations, and found significant challenges for a host of theories that diverge from expected utility theory. 
%Our formal analysis often parallels that of existing known challenges for these theories such as diachronic inconsistencies exhibited by Dutch book arguments or paradoxes of sequential choice, but our interpretation of the results is different, understanding it as the decision theory undermining its own recommendations. 


In \cref{sect:reu} we showed that risk-weighted expected utility theory, and other theories that accommodate the Allais preferences are self-undermining in a particular way: for any such theory, there are particular ways of being uncertain about which decisions you'll face, where there is a single picking strategy that chooses in line with the recommendations the decision theory would make were you to face each possible decision you might face, and that strategy is not itself acceptable according to the decision theory. These decision theories undermine their own recommendations, recommending instead choosing in a way that the theory itself rejects. We generated these examples on the basis of the Allais preferences---and indeed any failure of the Independence Axiom would do. We then noted that we see the same phenomenon if we know we'll face a binary decision defined over two possible states of the world, and we place a uniform distribution over these different possible decisions; and similarly for a number of beta distributions we might place over them. And so the extent of the self-undermining is reasonably broad, but we don't have a precise general result that shows how broad it is.

In \cref{sect:eut} we showed that traditional Savage-style expected utility theory does not have the same flaw: that it always recommends its own picking strategies.

We then turned to decision theories that accommodate ambiguity and imprecision. In \cref{sect:gamma}, we say that $\Gamma$-maximin is self-undermining in the way the Allais-permitting theories were, and we generated the witness to this using the Ellsberg preferences.\todoold{are we going to say something about} That is, they can rule out their (only) picking strategy as impermissible. 

In \cref{sect:e-admiss} we observed that E-Admissibility and Maximility aren't vulnerable to the same challenge as they judge some of their picking strategies as permissible. However, in the case of E-Admissibility, we noted that it was only the $\EU_p$ strategies that they judge as acceptable, thus requiring their picking strategy to coordinate across decisions. 

%In \cref{sect:e-admiss}, we noted that the only E-Admissible strategies are those that are EU strategies: that is, they pick for $\EU_p$ for some probability in the credal set $\IP$.\todoold{check phrasing} But since these pick for E-Admissibility, that theory is not self-undermining in the same way. And, if we assume that your uncertainty about the decision problem is precise and requires almost everywhere decisiveness and countably additive, the only strategies that Maximality permits are EU ones. But again, since these pick for Maximality, that theory is not self-undermining in this way. 

%In \cref{sect:Max} we showed that the more permissive decision theories such as Maximality still have the same feature, that only EU strategies are evaluated as permissible, thus it undermines its own recommendations when there is non-trivial imprecision. However, for these results we needed to make a stronger assumption: that one has \emph{precise} probabilities over which decision she'll be faced with, along with an assumption that this requires almost everywhere decisiveness over a whole range of decisions. Whilst this is a limitation of our results, it nonetheless shows a whole slew of cases where these theories collapse into EU. \todoold{check that phrasing}

In \cref{sect:nu} we reevaluated what we are judging and instead proposed evaluating not the deterministic picking strategies compatible with the recommendations of the decision theory but instead using probabilistic picking strategies. This might be because you are using a randomisation process, or just that you have uncertainty about how you'll pick which is governed by a precise probability, or simply that how we judge the utility of a choice rule should be given by a mixture of the utilities of the actions that are not ruled out. 
We noted that all our previous results generalised to this setting and moreover, that the situation became worse for the imprecise decision theories:
 E-Admissibility now judges as impermissible any regular $\prpickstrat$ which picks for it, as it no longer looks like $\EU_p$ for any $p$, at least given some mild assumptions on the credal set. 
For Maximality, we were able to show something similar, that it deems impermissible any regular $\prpickstrat$ that picks for it, although in this case we needed a much stronger assumption: that one's uncertainty over which decision she'll be faced with is governed by a precise probability, along with a broadness assumption that this requires almost everywhere decisiveness. 
	These considerations highlight that one should not in general pick by randomisation amongst the non-rejected options, or have uncertainty over how you'll pick in a way that amounts to randomisation. Imprecise decision theories see value in coordination. %have to see the value in coordination. 
%What these considerations highlight is the general 
%We saw that these considerations while \emph{prima facie} challenging, are perhaps less worrying or surprising than one might expect.
%Since these theories require one to pick in accordance with expected utility theory for some probability, one might try to extend the analysis to use E-Admissibility itself to judge this set of picking strategies. And, at least under our particular way of spelling this out, we obtain a similar challenge: since each probability function $p^*$ thinks you should pick in accordance with $\EU_{p^*}$, they agree that the set of all $\EU_p$ strategies is at least as bad as an alternative set, and some precisification is better. 



	We also considered extending the theories to judge imprecise picking strategies, for example, judging the set of all expected utility picking strategies. We proposed that every probability function $p$ evaluates that the precise picking strategy of picking in accordance with $\EU_{p}$ itself is preferable to this imprecise picking strategy, after all, it is expected to be at least as good as all the picking strategies in the set, and better than some, namely any that pick for some other probability. Thus, under this way of applying E-Admissibility to judge imprecise picking strategies, the only strategy which are not judged impermissible are those which correspond to expected utility for a particular probability. \todooldinfo{ARGH!}

%
%We understand this as a result that these decision theories for imprecise probability do not judge themselves to be optimal. 
%So if this is how we ask an imprecise decision theory to judge itself, they rule themselves out as impermissible. 
%We also considered imprecise picking strategies. This is still susceptible to the analogue result in the E-Admissibility case: even if one's picking strategy is imprecise, to be E-Admissible it must look like an $\EU_p$ strategy, thus requiring coordination. 
The analogous results won't apply to Maximality for imprecise picking strategies. 
Another approach which would undermine our results would be to argue that the utility of an imprecise choice set should not amount to being a mixture, a defence which we acknowledge is open. It is, for example, very easy if one sets $\U(\c,D,\omega)=\sup\{\U(a)(\omega)\given a\in\c(D)\}$ then of course one should have maximally imprecise choice function. But motivating and justifying any such analysis remains an important task. 
%
%{\color{violet}Some defences might be given by, for example, considering imprecise picking strategies, or ways of picking that don't amount to randomisation or mixtures but track the good options somehow. Whilst we acknowledge these defences, they are harder to implement than mixing and would like further analysis and motivation. 
%}
%{\color{orange}The defender of imprecise probabilities might argue that she should have imprecise opinions about which picking strategy she'll use, or that we should assign utility values just to the imprecise choice rule that provide imprecise utilities. These responses are not particularly helpful for the defender of E-admissibility as we still have the feature that the EU strategies are the only ones that are permissible, so if we extend the considerations to being imprecise in SLIGHTLY CONTROVERTIAL in a natural way we will still obtain the undermining feature. These defences do, however, help the defender of Maximality. It means, however, that one needs to have a method of picking which does not amount to randomisation, and/or she should abandon the decision theory if she does happen to be precise about how she'll pick. And if we are simply saying that utilities are imprecise, (some of) the authors remain unhappy because we want to ask of more from a decision theory: we need some guidance about what to do; simply holding hands up and saying, well, here's a set of options which are not ruled out but offering no guidance of how to pick, then it's not a very useful decision theory!
%
%There is a different defence that the imprecise defender may argue for which we think would be successful, which will result in the feature that there's no representation in terms of a mixture of utilities of the choiceworthy options. If for example $\U(C,D,\omega)=\sup\{U(a,\omega)\given a\in C(D)\}$, then our results would not hold, in fact, then one should be maximally imprecise in every decision, setting $C(D)=D$. This, however, needs more justification, why should it be evaluated this way. A defence by Jasper etc\todoold{what?} say that when there's an imprecise decision set one should do something else, for example gather more evidence. We're not sure however, whether that's that helpful. Why wasn't the gathering more evidence option an option that was available in the first setting up of the decision problem? 
%}
%\todooldinfo{RP: I think CCM and JK better placed to finalise the previous couple of paragraphs.}




To summarise: we have found %significant 
challenges for any of the decision theories we've considered that depart from expected utility theory. 
%When we ask a whole range of decision theories how they think one should pick, they pretty systematically recommend picking in accordance with expected utility theory, and thus undermining their own recommendations whenever those recommendations don't collapse into the recommendations of expected utility theory. 
When we ask a whole range of decision theories how they think one should pick, they pretty systematically recommend picking in accordance with expected utility theory. For some of the theories we considered (REU, $\Gamma$-maximin), this undermines their own recommendations whenever they don't collapse into the recommendations of expected utility theory. For others (E-Admissibility, Maximality), %it rather reflects a certain value, \emph{viz.}, the value of coordinating how you resolve incomparability across decision problems.
the situation is less clear, as picking in accordance with expected utility theory is compatible with the theory and choice-worthy according to the theory, however other picking strategies are deemed impermissible, and all regular probabilistic picking strategies are deemed impermissible. These theories thus have to accept a deep-seated value for coordinating how to resolve incomparability across decision problems.
% This may simply reflect a deep-seated value for coordinating how to resolve incomparability across decision problems; a value that may be palatable to proponents of those theories.
 What \emph{is} clear is that decision theorists must face the question of how to pick head on. %must theorise more systematically about how to pick.}}
\todooldinfo{Recheck the conclusion??}




\appendix

%\section{REU}
%\todooldinfo{As above, we need to think about how to present this.}\todooldinfo{???}

%\section{Setup?}
%We will work in the appendix throughout with $\prpickstrat$. Recall the definitions from \cref{def:nu stuff}


\bibliographystyle{apa-good}
\bibliography{bibliography}


\section{Summary of results}
There are two main dimensions of stronger undermining results:
\begin{enumerate}
	\item\label{itm:uncertainty} Uncertainty over possible decisions:
	\begin{enumerate}
		\item\label{dimension:uncertainty:exists} Just a single example of being uncertain where it’s undermining.
		\item\label{dimension:uncertainty:plausible} A plausible way of being uncertain where it’s undermining.\\Uniform
		\item\label{dimension:uncertainty:range} It’s undermining for a decent range of ways of being uncertain.\\When precise, requires almost everywhere decisiveness...
		\item\label{dimension:uncertainty:all} It’s undermining in almost all ways of being uncertain.\\All, except those where it reduces to EU.
	\end{enumerate}
	
	\item\label{dimension:strategies} Are all or just some of the strategies deemed impermissible?
	\begin{enumerate}[label=(\alph*), ref=\theenumi\alph*]
		\item\label{dimension:strategies:some} All non-EU strategies are impermissible. So (typically), some of its strategies ruled impermissible. 
%		\begin{itemize}
%			\item Each choiceworthy act is part of a choiceworthy strategy: for each $D\in\D$ and $a\in\c(D)$ there’s some $\s\in\c(\S)$ with $\s(D)=a$.
%			\item Some decisions have choiceworthy acts which are not picked by any choiceworthy strategy.
%		\end{itemize}
		\item\label{dimension:strategies:allprob} All its regular probabilistic picking strategies deemed impermissible.% does this lie between the others?
		\item\label{dimension:strategies:all} All its picking strategies ruled impermissible.
	\end{enumerate}
\end{enumerate}

\bigskip

\begin{itemize}
	\item EU: \label{itm:EU}
	\begin{itemize}
		\item Has none of these dimensions of undermining: in all ways of being uncertain, all picking strategies are choiceworthy.\cref{thm:eu-self-rec,thm:eu-uniquely-optimal,thm:eu-nu-nec-suff}
	\end{itemize}
	
	\item Risk / Allais cases / Independence failures\label{itm:allais}
	\begin{itemize}[nosep]
		\item An instance of uncertainty (\ref{dimension:uncertainty:exists}) where all picking strategies are rejected (\ref{dimension:strategies:all}). \Cref{sect:reu}
		\item For REU/Quiggin: strengthen to the uniform distribution (\ref{dimension:uncertainty:plausible}), still rejecting all strategies (\ref{dimension:strategies:all}). \Cref{sect:reu:other mu}
	\end{itemize}
	
	\item $\Maximin$\label{itm:maximin}
	\begin{itemize}[nosep]
		\item Some way of being uncertain (\ref{dimension:uncertainty:exists}) where all strategies are rejected (\ref{dimension:strategies:all}). \Cref{sect:Ellsberg}
		\item If one’s uncertainty is precise, countably additive, and requires almost‐everywhere decisiveness (\ref{dimension:uncertainty:range}), then its only choiceworthy strategies are $\EU$ picking strategies (\ref{dimension:strategies:some}). \Cref{thm:gamma-nu}
		\item If one’s uncertainty is precise, countably additive, and requires almost‐everywhere decisiveness (\ref{dimension:uncertainty:range}), then no regular probabilistic picking strategy is choiceworthy (\ref{dimension:strategies:allprob}). \Cref{thm:gamma-nu}
	\end{itemize}
	\item $\EAd$
	\begin{itemize}
		\item For all uncertainty, (\ref{dimension:uncertainty:all}), its only choiceworthy strategies are $\EU$ picking strategies (\ref{dimension:strategies:some}). There are others, which are ruled out. \Cref{thm:ead-equiv[indep]} (also, when decision-state dependence: \cref{thm:ead-equiv[dep]})
		\item For all uncertainty which is imprecise (\ref{dimension:uncertainty:all}), all regular picking strategies are ruled out (\ref{dimension:strategies:allprob}). \Cref{thm:ead-nu-reg-nec}
		\item For all uncertainty (\ref{dimension:uncertainty:all}), the only imprecise picking strategy which is not rejected (under our proposed extension of that for EAd) is to be precise and follow expected utility (NO REF ABOVE??) \Cref{sect:IPpicking}
	\end{itemize}
	\item Maximality:
	\begin{itemize}
		\item  If one’s uncertainty is precise, countably additive, and requires almost‐everywhere decisiveness (\ref{dimension:uncertainty:range}), then its only choiceworthy strategies are $\EU$ picking strategies (\ref{dimension:strategies:some}).  \Cref{thm:max-suff}
		\item If one’s uncertainty is precise, countably additive, and requires almost‐everywhere decisiveness (\ref{dimension:uncertainty:range}), then no regular probabilistic picking strategy is choiceworthy (\ref{dimension:strategies:allprob}) \Cref{thm:max-nu-reg-nec}
	\end{itemize}
\end{itemize}

\bigskip 
ARE THEY STRICTLY ORDERED? 2B IMPLIES 2A?? NOT REALLY. ADD PIC FOR ORDERING, SO WE KNOW WHAT OTHER THMS WE HAVE AS WEAKER ONES??
\begin{center}
	\begin{tabular}{l|cccc}
	& \ref{dimension:uncertainty:exists} & \ref{dimension:uncertainty:plausible} & \ref{dimension:uncertainty:range} & \ref{dimension:uncertainty:all} \\ \hline
	\ref{dimension:strategies:some}    &  &  &  \makecell[l]{$\Maximin$\\$\Maximality$}& \makecell[l]{$\EAd$} \\
	\ref{dimension:strategies:allprob} &  &  &  \makecell[l]{$\Maximin$\\$\Maximality$}&  \makecell[l]{$\EAd$} \\
	\ref{dimension:strategies:all}     & \makecell[l]{Allais\\$\Maximin$} & REU &  &  
\end{tabular}
\end{center}

%\begin{tabularx}{1.2\linewidth}{X|l|l|l|l|l|}
%&Allais &EU&$\Maximin$&$\EAd$&$\Maximality$\Bstrut\\\hline\Tstrut
%bad: For some $\mu$, every $\s$ which picks for $\c$ is not in $\c(\S)$\newline
%good: For all $\mu$, some $\s$ which picks for $\c$ is in $\c(\S)$
%	&bad&good&bad&good &good\Bstrut\\\hline\Tstrut
%bad: For some $\mu$, some $\s$ which picks for $\c$ is not in $\c(\S)$ \newline
%good: For all $\mu$ and $\s$, if $\s$ picks for $\c$ it is in $\c(\S)$
%	&bad&good&bad&bad &bad\Bstrut\\\hline\Tstrut
%bad: For many $\mu$, every $\s$ which picks for $\c$ is not in $\c(\S)$\newline
%good: 
%	&?uniform&good&?&good &good\Bstrut\\\hline\Tstrut
%bad: For many $\mu$, some $\s$ which picks for $\c$ is not in $\c(\S)$\newline
%good: 
%	&?&good&bad&bad &bad ($\mu$ prec \& req dec)\Bstrut\\\hline\Tstrut
%bad: For many $\mu$, every regular $\prpickstrat$ which picks for $\c$ is not in $\c(\PrPickStrategies)$\newline
%good: 
%&?&good&?&bad &bad($\mu$ prec \& req dec)\Bstrut\\\hline\Tstrut
%bad: For many $\mu$, some regular $\prpickstrat$ which picks for $\c$ is not in $\c(\PrPickStrategies)$\newline
%good: 
%&?&good&?&bad &bad ($\mu$ prec \& req dec)\Bstrut\\\hline\Tstrut
%Imprecise $\imprecpickstrat$??
%\end{tabularx}

\begin{colored}{violet}

	

	
	
	
	\section{Measure theoretic considerations}\label{sect:appendix:measure}
	Throughout the paper, we have ignored the question of measurability. We have considered various probability measures: $p$ a probability on $\Omega$, $\mu$ a probability on $\Decs$, and $\pb$ a probability on $\Omega\times\Decs$. To make this precise, we should fix the $\sigma$-algebras on each of these spaces.
	
	Since $\Omega$ is finite, we can equip it with the discrete $\sigma$-algebra, $\mathcal{F}=\wp(\Omega)$, so that every subset of $\Omega$ is measurable. 
	
	$\Decs$ is defined as finite subsets of the set of acts $\A$, with $\A$ simply being an arbitrary non-empty set. 
	
	By fixing a utility function $\U:\A\times\Omega\to\Re$ we can treat $\A$ as a subset of $\Re^n$, where $n=\#\Omega$, the cardinality of $\Omega$. 
	We will fix the $\sigma$-algebra on $\A$ as inherited from the standard Borel $\sigma$-algebra on $\Re^n$. 
	
	To construct a $\sigma$-algebra on $\Decs$, we consider $\Sigma$ the smallest $\sigma$-algebra containing each $\{D\in\Decs\given \#(D\cap A)=k\}$ for $A$ measurable subset of $\A$ and $k\in\mathbb{N}\setminus\{0\}$. Equivalently, it can be seen as the countable disjoint union of $k$-tuples in $\Re^n$, modulo permutations, with the obvious inherited $\sigma$-algebra.\todo{more care}
	
%	We then require any probability $p$$\mu$ on $\Decs$ to be a probability measure on $(\Decs,\Sigma)$; and $\pb$ to be a probability measure on $(\Omega\times\Decs,\wp({\Omega})\otimes \Sigma)$
	
	All the results in this paper are restricted to measurable picking strategies $\s:\Decs\to\A$, or measurable probabilistic picking strategies $\prpickstrat:\Decs\to\Delta(\A)$ so that the induced payoffs, $\U(\s),\U(\prpickstrat):\Omega\times\Decs\to\Re$, are measurable random variables with respect to $(\Omega\times\Decs,\mathcal{F}\otimes \Sigma)$, recalling that $\U(\prpickstrat)(\omega,D):=\Exp_{\prpickstrat_D}[\U(\cdot)(\omega)]$.
	
	
	
\section{Decision-State Dependence}\label{sect:appendix:decdep}


When we have decision-state dependence, we made use of the idea of conditional probabilities. 

For $b$, a probability measure on $(\Omega\times\Decs,\mathcal{F}\otimes \Sigma)$, we assumed we had $b(\cdot | D)$, probability functions on $\Omega$, defined for $b_\Decs$-almost every $D$. The important feature that our results rely on is that for bounded (or integrable) measurable random variables $X:\Omega\times\Decs\to\Re$, (in particular, for our $\U(\s)$ or $\U(\prpickstrat)$), the law of total expectation holds: $\Exp_b[X]=\Exp_{b_\Decs}[\Exp_{b(\cdot|D)}[X]]$.

When $\Decs$ is supported on a finite subset of $\Decs$, we can just define $b(\cdot | D)$ by the ratio formula for any $D$ with $b_\Decs(D)>0$ and check that the law of total expectation holds. 

In the more general setting, we need to make use of further machinery. Under our imposed $\sigma$-algebra, $(\Omega\times\Decs,\wp({\Omega})\otimes \Sigma)$ is a standard Borel space, and so the Regular Conditional Probability theorem guarantees the existence of $b(\cdot|D)$ for $b_\Decs$ almost every $D$ which is a probability measure on $\Omega$ and such that for every bounded (or integrable) measurable random variable $X$, the law of total expectation holds: $\Exp_b[X]=\Exp_{b_\Decs}[\Exp_{b(\cdot|D)}[X]]$.\todo{Ref to this!!!}

When $b=p\times\mu$, then $b_\Decs=\mu$ and $b(\cdot|D)=p$ for all $D$; and $\Exp_{p\times\mu}[X]=\Exp_{\mu}[\Exp_{p}[X]]$.



%When $b$ is supported on finite subset of $\Decs$, this is easy to define: we update $b$ by the ratio formula, whenever $b(D)\neq 0$. More carefully:
%
%Let $b_\omega$ a measure on $\Decs$ given by $b_\omega(\omega)=b(\{\omega\}\times E)$ for $E\subseteq D$ measurable. Put $b_\Decs=\sum_{\omega\in\Omega} b_w$, is the marginal of $b$ on $\D$. 
%
%If $b_\Decs$ is supported on a finite subset $\Decs'\subseteq\Decs$, i.e., $b_\Decs(\Decs')=1$,, then we can just define $b(\cdot|D^*)$ as a probability measure over $\Omega$ given by the ratio formula, i.e., $b(\omega|D^*)=\frac{b(\{\omega\}\times \{D^*\})}{b(\Omega\times \{D^*\})}=\frac{b_\omega(D^*)}{b_\Decs(D^*)}$, whenever $b_\Decs(D^*)>0$; which is for $b_\Decs$ almost all $D^*$, by assumption of finite support. 
%
%
%Then, for any random variable, $X:\Omega\times\Decs\to\Re$, (in particular, for our $\U(\s)$ or $\U(\prpickstrat)$), $\Exp_b[X]=\Exp_{b_\Decs}[\Exp_{b(\cdot|D)}[X]]$ 
%This is because
%\begin{align}
%	\Exp_b[X]&=\sum_{D\in\Decs'}\sum_{\omega\in\Omega}X(\omega,D)b(\omega,D)\\
%	&=\sum_{D\in\Decs'}\sum_{\omega\in\Omega}X(\omega,D)\left[b_\Decs(D)\frac{b(\omega,D)}{b_\Decs(D)}\right]\\
%	&=\sum_{D\in\Decs'}b_\Decs(D)\sum_{\omega\in\Omega}X(\omega,D)\left[\frac{b(\omega,D)}{b_\Decs(D)}\right]\\
%	&=\Exp_{b_\Decs}[\Exp_{b(\cdot|D)}[X]]
%\end{align}
%
%
%Now, if $b$ is supported on an infinite set $\Decs$, we need to use some more machinery to define the appropriate conditional probabilities. 
%
%
%
%Let $b_\omega$ be a measure on $\Decs$ given by $b_\omega(E)=b(\{\omega\}\times E)$ for $E\subseteq \Decs$ measurable. Then the marginal on $\Decs$ is given by  $b_\D=\sum_{\omega\in\Omega} b_w$. 
%
%As $b_\D$ is the finite sum of these $b_\omega$, they are absolutely continuous with respect to it; and thus by Radon–Nikodym theorem, for each $\omega$ there is a (measurable) function $f_\omega$ on $\Decs$ (unieq)such that for all measurable $E\subseteq\Decs$, $b_\omega(E)=\int_{E}f_\omega(D) b_\D(\diff D)$. 
%Put $b(\omega |D):= f_\omega(D)$. 
%
%Since $b_\D=\sum_{\omega\in\Omega}b_\omega$, one can check that for $b_\D$ almost every $D$, $b(\cdot |D)$ is a probability measure over $\Omega$.
%
%We have $b_\omega(E)=\int_{E}b(\omega |D) b_\D(\diff D)$
%
%For any absolutely integrable $X: \Omega\times \Decs\to\Re$ (for example, when $X$ is bounded)
%\begin{align}
%	\Exp_b[X]&=\int_{\Omega\times\Decs} X(\omega, D) b(\diff \omega,\diff D)\\
%	&=\sum_\omega\int_\D X(\omega,D) b_\omega (\diff D)\\
%		&=\sum_\omega\int_\D X(\omega,D) b(\omega |D) b_\D(\diff D)\\
%		&=\int_\D [\sum_\omega X(\omega,D) b(\omega |D)] b_\D(\diff D)\\
%		&=\Exp_{b_\D}[\Exp_{b(\cdot|D)}[X]]
%\end{align}
%
%\bigskip
%
%ALTERNATIVE...
%
%
%
%
%Define its marginal on $\Decs$ by $b_\Decs(E)=b(\Set{(\omega,D)\given D\in E})$ for $E\subseteq\Decs$. 
%By the disintegration theorem (since $\Omega$ and $\Decs$ are standard Borel), there are conditional probabilities $b(\cdot | D)$ which are unique for $b_\Decs$-almost all $D$. 
%
%Note that $\Exp_b[X]=\Exp_{b_\Decs}[\Exp_{b(\cdot|-)}[X]]$ for any $X$ a measurable random variable on $\Omega\times \Decs$. 
%
%Note that if $b$ is a product, $p\times\mu$, then $b_\Decs$ is just $\mu$ and $b(\cdot|D)=p$ for all $D$. 
\section{EU and E-admissibility}

	
\subsection{EU}
Our first series of results rely on the fact that a picking strategy is an optimiser of expected utility iff it is an expected utility picking strategy (or at least looks like one, i.e., is $\mu$-surely one). This core result, stated in \cref{thm:eu-self-rec,thm:eu-uniquely-optimal}, is then extended to cover cases of act-state dependence (\cref{thm:eu-dep}) and probabilistic picking strategies (\cref{thm:eu-nu-nec-suff}). 

We will prove the result in generality to cover all these cases. 

%
%\begin{theorem}\label{thm:EU appendix} 
%	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
%		\item  If $\nu$ $\mu$-surely picks for $\EU_{p}$ then, for any $\nu'$, $\Exp_{p\times\mu}[\U(\nu)]\geq\Exp_{p\times\mu}[\U(\nu')]$
%		\item  If $\nu$ $\mu$-surely picks for $\EU_{p}$ and $\nu'$ does not, then $\Exp_{p\times\mu}[\U(\nu)]>\Exp_{p\times\mu}[\U(\nu')]$.
%	\end{enumerate}
%\end{theorem}
%\begin{proof}
%	Since $\U$ is assumed to be bounded, $\abs{\U(a)(\omega)}\leq M$ for all $a\in\A$ and $\omega\in\Omega$. Thus $\abs{\U(\nu)(\omega,D)}\leq M$, so $\int_{\Omega\times \D} \abs{\U(\nu)(\omega,D)} \; \pb(\diff \omega,\diff D)\leq M$, i.e., it is absolutely integrable. Thus, by Fubini's theorem, integrals can be exchanged freely. 
%	
%	The basic reason is by exchange of expectations: $$\Exp_{p\times\mu}[\U(\nu)]:=\Exp_{p\times\mu}[\Exp_{\nu_D}[\U]]=\Exp_\mu[\Exp_{\nu_D}[\Exp_p[\U]]]$$.
%	This exchange is legitimate because $\Omega$ and each $D$ are finite, so it's exchanging an integral with two finite sums. It is also legitimate without the assumption that each $D$ is finite (we mentioned the possibility that it is merely compact), as $\U$ is bounded, so it is absolutely integrable and by Fubini's theorem, integrals can be exchanged at will. 
%	
%	One can observe that this obtains it optimum when each $\nu_D(\EU_p(D))=1$, putting maximal weight on those acts which maximise expected utility, and reaches this maximal exactly when $\nu_D(\EU_p(D))=1$ for $\mu$-almost every $D\in\D$. 
%
%
%\end{proof}

\begin{theorem}\label{thm:EU appendix} 
	\hspace*{1mm}
	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
		\item  If $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ then, for any $\prpickstrat'$, $\Exp_\pb[\U(\prpickstrat)]\geq\Exp_\pb[\U(\prpickstrat')]$
		\item  If $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ and $\prpickstrat'$ does not, then $\Exp_\pb[\U(\prpickstrat)]>\Exp_\pb[\U(\prpickstrat')]$.
	\end{enumerate}
\end{theorem}
\begin{proof}

%$b$ is a measure on $\Omega\times\D$. $\Omega$ finite, we consider all subsets measurable; it is a standard Borel space. 
%Given the utility, $\Decs$ can be considered as finite subsets of $[-M,M]^{|\Omega|}\subseteq\Re^n$; again a standard Borel space. 
%So it admits a regular conditional probability kernel $b(\cdot \given D):\Omega\to [0,1]$ for $b_\D$-almost every $D$. Where 

As in \cref{sect:appendix:decdep}, we have $b(\cdot\given D)$ defined for $b_\Decs$ almost every $D$ where for every bounded measurable random variable $X:\Omega\times\Decs\to\Re$, the law of total expectation holds: $\Exp_b[X]=\Exp_{b_\Decs}[\Exp_{b(\cdot|D)}[X]]$.

Thus, $\Exp_{b}[\U(\prpickstrat)]=\Exp_{b_\Decs}[\Exp_{b(\cdot | D)}[\U(\prpickstrat)]]$, as $\U(\prpickstrat)$ is a measurable random variable on $\Omega\times\Decs$ and is bounded by our assumption on $\U$. 


Consider any $D^*\in\D$ for which $b(\cdot\given D^*)$ is well defined. Then\footnote{More carefully:
	\begin{align}
		\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)(\cdot,D^*)]&=	\sum_{\omega\in\Omega}b(\omega| D^*)\sum_{a\in D^*}\prpickstrat_{D^*}(a)\; \U(a)(\omega)&&\text{definition of $\U(\prpickstrat)$}\\
		&=\sum_{a\in D^*}\prpickstrat_{D^*}(a)\sum_{\omega\in\Omega}b(\omega| D^*)\;\U(a)(\omega)
\end{align}	}
 $$\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)]=\Exp_{b(\cdot | D)}[\Exp_{\nu_{D^*}}[\U]]=\Exp_{\nu_{D^*}}[\Exp_{b(\cdot | D)}[\U]].$$

$\Exp_{b(\cdot | D^*)}[\U(a)]$ is maximised, by definition, at any $a\in \EU_{\pb(\cdot|-)}(D^*)$. 

Thus $\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)]=\Exp_{\nu_{D^*}}[\Exp_{b(\cdot | D^*)}[\U]]$ is maximised when $\prpickstrat_{D^*}(\EU_{\pb(\cdot|-)}(D^*))=1$. 

And so $\Exp_{b}[\U(\prpickstrat)]=\Exp_{b_\Decs}[\Exp_{b(\cdot | D)}[\U(\prpickstrat)]]$ is maximised when $\prpickstrat_{D^*}(\EU_{\pb(\cdot|-)}(D))=1$ for $b_\Decs$ almost every $D$. 
i.e., when $\prpickstrat$ $b_\Decs$-surely picks for $\EU_{b(\cdot|-)}$. 
This gives our claims.
%Consider any $D^*\in\D$ for which $b(\cdot\given D^*)$ is well defined. Then
%\begin{align}
% &	\Exp_{\omega\sim b(\cdot | D^*)}[\U(\prpickstrat)(\omega,D^*)]\\
% 	&=\Exp_{\omega\sim b(\cdot | D^*)}[\Exp_{a\sim \prpickstrat_{D^*}}[\U(a)(\omega)]]&&\text{definition of $\U(\prpickstrat)$}\\
% 	&=\Exp_{a\sim\prpickstrat_{D^*}}[\Exp_{\omega\sim b(\cdot | D^*)}[\U(a)(\omega)]].&&\text{$\Omega$ and $D^*$ are finite.}
% \end{align}
%
%
%$\Exp_{\omega\sim b(\cdot | D^*)}[\U(a)(\omega)]$ is maximised, by definition, at any $a\in\EU_{b(\cdot|-)}(D^*)$. Thus $\Exp_{a\sim\prpickstrat_{D^*}}[\Exp_{\omega\sim b(\cdot | D^*)}[\U(a)(\omega)]]$ is maximised just when $\prpickstrat_{D^*}(\EU_{b(\cdot|-)}(D^*))=1$.
%
%Thus, $\Exp_{b}[\U(\prpickstrat)]=\Exp_{b_\Decs}[\Exp_{b(\cdot | D)}[\U(\prpickstrat)]]$ is maximised just when for $\pb_\Decs$ almost every $D$,  $\prpickstrat_{D^*}(\EU_{b(\cdot|-)}(D))=1$, which is to say that $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$. 
%
%
%Observe that $\Exp_{b(\cdot | D)}[\U(\prpickstrat)]=\Exp_{\nu_{D^*}}[\Exp_{b(\cdot|D^*)}[\U]]$.
%\begin{align}
%	\Exp_{b(\cdot | D)}[\U(\prpickstrat)(D,\omega)]&=\Exp_{b(\cdot| D)}[\Exp_{\prpickstrat_D}[\U(a)(\cdot)]]&&\text{definition  of $\U(\prpickstrat)$}\\
%	&=\Exp_{\prpickstrat_D}[\Exp_{b(\cdot | D)}[\U(a)]]&&\text{ as both are just finite weighted sums.\footnote{\begin{align}
%					\sum_{\omega\in\Omega}b(\omega| D)\sum_{a\in D}\prpickstrat_D(a)\; \U(a)(\omega)&&\text{definition of $\U(\prpickstrat)$}\\
%				&=\sum_{a\in D}\prpickstrat_D(a)\sum_{\omega\in\Omega}b(\omega| D)\;\U(a)(\omega)\\
%	\end{align}}}
%\end{align}	
%
%Consider any $D^*\in\D$ for which $b(\cdot\given D^*)$ is well defined. Then 
%		$\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)(\cdot,D^*)]
%		%=\Exp_{b(\cdot | D^*)}[\Exp_{\prpickstrat_{D^*}}[\U]]$ by the definition of $\U(\prpickstrat)$, which $
%		=\Exp_{\prpickstrat_{D^*}}[\Exp_{b(\cdot | D^*)}[\U]]$.
%More carefully:
%\begin{align}
%	\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)(\cdot,D^*)]&=	\sum_{\omega\in\Omega}b(\omega| D^*)\sum_{a\in D^*}\prpickstrat_{D^*}(a)\; \U(a)(\omega)&&\text{definition of $\U(\prpickstrat)$}\\
%				&=\sum_{a\in D^*}\prpickstrat_{D^*}(a)\sum_{\omega\in\Omega}b(\omega| D^*)\;\U(a)(\omega)
%\end{align}	
%
%$a\in\EU_{b(\cdot|-)}(D^*)$, by definition, iff it is a maximiser within $D^*$ of  $\Exp_{b(\cdot| D^*)}[\U(a)]=\sum_{\omega\in\Omega}b(\omega| D^*)\;\U(a)(\omega)$. Let this maximum value be $t_b(D^*)$.
%
%Thus, $\Exp_{b(\cdot | D^*)}[\U(\prpickstrat)(D^*)]\leq t_b(D^*)$ with equality just when $\prpickstrat_{D^*}$ is supported on $\EU_{b(\cdot | -)}(D^*)$.
%
%Thus $
%	\Exp_{b}[\U(\prpickstrat)]=\Exp_{b_\Decs}[\Exp_{b(\cdot | D)}[\U(\prpickstrat)]]
%$
%obtains its maximum at $\Exp_{b_\Decs}[t_b(D)]$ when $\prpickstrat_D(\EU_{b(\cdot|-)}(D))=1$ for $b_\D$-almost all $D$, i.e., when $\prpickstrat$ $\pb_\Decs$-surely picks for $\EU_{b(\cdot|-)}$.
\end{proof}

This suffices to prove \cref{thm:eu-self-rec,thm:eu-uniquely-optimal,thm:eu-nu-nec-suff,thm:eu-dep}

\end{colored}



\begin{colored}{red}
	
\subsection{EU}
Our first series of results rely on the fact that a picking strategy is an optimiser of expected utility iff it is an expected utility picking strategy (or at least looks like one, i.e., is $\mu$-surely one). This core result, stated in \cref{thm:eu-self-rec,thm:eu-uniquely-optimal}, is then extended to cover cases of act-state dependence (\cref{thm:eu-dep}) and probabilistic picking strategies (\cref{thm:eu-nu-nec-suff}). 


We present our result in generality, covering both the decision-state dependence and probabilistic picking strategy cases. 

Suppose $\pb$ is probability measure over $\Omega\times\D$ which is absolutely continuous with respect to $u\times\lambda$, where $u$ is the uniform distribution on $\Omega$ and $\lambda$ is the restriction of the Lebesgue measure to $\D$\todo{What does that mean??}. Let $f:\Omega\times\D\rightarrow\mathbb{R}_{\geq0}$ be a Radon-Nikodym derivative (or density) of $\pb$, so that for any measurable $A\subseteq\Omega\times\D$
\[
b(A)=\sum_{\omega\in\Omega}\int_{A_\omega} f(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)
\]
where, for any $\omega\in\Omega$, $A_\omega:=\left\{\left<\omega^*,D^*\right>\in A\given \omega=\omega^*\right\}$.

The marginal density, $f_\D$, is given by $f_\D(D)=\sum_{\omega\in\Omega}f(\omega,D)$. 

	
Let $\D':=\{D\in\D\given f_\D(D)>0\}$.

\begin{definition}
	For any $D\in\D'$ and any $a\in D$, $\pb(\cdot|D)$ is a probability function over $\Omega$ defined by $\pb(\omega|D)=\frac{f(\omega,D)}{f_\D(D)}$. \todo{why do we sometimes use angle brackets?}
	So,  \[\Exp_{\pb(\cdot|D)}[\U(a)]:=\displaystyle\sum_{\omega\in\Omega}\frac{f(\omega,D)}{f_\D(D)}\U(a)(\omega)\]
\end{definition}
	
	Recall \cref{def:cond}, defining the choice function $\EU_{\pb(\cdot|-)}$: For $D\in\D'$ (when this is well-defined), $\EU_{\pb(\cdot|-)}(D)=\EU_{\pb(\cdot|D)}(D)$, which, using \cref{def:EU} is 
		\[a\in\EU_{\pb(\cdot|-)}(D)\text{ iff for all $a'\in D$, }\Exp_{\pb(\cdot|D)}[\U(a)]\geq\Exp_{\pb(\cdot|D)}[\U(a')].\]
	If $D\notin\D'$, we can put $\EU_{\pb(\cdot|-)}(D)$ arbitrary, for example just make it equal $\D$. 
	A picking strategy $\s\in\S$ is then said to $\pb_\Decs$-surely pick for $\EU_{\pb(\cdot|-)}$ when \[\pb\{\seq{\omega,D}\given\omega\in\Omega,\, \s(D)\in \EU_{\pb(\cdot|-)}(D)\}=1\] i.e., for the marginal $\pb_\D$, $\pb_\D\{D\given \s(D)\in\EU_{\pb(\cdot|-)}(D)\}=1$. A probabilistic picking strategy $\prpickstrat$ is said to $\pb_\Decs$-surely pick for $\EU_{\pb(\cdot|-)}$ when \[\pb\{\seq{\omega,D}\given\prpickstrat_D(\EU_{\pb(\cdot|-)}(D))=1\}=1\]


	
%\begin{definition}
%	$\EU_{\pb(\cdot|-)}$ is the choice function defined by \[\EU_{\pb(\cdot|-)}(D):=\EU_{\pb(\cdot|D)}(D)\]when $D\in\D'$, i.e., when $f_\D(D)>0$; and when $D\notin\D'$, we put $\EU_{\pb(\cdot|-)}(D)=D$ (this is arbitrary, and it won't play a role in our result).\todoold{I added the when def!!} 
%\end{definition}
%
%
%\todo{we should make the theorem already cover the prob case!!}
%\begin{definition}
%A picking strategy $\s$ \emph{$\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$} iff \[\pb\{\seq{\omega,D}\given\omega\in\Omega,\, \s(D)\in \EU_{\pb(\cdot|-)}(D)\}=1\] i.e., for the marginal $\pb_\D$, $\pb_\D\{D\given \s(D)\in\EU_{\pb(\cdot|-)}(D)\}=1$.
%\end{definition}

\begin{theorem}\label{thm:EU appendix} 
	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
		\item  If $\nu$ $\mu$-surely picks for $\EU_{p}$ then, for any $\nu'$, $\Exp_{p\times\mu}[\U(\nu)]\geq\Exp_{p\times\mu}[\U(\nu')]$
		\item  If $\nu$ $\mu$-surely picks for $\EU_{p}$ and $\nu'$ does not, then $\Exp_{p\times\mu}[\U(\nu)]>\Exp_{p\times\mu}[\U(\nu')]$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	The basic reason is by exchange of expectations: $$\Exp_{p\times\mu}[\U(\nu)]:=\Exp_{p\times\mu}[\Exp_{\nu_D}[\U]]=\Exp_\mu[\Exp_{\nu_D}[\Exp_p[\U]]]$$. The exchange is .
	 and one can observe that this obtains it optimum when each $\nu_D(\EU_p(D))=1$, putting maximal weight on those acts which maximise expected utility, and reaches this maximal exactly when $\nu_D(\EU_p(D))=1$ for $\mu$-almost every $D\in\D$. 
	
	
	
	
	
	For maximum perspicuity, we will write everything out with integrals, to keep track of the variables. However, note that $\Omega$ and each $D$ have been assumed to be finite. 
	
	Let $t_p(D):=\sup_{a\in D}\Exp_p[\U(a)]$, the maximum attainable expected utility on $D$. $\EU_p(D)=\{a\in D\given \Exp_p[\U(a)]=t_p(D)\}$.
	$\nu_D$ is a measure over $D$, so:
	\begin{align}
		\int_D \Exp_p[\U(a)]\;\nu_D(\diff a)&\leq t_p(D)\\
			\text{ and }	\int_D \Exp_p[\U(a)]\;\nu_D(\diff a)&= t_p(D)\text{ iff }\nu_D(\EU_p(D))=1
	\end{align}
	
	Therefore 
	\begin{align}
	\int_\D\int_D \Exp_p[\U(a)]\;\nu_D(\diff a)\mu(\diff D)&\leq \int_\D t_p(D)\mu(\diff D)\\
	\text{ and }	\int_\D\int_D \Exp_p[\U(a)]\;\nu_D(\diff a)\mu(\diff D)&= \int_\D t_p(D)\mu(\diff D)\\\text{ iff }\mu\{D\given \nu_D(\EU_p(D))=1\}=1
\end{align}
	
	So for all $a\in D$, $\Exp_p[\U(a)]\leq t_p(D)$, with equality just when $a\in \EU_p(D)$. 
	
	$\nu_D$ is a measure over $D$ ($D$ is finite, but we write it as an integral for), so $\Exp_{\nu_D}[\Exp_p[\U]]\leq t_p(D)$, with equality just when $\nu_D(\EU_p(D))=1$. 
	
	Thus, $\Exp_\mu[\Exp_{\nu_D}[\Exp_p[\U]]]\leq \Exp_\mu[t_p(D)]$, with equality just when $\mu\{D\given \nu_D(\EU_p(D))=1\}=1$. 
	
	Now, observe that $\Exp_{p\times\mu}[\U(\nu)]=$
	
\end{proof}

\begin{theorem}\label{thm:EU appendix} 
	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
	\item  If $\nu$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ then, for any $\nu'$, $\Exp_\pb[\U(\nu)]\geq\Exp_\pb[\U(\nu')]$
	\item  If $\nu$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ and $\nu'$ does not, then $\Exp_\pb[\U(\nu)]>\Exp_\pb[\U(\nu')]$.
\end{enumerate}
\end{theorem}
\begin{proof}


\end{proof}



\begin{theorem}\label{thm:EU appendix} 
	\hspace*{1mm}
	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
		\item  If $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ then, for any $\s'$, $\Exp_\pb[\U(\s)]\geq\Exp_\pb[\U(\s')]$
		\item  If $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ and $\s'$ does not, then $\Exp_\pb[\U(\s)]>\Exp_\pb[\U(\s')]$.
	\end{enumerate}
\end{theorem}


\begin{proof}
	For any $\omega\in\Omega$ and $D\in\D'$, let $t_\pb(\omega,D):=\sup\{\Exp_{\pb(\cdot|D)}[\U(a)]\given a\in D\}$. We can define it arbitrarily for $D\notin \D'$. 
	Note that $t_\pb(\omega,D)$ only depends on $D$. Nevertheless, we define it as a function on $\Omega\times\D$ in order to take expectations relative to $\pb$. 
	
	Suppose $D\in\D'$. By definition of $t_\pb$, $\Exp_{\pb(\cdot|D)}[\U(a)]\leq t_\pb(\omega^*,D)$ for any $\omega^*\in\Omega$. 
	For $\prpickstrat$ a probabilistic picking strategy, $\prpickstrat_D$ is a measure over $D$, so $\U(\prpickstrat)(\omega,D):=\Exp_{\prpickstrat_D}[\U(a)(\omega)]=\sum_{a\in D}\prpickstrat_D(a)(\omega)$. Since 
	
	By definition of $\EU_{\pb(\cdot|-)}(D)$, $\Exp_{\pb(\cdot|D)}[\U(a)]= t_\pb(\omega^*,D)$ exactly when $a\in \EU_{\pb(\cdot|-)}(D)$. 
	
	
	
	Now, 
	
	%	Observe that $\Exp_{\pb(\cdot|D)}[\U(a)]\leq t_\pb(\omega,D)$ for all $a\in D$, with equality iff $a\in \EU_{\pb(\cdot|-)}(D)$, by definition of .\todoold{this is only true for $D\in\D'$!}
	
	%	Let $\X:=\{\left<\omega,D\right>\in\Omega\times\D\given f_\D(D)>0\}$.
	
	%	For any $\omega\in\Omega$, let $\X_\omega:=\left\{\left<\omega^*,D^*\right>\in\X\given \omega=\omega^*\right\}$.	
	
	%	For any $\omega\in\Omega$, let $\X_\omega:=\left\{\left<\omega,D\right>\given D\in\D'\right\}$.	
	
	
	
	{\allowdisplaybreaks
		\begin{align}
			\Exp_\pb[\U(\prpickstrat)]&=\sum_{\omega\in\Omega}\int_{\D} \U(\prpickstrat)(\omega,D) f(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\sum_{\omega\in\Omega}\int_{\D'} \U(\prpickstrat)(\omega,D) f(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\int_{\D'}\left[\sum_{\omega\in\Omega}\frac{f(\omega,D)}{f_\D(D)}\U(\prpickstrat)(\omega,D)\right]f_\D(D)\frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\int_{\D'}\Exp_{\pb(\cdot|D)}[\U(\prpickstrat)(\omega,D)]\;f_\D(D)\frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\sum_{\omega\in\Omega}\int_{\D'}\Exp_{\pb(\cdot|D)}[\U(\prpickstrat)(\omega,D)]\;f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
			&\leq\sum_{\omega\in\Omega}\int_{\D'}t_\pb(\omega,D)f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\sum_{\omega\in\Omega}\int_{\D}t_\pb(\omega,D)f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
			&=\Exp_\pb[t_\pb]
	\end{align}}
	
	Thus, $\Exp_\pb[\U(\s)]\leq \Exp_\pb [t_\pb]$ with equality iff $\Exp_{\pb(\cdot|D)}[\U(\prpickstrat)(\omega,D)]=t_\pb(\omega,D)$ almost everywhere, which is true iff $\pb\{\left<\omega,D\right> \in \Omega\times\D \given \s(D)\in \EU_{\pb(\cdot|-)}(D)\}=1$, i.e., iff $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$. Both (i) and (ii) follow immediately from this.
\end{proof}

\begin{color}{red}
\begin{theorem}\label{thm:EU appendix} 
\hspace*{1mm}
	\begin{enumerate}[label=\normalfont(\roman*), ref=\roman*]
		\item  If $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ then, for any $\s'$, $\Exp_\pb[\U(\s)]\geq\Exp_\pb[\U(\s')]$
		\item  If $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$ and $\s'$ does not, then $\Exp_\pb[\U(\s)]>\Exp_\pb[\U(\s')]$.
	\end{enumerate}
\end{theorem}


\begin{proof}
	For any $\omega\in\Omega$ and $D\in\D'$, let $t_\pb(\omega,D):=\sup\{\Exp_{\pb(\cdot|D)}[\U(a)]\given a\in D\}$. We can define it arbitrarily for $D\notin \D'$. 
	Note that $t_\pb(\omega,D)$ only depends on $D$. Nevertheless, we define it as a function on $\Omega\times\D$ in order to take expectations relative to $\pb$. 
	
	Suppose $D\in\D'$. By definition of $t_\pb$, $\Exp_{\pb(\cdot|D)}[\U(a)]\leq t_\pb(\omega^*,D)$ for any $\omega^*\in\Omega$. By definition of $\EU_{\pb(\cdot|-)}(D)$, $\Exp_{\pb(\cdot|D)}[\U(a)]= t_\pb(\omega^*,D)$ exactly when $a\in \EU_{\pb(\cdot|-)}(D)$.
	
	Now, 
	
%	Observe that $\Exp_{\pb(\cdot|D)}[\U(a)]\leq t_\pb(\omega,D)$ for all $a\in D$, with equality iff $a\in \EU_{\pb(\cdot|-)}(D)$, by definition of .\todoold{this is only true for $D\in\D'$!}
	
%	Let $\X:=\{\left<\omega,D\right>\in\Omega\times\D\given f_\D(D)>0\}$.
	
%	For any $\omega\in\Omega$, let $\X_\omega:=\left\{\left<\omega^*,D^*\right>\in\X\given \omega=\omega^*\right\}$.	
	
%	For any $\omega\in\Omega$, let $\X_\omega:=\left\{\left<\omega,D\right>\given D\in\D'\right\}$.	

	

{\allowdisplaybreaks
\begin{align}
\Exp_\pb[\U(\s)]&=\sum_{\omega\in\Omega}\int_{\D} \U(\s(D))(\omega) f(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)\\
&=\sum_{\omega\in\Omega}\int_{\D'} \U(\s(D))(\omega) f(\omega,D) \frac{1}{|\Omega|}\diff\lambda(D)\\
&=\int_{\D'}\left[\sum_{\omega\in\Omega}\frac{f(\omega,D)}{f_\D(D)}\U(\s(D))(\omega)\right]f_\D(D)\frac{1}{|\Omega|}\diff\lambda(D)\\
&=\int_{\D'}\Exp_{\pb(\cdot|D)}[\U(\s(D))]f_\D(D)\frac{1}{|\Omega|}\diff\lambda(D)\\
&=\sum_{\omega\in\Omega}\int_{\D'}\Exp_{\pb(\cdot|D)}[\U(\s(D))]f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
&\leq\sum_{\omega\in\Omega}\int_{\D'}t_\pb(\omega,D)f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
&=\sum_{\omega\in\Omega}\int_{\D}t_\pb(\omega,D)f(\omega,D)\frac{1}{|\Omega|}\diff\lambda(D)\\
&=\Exp_\pb[t_\pb]
\end{align}}

	Thus, $\Exp_\pb[\U(\s)]\leq \Exp_\pb [t_\pb]$ with equality iff $\Exp_{\pb(\cdot|D)}[\U(\s(D))]=t_\pb(\omega,D)$ almost everywhere, which is true iff $\pb\{\left<\omega,D\right> \in \Omega\times\D \given \s(D)\in \EU_{\pb(\cdot|-)}(D)\}=1$, i.e., iff $\s$ $\pb_\Decs$-surely picks for $\EU_{\pb(\cdot|-)}$. Both (i) and (ii) follow immediately from this.
	\end{proof}


\begin{theorem}\label{thm:EU appendix}Suppose $p$ is a probability function over $\Omega$ and $\mu$ is a measure over $\D$. Then:
	\begin{enumerate}
		\item  If $\prpickstrat$ $\mu$-surely picks for $\EU_p$ then, for any $\prpickstrat'$, $\EU_{p\times\mu}[\U(\prpickstrat)]\geq\EU_{p\times\mu}[\U(\prpickstrat')]$
		\item If $\prpickstrat$ $\mu$-surely picks for $\EU_p$, and $\prpickstrat'$ does not, then $\EU_{p\times\mu}[\U(\prpickstrat)]>\EU_{p\times\mu}[\U(\prpickstrat')]$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	Let $m_D(p):=\sup\{\Exp_p[\U(a)]\given a\in D\}$.
	
	So $\Exp_p[\U(a)]\leq m_D(p)$ for all $a\in D$, with equality only if $a\in \EU_p(D)$. 
	
	Since $\prpickstrat_D$ is a probability over $D$, we know that $\Exp_{\prpickstrat(D)}\Exp_p[\U]\leq m_D(p)$ with equality only if $\prpickstrat_D(\EU_p(D))=1$. And note that $\Exp_{\prpickstrat(D)}\Exp_p(a)=\Exp_p[\U(\prpickstrat)(D)]$
%	
%	{\color{cyan}
%		If $P$ is probability over $\Omega\times\D$, 
%		$\Exp_P[\U(\prpickstrat)]=\Exp_{P(\D)}\Exp_{P(\cdot|D)}\U(\prpickstrat)$. This is a standard fact about iterated expectations. We have shown that $\Exp_{P(\cdot|D)}\U(\prpickstrat)\leq m_D(p(\cdot|D))$, with equality iff $\prpickstrat_D(\EU_{P(\cdot|D)}(D))=1$. Thus, $\Exp_P[\U(\prpickstrat)]\leq\Exp_{P(\D)} m_D(p(\cdot|D))$ with equality only if $P(\{D\given\prpickstrat_D(\EU_{P(\cdot|D)}(D))=1 \}=1$, i.e., only if $\prpickstrat$ $P$-surely picks for $\EU_P$.)
%		
%%		$\Exp_P[\U(\prpickstrat)]=\sum_{D\text{ s.t., }P(D)\neq 0} P(D)\sum_\omega \frac{P(D\wedge\omega)}{P(D)}\U(\prpickstrat,D,\omega)$
%		}
		
	Thus, $\EU_{p\times\mu}[\U(\prpickstrat)]\leq \Exp_\mu [t(p)]$ with equality only if $\mu\Set {D \given \prpickstrat_D(\EU_p(D))=1} = 1$, i.e., only if $\prpickstrat$ $\mu$-surely picks for $\EU_p$. 
%	By definition of $\EU_p$, we have that $a\in\EU_p(D)$ iff $\Exp_p\U(a)\geq \Exp_p\U(b)$ for all $b\in D$. 
%	So if $a\in\EU_p(D)$ and $b\in D\setminus \EU_p(D)$ then $\Exp_p\U(a)>\Exp_p\U(b)$.
%	
%	Let $m_{p,D}:=\Exp_p(a)$ for any $a\in\EU_p(D)$, observing that this exists because we have assumed $D$ is finite so $\EU_p(D)\neq\emptyset$, and that it is well-defined because any $a,a'\in\EU_p(D)$ have $\Exp_p(a)=\Exp_p(a')$. 
%	
%	Thus, if $\prpickstrat_D(\EU_p(D))=1$, then $\Exp_{\prpickstrat(D)}\Exp_p(a)=m_{p,D}$. 
%	And if $\prpickstrat_D(\EU_p(D))\neq 1$, then $\Exp_{\prpickstrat(D)}\Exp_p(a)<m_{p,D}$.
%	
%	Thus, if $\prpickstrat$ does $\mu$-surely pick for $\EU_p$, i.e., $\mu\{D\given \prpickstrat_D(\EU_p(D))=1\}=1$, then $\Exp_\mu\Exp_{\prpickstrat(D)}\Exp_p(a)=\Exp_\mu m_{p,D}$
%	
%	If $\prpickstrat$ does not $\mu$-surely pick for $\EU_p$, then $\mu\{D\given \prpickstrat_D(\EU_p(D))\neq 1\}>0$, then $\Exp_\mu\Exp_{\prpickstrat(D)}\Exp_p(a)<\Exp_\mu m_{p,D}$. 
%	
%	Observe that $\EU_{p\times\mu}\Exp_{\prpickstrat(D)}a=\Exp_\mu\Exp_{\prpickstrat(D)}\Exp_p(a)$ by interchange of expectations. This obtains our result. \todooldinfo{is this cleaner than RPs writeup?}
\end{proof}
\end{color}


All the theorems about Expected Utility Theory and E-Admissibility, as well as the existence of Maximality strategies can be seen as quick corollaries of this: 
\Cref{thm:eu-self-rec,thm:eu-uniquely-optimal,thm:eu-nu-nec-suff,thm:eu-dep}
%\cref{thm:eu-self-rec,thm:eu-uniquely-optimal,thm:eu-nu-nec-suff,thm:eu-dep,thm:ead-equiv[indep],thm:ead-existence[dep],thm:ead-existsimpermissibel[indep],thm:ead-nu-nec-suff,thm:ead-nu-reg-nec,thm:ead-suff-indep,thm:max-suff,thm:max-suff[dep],thm:max-nu-suff,thm:ead-equiv[dep]}

\begin{comment}
	
In fact, we can create a version of this result allowing dependence between the decision problem you're faced with and the state of the world, assuming that when you make a decision you first conditionalise your probabilities on which decision problem you're faced with and then use expected utility using that updated probability to make the decision. 
This is because $\Exp_\pb[\U(\prpickstrat)]=\Exp_{\pb_\D}\Exp_{\pb(\cdot|D)}\U(\prpickstrat)$, where $\pb_\D$ is the marginal. Each $\Exp_{\pb(\cdot|D)}\U(\prpickstrat)$ is maximised when $\prpickstrat_D(\EU_{\pb(\cdot|D)}(D))=1$, at least, whenever this is well-defined. So, similarly, $\Exp_\pb[\U(\prpickstrat)]$  is maximised when $\pb_\D\{D\given \prpickstrat_D(\EU_{\pb(\cdot|D)}(D))=1\}=1$.
\end{comment}

%Since, 
%		If $P$ is probability over $\Omega\times\D$, 
%$\Exp_P[\U(\prpickstrat)]=\Exp_{P(\D)}\Exp_{P(\cdot|D)}\U(\prpickstrat)$. This is a standard fact about iterated expectations. We have shown that $\Exp_{P(\cdot|D)}\U(\prpickstrat)\leq t_D(p(\cdot|D))$, with equality iff $\prpickstrat_D(\EU_{P(\cdot|D)}(D))=1$. Thus, $\Exp_P[\U(\prpickstrat)]\leq\Exp_{P(\D)} t_D(p(\cdot|D))$ with equality only if $P(\{D\given\prpickstrat_D(\EU_{P(\cdot|D)}(D))=1 \}=1$, i.e., only if $\prpickstrat$ $P$-surely picks for $\EU_P$.)




\end{colored}\begin{colored}{violet}
\subsection{E-Admissibility}
This will then imply the results about E-Admissible strategies, as what is judged as E-Admissible is just those options which are maximisers of expected utility for one of the members of the credal set, and thus are just the expected utility picking strategies. 
\cref{thm:ead-equiv[indep],thm:ead-existence[dep],thm:ead-existsimpermissibel[indep],thm:ead-nu-nec-suff,thm:ead-nu-reg-nec,thm:ead-suff-indep,thm:ead-equiv[dep]} thus follow.


As Maximality is a more permissive theory than E-Admissibility, it thus also implies our result that there exist some Maximality strategies which are themselves judged as Maximal: \cref{thm:max-suff,thm:max-suff[dep],thm:max-nu-suff}


So we have proved \cref{thm:eu-self-rec,thm:eu-uniquely-optimal,thm:eu-nu-nec-suff,thm:eu-dep,thm:ead-equiv[indep],thm:ead-existence[dep],thm:ead-existsimpermissibel[indep],thm:ead-nu-nec-suff,thm:ead-nu-reg-nec,thm:ead-suff-indep,thm:max-suff,thm:max-suff[dep],thm:max-nu-suff,thm:ead-equiv[dep]}


\end{colored}
\section{Maximality}
To prove \cref{thm:max-nu,thm:max-nu-reg-nec}, we turn to two versions of Abraham Wald's (\citeyear{wald1947cct}) Complete Class Theorem. We'll state them in a general setting and then explain how they apply to our case. To state them, we need some definitions.
\subsection{Wald theorem}


\begin{itemize}
	\item $\Omega$ is a finite set of states.
	\item Probability $p$ is a normalised non-negative function. For $X$ a random variable, $X\in\Re^\Omega$, i.e., $X:\Omega\to\Re$, will also write $p(X)=\sum_{\omega\in\Omega}p(\omega)X(\omega)$ applying the probability function to random variables. 
	\item $\O$ is a set of ``options''. In our application they will be various (probabilistic) picking strategies, $\prpickstrat$.
	\item $\Uwald:\O\times\Omega\to [l,u]$ is a bounded ``utility function''. In our application, it will be $\Uwald(\prpickstrat,\omega)=\Exp_{\mu^*}\U(\prpickstrat,\omega)=\Exp_{\mu^*}\Exp_{\prpickstrat_D}\U(a)(\omega)$ \begin{itemize}
		\item We have assumed it is bounded; however all that is actually needed for the proof is that it is bounded above, i.e., we could allow $\Uwald:\O\times\Omega\to (-\infty,0]$. 
		\item The utility profile of an option is $\Uwald(o)\in\Re^\Omega$ as expected, i.e., $\Uwald(o)(\omega)=\Uwald(o,\omega)$, however $\Uwald$ is specified. 
	\end{itemize}
\end{itemize}


%$\O$ is the set of picking functions. 
%
%We have $\Uwald:\O\to [0,1]$; as we have assumed that utilities are bounded. In fact, all that is needed for the result is that they are bounded above; we could have $\Uwald:\O\to(-\infty,0]$. 
%Background assumptions:
%\begin{itemize}
%	%	\item Finitely many possibilities, $\Omega$. 
%	\item Utilities are bounded (at least bounded above). This is essential, though it also follows from the existence of Bayes optimal points [I think, somehow spelled out]. 
%	\item Utilities are finite [should be removable]
%\end{itemize}
%$\O$ is a collection of ``options''. 
%In our setting, these will be the picking strategies, probabilistic or deterministic. 
%Perhaps they are possible credence functions, or other kinds of uncertainty models, or update rules, or decision theories, or ice cream flavours. We assume we have some measure of utility of those options, simply a function $\Uwald:\O\times\Omega\to\Re$. This might be epistemic utility, or practical utility, or whatever else it is that one is trying to optimise. 
%We can think of $\Uwald(o)$ as a vector in $\Re^\Omega$. 


%
%
%\begin{definition}
%	A probability $p$ is a non-negative normalised linear functional on $\Re^\Omega$. That is, $p:\Re^\Omega$ such that
%	\begin{enumerate}
%		\item Non-negative: $X(\omega)\geq Y(\omega)\implies p(X)\geq p(Y)$
%		\item Normalised: $p(\mathbf{1})=\mathbf{1}$
%		\item Linear: $p(cX+dY)=cp(X)+dp(Y)$.
%	\end{enumerate}
%	When $\Omega$ is finite, we might think of such $p$ as specified by probability mass function $p(\omega)$ for each $\omega$ which are non-negative and normalised. Then for any $X\in\Re^\Omega$, $p(X)=\sum_\omega p(\omega)X(\omega)$. 
%	
%	The utility profile of an option is $\Uwald(o)\in\Re^\Omega$ as expected, i.e., $\Uwald(o)(\omega)=\Uwald(o,\omega)$, however $\Uwald$ is specified. 
%	
%	An option $o$ is Bayes for $p$ in $\O$  iff $p(\Uwald(o))\geq p(\Uwald(o'))$ for every $o'\in \O$. 
%	%	
%	%	We're working in infinite dimensions. A ``probability'' $p$ is just a linear functional which is positive and normalised. That is, we're merely assuming finite additivity. The notion of being Bayes is defined similarly. 
%\end{definition}


%Consider 












 %They don't require the Bayes option to be identical to the original probabilities. 


%I believe that JK's addition to the usual Wald is to identify the features of $\Uwald$ that allows the argument to go through rather than relying on the integral form of the utility function. 


%
%We can start thinking of everything just with vectors. Let
%\begin{equation}
%	\Uwald(\O):=\{\Uwald(o)\given o\in \O\}\subseteq\Re^\Omega
%\end{equation}
%\begin{definition}Bayes-existing and Bayes-continuous iff: for every probability $p$, there is some $X\in \Uwald(\O)$ which is Bayes for $p$. And if $o$ is Bayes for $p$ and $p_1,p_2,\ldots$ are probabilities converging to $p$ then there is some sequence of $y_n\in \Uwald(\O)$, with each $y_n$ Bayes for $p_n$, and tBayes options for $p_n$ whose which converge to $
%\end{definition}






%\begin{definition}
%	$\Uwald$ is Bayes-existing on $\O$ iff for every probability $P$ there is some $o\in \O$ which is Bayes for $P$. 
%\end{definition}
\begin{definition}
	$o$ is \emph{strictly dominated} iff there is $o'\in\O$ with $\Uwald(o',\omega)>\Uwald(o,\omega)$ for all $\omega$. $o$ is called \emph{admissible} if it is not strictly dominated. 
	
	$o$ is \emph{weakly dominated} iff there is $o'\in\O$ with $\Uwald(o',\omega)\geq\Uwald(o,\omega)$ for all $\omega$ and $\Uwald(o',\omega)>\Uwald(o,\omega)$ for some $\omega$ .
	
	For a probability function $p$ over $\Omega$ we will use $p(\Uwald(o)):=\Exp_p[\Uwald(o)]$. \todo{are we assuming finite $\Omega$??}
	$o$ is \emph{Bayes for $p$} iff $o\in\EU_p(\O)$; that is, for all $o'\in\O$, $p(\Uwald(o))\geq p(\Uwald(o'))$. 
	$o$ is \emph{Bayes} iff there is some probability $p$ such that $o$ is Bayes for $p$. \todo{I don't think we'd defined it! added it here. }
\end{definition}






%This tells us that to avoid dominance, you need to be the sort of thing that probabilities think are optimal. Even if $\Uwald$ is measuring accuracy of credences themselves, it doesn't tell us that probabilities themselves are the things that are non-dominated unless one has an additional assumption that probabilities themselves are exactly the kinds of things that are evaluated as optimal by probabilities, which is exactly what strict propriety gives us. 
%We need to show:
%\begin{itemize}
%	\item If $o$ is Bayes then it is not strictly dominated; and if is Bayes for a regular probability then it is not weakly dominated. 
%	\item If $o$ is not weakly dominated, then it is Bayes.
%	\item If $o$ is merely weakly dominated, i.e., weakly dominated but not strictly dominated, then it is Bayes for a irregular probability. \todoold{is this the way we show it? Or do we do the whole proof again for the strict one?}
%\end{itemize}
%Moreover, \begin{itemize}
	%	\item If $o$ is weakly dominated, then there is $o'$ which dominates it which is Bayes (and thus is not itself even weakly dominated).
	%\end{itemize}\todoold{does this follow from the above? No. Actually, it's not true without other assumptions. E.g, when $\Uwald(\O)$ is convex but not necessarily closed then it isn't true. It is what we prove in the cts case.}
	
	%Maybe we should instead show:
	
	%\begin{theorem}[Complete Class Theorem]\label{thm:cct}\ 
	%	\begin{enumerate}[{\normalfont (I)}]
		%		\item If $o$ is Bayes then it is admissible. 
		%		\item Suppose $\Uwald$ is convex on $\O$. Then, $o$ is Bayes iff $o$ is admissible.
		%		\item Suppose $\Uwald$ is Bayes-continuous on $\O$ and Bayes-existing on $\O$. Then, $o$ is Bayes iff $o$ is admissible.
		%	\end{enumerate}
	%\end{theorem}
	
	
	
	\begin{lemma}\label{thm:cct:Bayes implies admiss}
		If $o$ is Bayes then it is not strictly dominated.
	\end{lemma}
	\begin{proof}
		Suppose it is strictly dominated. Then there is $o'$ with $\Uwald(o,\omega)<\Uwald(o',\omega)$ for all $\omega$. So every probability $p$ will have $p( \Uwald(o))<p(\Uwald(o'))$\todo{Have we introduced this notation yet?}. Thus it is not Bayes.
		%	
		%	Suppose $o$ is weakly dominated. Then every \emph{regular} probability has $p\cdot \Uwald(o')>p\cdot \Uwald(o)$, so it is not Bayes for any regular probability. 
	\end{proof}
	
	
	We will start thinking directly about vectors in $\Re^\Omega$. 
	\begin{align} 
		\Uwald(\O) &:= \Set{\Uwald(o)\given o\in \O}\subseteq\Re^\Omega\\
		\Conv(\Uwald(\O))&:=\mathsf{ConvHull}(\Uwald(\O))
	\end{align}
	We will also work with $\cl(\Conv(\Uwald(\O)))$, which we define as the closure in the product topology. This can also be characterised by limits of sequences, or more generally of nets: if a sequence (or net) of members of $\Conv(\Uwald(\O))$ is such that for each coordinate, $\omega$, $X_\alpha(\omega)\longrightarrow X^*(\omega)$, then $X^*\in\cl(\Conv(\Uwald(\O)))$. 
	
%	We extend the notion of being Bayes for vectors as follows.
%	\begin{definition}[Bayes for vectors]
%		$X\in\Re^\Omega$ is \emph{Bayes for $p$ in $\Uwald(\O)$} iff $p(X)\geq p(Y)$ for all $Y\in \Uwald(\O)$.
%		$X$ is \emph{Bayes in $\Uwald(\O)$} if there is some probability $p$ where $X$ is Bayes for $p$ in $\Uwald(\O)$.
%	\end{definition}
%	Note that we haven't actually required $X$ to be in the set $\Uwald(\O)$ for it to be Bayes in $\Uwald(\O)$. The proof doesn't rely on this fact in any way, but it means that we're proving something a bit stronger: Bayes is equivalent to being non-dominated in general, whether the $X$ is in $\Uwald(\O)$ or not.
	
	The notion of being dominated by something in a set is defined for vectors as is obvious. 
	
	\begin{infversion}
		I'm allowing infinite disutility. $\Uwald:\Re^\Omega\to[-\infty,+\infty)$. 
	\end{infversion}
	
	Our first lemma says that if a vector is not Bayes in $\Uwald(\O)$ then it is strictly dominated in the convex hull.
	\begin{lemma}\label{thm:cct:admiss in conv are Bayes in conv}
		If $X\in\Re^\Omega$ is such that there is no probability $p$ with $p(X)\geq p(\Uwald(o))$ for all $o\in\O$, then there is $Y\in\Conv(\Uwald(O))$ which strictly dominates $X$, i.e., $Y(\omega)>X(\omega)$ for all $\omega$.
	\end{lemma}
	\begin{proof}
		Suppose $X$ is not strictly dominated in $\Conv(\Uwald(\O))$. 
		Let $$\Dom_X :=\{Y\in \Re^\Omega\given \text{for all $\omega$, }Y(\omega)>X(\omega)\},$$ i.e., the \begin{infversion}
			finite
		\end{infversion} strict dominators of $X$; so we know that $\Dom_X $ and $\Conv(\Uwald(\O))$ \begin{infversion}
			$\cap Fin$ [qu: might it be empty?; we know that $\Dom_X $ is non-empty]
		\end{infversion}are disjoint by assumption. They are also both convex so they can be separated by a linear functional, by the separating hyperplane theorem  
		\todo{Find a proper reference for what we need!}
%		\citep[Theorem 5.61]{aliprantis2006infinite}. 
%		\todo{this ref doesn't get cts!! we don't actually need it though. So i removed from statement}
%		That is, there is a non-zero linear functional, $f:\Re^\Omega\to\Re$ with $f\neq 0$ and constant $c$ s.t.,  $f(Y)\geq c \geq f(Z)$ for any $Y\in \Dom_X $ and $Z\in \Conv(\Uwald(\O))$.\footnote{In fact, to apply their result, we need that at least one of them has an internal point. In finite dimensions, this is interior points \citep[Thm.~5.60]{aliprantis2006infinite}, and observe that $\Dom_X$ is non-empty and open, so has interior points. 
%%		 \begin{infversion}
%%			Although, it may fail if we allowed $+\infty$; so we'd need to impose that utilities are not $+\infty$ to get this bit.
%%		\end{infversion}
%	}\todo{it needs internal rather than interior opint,...}

		
		We need to show that $f$ is non-negative. Let $Z\geq 0$ and suppose $f(Z)<0$. Then take any $Y\in \Dom_X $ and consider $Y+kZ$, observing that it is still in $\Dom_X $. And $f(Y+kZ)=f(Y)+kf(Z)$ can be arbitrarily small, and in particular less than $c$ by making $k$ large enough. Contradicting. 
		
		We also know that $f(\omega)\neq 0$ for some $\omega$, otherwise it would be $0$ everywhere (as by linearity, it has the form $f(X)=\sum_{\omega\in\Omega}f(\omega)X(\omega)$) and the theorem gives us a non-zero linear functional.\todo{iadded expln}
		
%				Now, any non-trivial monotone linear functional must have $f(\vec{1})\neq 0$.\todoold{why?????} And thus, we can ``normalise'' $f$ by dividing through by $f(\vec{1})$. \todoold{footnote that this isn't the usual notion of normalisation.}
		
		We can thus normalise $f$ to obtain our probability $p$ with $p(Y)\geq c \geq p(Z)$ for any $Y\in \Dom_X $ and $Z\in \Conv(\Uwald(\O))$.\begin{infversion}
			$\cap Fin$. If $Z$ not in Fin, then we also want to check that $p(Z)<c$. 
		\end{infversion}
		
		Since $X\in\cl(\Dom_X )$, also $p(X)\geq c$. Thus, $p(X)\geq p(Z)$ for all $Z\in \Uwald(\O)$ (as $\Uwald(\O)\subseteq \Conv(\Uwald(\O))$) as required.
	\end{proof}
	
		This now gives us:
	\begin{corollary}
		If $\Uwald(\O)$ is convex, then 
		$o$ is Bayes iff $o$ is not strictly dominated.
	\end{corollary}
	Note that this requires $\Uwald(\O)$ to be convex, not $\O$. This doesn't however, show that if it is not Bayes it is strictly dominated by something with is itself not dominated, and thus is Bayes. We will get that from the next result. 
	
	%\begin{corollary}
	%	Suppose $\Uwald(\O)$ is convex. If $X$ is not Bayes in $\Uwald(\O)$ then it is strictly dominated in $\Uwald(\O)$. \todoold{For this case we don't have the result that it's strictly dominated by a Bayes option using \cref{thm:cct:dominator on boundry} and the fact that the $Z\in\cl(\Uwald(\O))$ which dominates it is not weakly dominated, and thus, by what we already have here, it is Bayes.. }
	%\end{corollary}
	%\begin{proof}
	%	Observe that $\Conv(\Uwald(\O))=\Uwald(\O)$ by assumption of convexity. So it follows immediately from \cref{thm:cct:admiss in conv are Bayes in conv}
	%\end{proof}
	
%	Our second lemma says that if $X$ is strictly domiant
	\begin{lemma}\label{thm:cct:dominator on boundry}
		Suppose $X$ is strictly  dominated in $\Conv(\Uwald(\O))$. Then we can find $Z\in\cl(\Conv(\Uwald(\O)))$ which strictly dominates $X$ and is itself not even weakly dominated in $\Conv(\Uwald(\O))$. \begin{infversion}
			and $Z$ in Fin.
		\end{infversion}
		%	If $X$ is not Bayes in $\Uwald(\O)$ then there is $X'\in\cl(\Conv(\Uwald(\O)))$ s.t. $X'$ dominates $X$ and $X'$ is non-dom in $\Conv(\Uwald(\O))$. 
	\end{lemma}
	In the Wald setting, the $Z\in\cl(\Conv(\Uwald(\O)))$ which are not weakly dominated is called the lower boundary of the set (in our case, it would be ``upper boundary'' because we're working with positive utility rather than risk or disutility). This then says that being dominated entails that one is dominated by something in the lower boundary. It essentially depends on the fact that $\Uwald(\O)$ is bounded from above. 
	%\todoold{Note: just improving it in each coordinate isn't sufficient. }
	
	
	\begin{proof}
		Take any $Y\in \Conv(\Uwald(\O))$ which dominates $X$. Consider $A:=\{Z\in \cl(\Conv(\Uwald(\O)))\given  Z(\omega)\geq Y(\omega)\text{ for all $\omega$}\}$. Observe that this is closed (as it is the intersection of two closed sets) and bounded (as we assumed that utilities were bounded above), and thus compact. Let $f(Z):=\sum_\omega Z(\omega)$, observing that it is a continuous function. Thus, by the Extreme Value Theorem, it obtains its maximum somewhere in $A$. This maximum point will be as required. 
	\end{proof}
	%\begin{proof}
	%	[Rewriting that more explicitly, not sure it helped though!]
	%	Start with any point, $x_0\in\cl(\Conv(\Uwald(\O)))$, that strictly dominates $X$. Then if possible, take $x_1$ to be a point in $\cl(\Conv(\Uwald(\O)))$ weakly dominating $x_0$, if it is not possible, then we have already found our desired $Z$. Continue this, choosing points in $\cl(\Conv(\Uwald(\O)))$ which weakly dominate the previous point, if possible, and if not possible, we are done: we have found the desired $Z\in\cl(\Conv(\Uwald(\O)))$ which is not weakly dominated. If this process didn't halt, then we have a sequence $\langle x_0,x_1,\ldots\rangle$ each of which is in $\cl(\Conv(\Uwald(\O)))$ and weakly dominates the previous one. We can then consider extending this sequence to the transfinite. Take $x_\omega$ to be the (pointwise) supremum of this sequence, observing that it is a limit point of the sequence [topology of pointwise convergence] so is in $\cl(\Conv(\Uwald(\O)))$, and it weakly dominates all the previous points, and we can then continue the construction from there, through the transfinite. Since there are at most continuum-many points to be chosen from, this process must at some point terminate. And since one can always take the limit point of any such sequence and check that it is in $\cl(\Conv(\Uwald(\O)))$, it must be an actual point that is found where the process terminates, i.e., giving us our required $Z$.
	%\end{proof}
	
	It can alternatively be proved with an application of Zorn's lemma, an argument which also works when $\Omega$ is infinite.\footnote{Define $\preccurlyeq$ a partial order on $A$ as the natural coordinatewise order. For any chain, consider its pointwise supremum, which exists because utilities are bounded above, and checking that it is in the $A$ since it is closed and this is a pointwise limit. Consequently, every chain has an upper bound, allowing the application of Zorn's lemma to guarantee the existence of a maximal element, which will be as required. } 
	
%	This can be proved by an extreme value theorem. We instead prove it directly. This is because the proof carries over to the infinite dimensional setting. 
%	
%	\begin{proof}	
%		Let $E:=\{Y\in\cl(\Conv(\Uwald(\O)))\given Y(\omega)>X(\omega)\text{ for all $\omega$}\}.$
%		
%		
%		Define $\preccurlyeq$ a partial ordering on $\Re^\Omega$ by $Y\preccurlyeq Z$ iff $Y(\omega)\leq Z(\omega)$. This partially orders $E$. We will use Zorn's lemma.
%		
%		Let $C$ be a chain in $E$, i.e., a collection of members of $E$ which is totally ordered by $\preccurlyeq$, i.e., where for $Y,Y'\in C$, $Y\preccurlyeq Y'$ or $Y'\preccurlyeq Y$. 
%		
%		Define $Z_C\in\Re^\Omega$ by $Z_C(\omega):=\sup\{Y(\omega)\given Y\in C\}$. Since utilities are bounded above for each $\omega$, this is well defined. 
%		
%		$Z_C(\omega)=\sup\{Y(\omega)\given Y\in C\}\geq Y(\omega)$ for all $Y\in C$. Thus $Z_C\succcurlyeq Y$ for all $Y\in C$. Also $Z_C(\omega)\geq Y(\omega)>X(\omega)$. So, to check that $Z_C$ is an upper bound for $C$ which is in $E$ we just need to check that $Z_C\in \cl(\Conv(\Uwald(\O)))$. 
%		
%		Since $C$ is a totally ordered by $\preccurlyeq$, it is a directed set, and the collection $\{Y\}_{Y\in C}$ defines a net in $\Re^\Omega$. For any fixed $\omega\in\Omega$, $\{Y(\omega)\}_{Y\in C}$ is a non-decreasing net of real numbers, and thus converges to $Z_C(\omega)=\sup\{Y(\omega)\given Y\in C\}$.
%		Since we are working with the product topology, this coordinatewise convergence implies that the net $\{Y\}_{Y\in C}$ converges to $Z_C$; and thus, since each $Y\in E\subseteq \cl(\Conv(\Uwald(\O)))$, by the closeness of the set, also $Z_C\in\cl(\Conv(\Uwald(\O)))$. 
%		
%		We have thus shown that $Z_C$ is  an upper bound for the chain $\C$ in $E$. 
%		We can now apply Zorn's lemma to show the existence of a maximal element in $E$. That is, some $Z\in E$ with $Y\succcurlyeq Z$ and $Y\in E$ implies $Y=Z$. This $Z$ is in $\cl(\Conv(\Uwald(\O)))$ and strictly dominates $X$. It is also not even weakly dominated in $\cl(\Conv(\Uwald(\O)))$, after all, any such weak dominator would be in $E$, contradicting the maximality of $Z$. 
%	\end{proof}
	
	
	
	
	
	
	
	
	
%	\begin{proof}
%		\begin{infversion}
%			If $Y$ strictly dominates $X$, then $Y$ in Fin. 
%		\end{infversion}
%		Starting with any point that strictly dominates $X$, generate Y (possibly transfinite) sequence of members of $\cl(\Conv(\Uwald(\O)))$ each of which weakly dominates all the previous ones until no more can be found. This sequence might be transfinite but it will terminate, at least by the cardinality of $\Re^\Omega$. In fact, it must terminate at some successor ordinal, i.e., some point $Z\in\cl(\Conv(\Uwald(\O)))$ which is not weakly dominated in $\cl(\Conv(\Uwald(\O)))$, which will give us our required $Z$. 
%		
%		This is because, if our sequence is indexed by a limit ordinal, then we can consider the pointwise supremum, which exists because we have assumed that utilities are bounded above. And since the points in the sequence weakly dominate the earlier ones, and it is indexed by a limit ordinal, the supremum point is distinct from all the points in the sequence, and weakly dominates them all. This supremum point is the limit point of this sequence in the topology of pointwise convergence, because in each coordinate we have a non-decreasing sequence thus converging to its supremum. Thus, since $\cl(\Conv(\Uwald(\O)))$ is closed, i.e., it contains all its limit points, this pointwise supremum is in $\cl(\Conv(\Uwald(\O)))$ and it weakly dominates all the previous points. Thus, the sequence may be extended beyond this limit-ordinal.
%	\end{proof}
%	
%	\begin{proof}
%		[Alternative proof following JK]
%		
%		
%		Define $\preccurlyeq$ an ordering on $\Re^\Omega$ by $Y\preccurlyeq Z$ iff $Y(\omega)\leq Z(\omega)$ for all $\omega$. Take any point, $Y_0$, which strictly dominates $X$. Then let  $D:=\{Z\in \cl(\Conv(\Uwald(\O)))\given Z\succcurlyeq Y_0\}$. This is partially ordered by $\preccurlyeq$. 
%		
%		Moreover, observe that $D$ is compact. To show this, first observe that $D\subseteq \bigtimes_{\omega} [Y_0(D),\mathsf{upperbound}\Uwald(\omega)]$, which is compact by Tychonoff's theorem. And $D=\cl(\Conv(\Uwald(\O)))\cap\{Z\given Z\succcurlyeq Y_0\}$, both of which are closed, the former by stipulation, the latter since it is $\bigcap_\omega\{Z\given Z(\omega)\geq Y_0(\omega)\}$, each component of which can be seen to be closed, so it is an arbitrary intersection of closed sets. 
%		
%		So $D$ is a closed subset of a compact set, and thus is itself compact. 
%		
%		And thus, every net has a cluster point. $D$ is itself a directed set under $\preccurlyeq$. NO ITS NOT! ITS NOT DIRECTED... GEQ Y0 PART OF IT IS DIRECTED BUT NOT THE CL U PART. 		
%		
%		
%		
%		
%		
%	\end{proof}


%We say: \begin{definition}
%	$\Uwald(\O)$ is \emph{closed from above} iff for every $Z\in\cl(\Conv(\Uwald(\O)))$ which is not weakly dominated in $\Conv(\Uwald(\O))$ is itself in $\Uwald(\O)$. 
%\end{definition}
%\begin{corollary}
%	If $\Uwald(\O)$ is closed from above, then $o$ is Bayes iff $o$ is not strictly dominated. 
%\end{corollary}
%	This immediately gets us:
	
%	This now gives us:
%	\begin{corollary}
%		If $\Uwald(\O)$ is closed and convex, then 
%		$o$ is Bayes iff $o$ is not strictly dominated.
%	\end{corollary}
%	Note that this requires $\Uwald(\O)$ to be closed and convex, not $\O$. 
	
	
	This will not apply to our general case yet. For that, we need further assumptions on $\Uwald$ and $\O$. 
\todo{are these assns on U or O?}



\begin{definition}\ 
	\begin{itemize}
		\item $\O$ is \emph{Bayes-existing} (relative to $\Uwald$) iff for every probability $p$ there is some $o\in\O$ which is Bayes for $p$.
		\item $\Uwald$ is \emph{Bayes-continuous} iff for all $o\in\O$, if $o$ is Bayes for $p$ and $p_1,p_2,\ldots$ are probabilities converging to $p$ then there is some sequence of options $o_1,o_2,\ldots$, each of which is Bayes for $p_n$ whose utility profiles converge to those of $o$. 
	\end{itemize}
\end{definition}
For example, if $\Uwald$ is a measure of accuracy which is continuous and strictly proper, then these will be satisfied. They are, however, \emph{much} weaker than that.
	
	\begin{lemma}\label{thm:cct:bdry are the Bayes optimal}
		Suppose $\Uwald$ is Bayes-continuous and Bayes-existing. 
		
		
		Suppose $Z\in\cl(\Conv(\Uwald(\O)))$ is not weakly dominated in $\Conv(\Uwald(\O))$\begin{infversion}
			and $Z$ in Fin.
		\end{infversion}, then in fact $Z\in \Uwald(\O)$, moreover, it is in fact Bayes optimal in $\Uwald(\O)$. 
%		
%		The proof also shows t
	\end{lemma}
	In the Wald setting, that is, that the lower boundary of $\Conv(\Uwald(\O))$ is a subset of $\Uwald(\O)$. This is then called $\Uwald(\O)$ being ``closed from below'' [in our case, above, as we're doing positive utility not loss]. This proof is where the assumptions on $\Uwald$ are vital. 
	
	\begin{proof}
		%	\todoold{JK wanted a more direct proof rather than something like this pf contradiction; but it wont work. It relies essentially on the Bayes existing part - that there is such $\Uwald(o_{p^*})$, so will have to involve that directly. Bayes cty itself doesn't tell us anything about $Z$ which isn't in $\Uwald(\O)$.}
		Since $Z$ is not weakly dominated, it is also not strictly dominated. So by \cref{thm:cct:admiss in conv are Bayes in conv}, there is some $p^*$ with $p^*(Z)\geq p^*(\Uwald(o))$ for all $o$. 
		
		By Bayes existing, there is also some $o_{p^*}$ which is Bayes optimal for $p^*$ in $\Uwald(\O)$. 
		We will use Bayes continuity and existing to show that $\Uwald(o_{p^*})(\omega)\geq Z(\omega)$ for every $\omega$, i.e., either $\Uwald(o_{p^*})=Z$ or $\Uwald(o_{p^*})$ weakly dominates $Z$, which would contradict our assumption that $Z$ is not weakly dominated. 
		
		So, what we need to show is that for every  $\omega$, $\Uwald(o_{p^*})(\omega)\geq Z(\omega)$. 
		
		The argument we will make will work for every $\omega$. So hold fixed a single $\omega$, call it $\omega^*$. Let $\pi_{\omega^*}$ be the projection function for the ${\omega^*}$ that we are considering, i.e., $\pi_{\omega^*}(Y):=Y({\omega^*})$. Then define $p_n$ by:
		\begin{align}
			p_n=(1-\sfrac{1}{n})p^*+\sfrac{1}{n} \pi_{{\omega^*}}
		\end{align}(this depends on the $\omega^*$ under consideration). Observe that $p_n$ is probabilistic, so by Bayes existing, for each $n$, there is some $o_{p_n}$ which is Bayes optimal for $p_n$, i.e., $p_n(\Uwald(o_{p_n}))\geq p_n(\Uwald(o))$ for all $o$. Since we have assumed $Z\in\cl(\Conv(\Uwald(\O)))$, also $p_n( \Uwald(o_{p_n}))\geq p_n(Z)$.\footnote{To show this, we first observe it for any $Z\in \Conv(\Uwald(\O))$, just taking a mixture, and then taking limits can't break a $\geq$.}\todoold{should it be an official separate lemma?}
		
		We also know that $p^*(Z)\geq p^*(\Uwald(o_{p_n}))$, so, since $p_n$ is a mixture of $p^*$ and $\pi_{\omega^*}$,  to get that $p_n( \Uwald(o_{p_n}))\geq p_n(Z)$ we must in fact have that $\pi_{{\omega^*}}(\Uwald(o_{p_n}))\geq \pi_{\omega^*}(Z)$. That is, we can conclude that $\Uwald(o_{p_n})(\omega^*)\geq Z(\omega^*)$. 
		
		%		We will now show that $\Uwald(o_{p_n})({{\omega^*}^*})\geq Z({{\omega^*}^*})$ for all ${{\omega^*}^*}$. Suppose, for contradiction, that $Z({{\omega^*}^*})>\Uwald(o_{p_n})({{\omega^*}^*})$. Then $\pi_{{\omega^*}^*}(Z)>\pi_{{\omega^*}^*}(\Uwald(o_{p_n}))$. Since we know that $Z$ is Bayes for $p^*$ in $\Uwald(\O)$ and $\Uwald(o_{p_n})\in \Uwald(\O)$, also $p^*(Z)\geq p^*(\Uwald(o_{p_n}))$. Since $p_n$ is a mixture of $p^*$ and $\pi_{{\omega^*}^*}$ we would thus have $p_n(Z)>p_n(\Uwald(o_{p_n}))$, contradicting our choice of $\Uwald(o_{p_n})$ being Bayes for $p_n$. 
		
		Observe that $p_n\longrightarrow p^*$. So, by Bayes continuity, $\Uwald(o_{p_n})\longrightarrow \Uwald(o_{p^*})$, so $\Uwald(o_{p_n})({\omega})\longrightarrow \Uwald(o_{p^*})({\omega})$ for each ${\omega}$. Thus, since $\Uwald(o_{p_n})({\omega^*})\geq Z({\omega^*})$ for all $n$, also $\Uwald(o_{p^*})(\omega^*)\geq Z(\omega^*)$. 
		
		Since this worked for any $\omega$, i.e., for any $\omega$ we could construct the relevant sequence and apply this argument, we have in fact obtained that $\Uwald(o_{p^*})(\omega)\geq Z(\omega)$ for all $\omega$, i.e., either $\Uwald(o_{p^*})=Z$ or $\Uwald(o_{p^*})$ weakly dominates $Z$, which would contradict our assumption that $Z$ is not weakly dominated. 
	\end{proof}
	This proof in fact shows that the assumptions on $\Uwald$ are very strong. It shows that for every probability there is a unique member of $\cl(\Conv(\Uwald(\O)))$ which is not weakly dominated; thus also that for every regular probability, there is a unique Bayes option. 
	
%	\begin{theorem}
%		Suppose $\Uwald$ is Bayes existing and Bayes continuous. Then if $X$ is not Bayes in $\Uwald(\O)$, it is strictly dominated in $\Uwald(\O)$, moreover, it is strictly dominated by a Bayes point in $\Uwald(\O)$.
%	\end{theorem}
%	\begin{proof}
%		If $X$ is not Bayes in $\Uwald(\O)$, then, by \cref{thm:cct:admiss in conv are Bayes in conv}, it is strictly dominated in $\Conv(\Uwald(\O))$. 
%		
%		By \cref{thm:cct:dominator on boundry} it is thus strictly dominated by some $Z\in\cl(\Conv(\Uwald(\O)))$ which is itself not weakly dominated in $\Conv(\Uwald(\O))$. By \cref{thm:cct:bdry are the Bayes optimal}, in fact $Z\in \Uwald(\O)$, and moreover, it is in fact a Bayes point; as required.
%	\end{proof}
%	This applies to any $X$, whether or not it is in $\Uwald(\O)$ itself, but of course we actually want the result for $o$. 
	\begin{theorem}\label{thm:cct}
		Suppose $\Uwald$ is Bayes existing and Bayes continuous. If $o$ is not Bayes then there is $o'$ which strictly dominates it; moreover, it is strictly dominated by an option which is itself Bayes and not even weakly dominated. 
	\end{theorem}
\begin{proof}
			If $o$ is not Bayes in $\Uwald(\O)$, then, by \cref{thm:cct:admiss in conv are Bayes in conv}, $\Uwald(o)$ is strictly dominated in $\Conv(\Uwald(\O))$. 
	
	By \cref{thm:cct:dominator on boundry} it is thus strictly dominated by some $Z\in\cl(\Conv(\Uwald(\O)))$ which is itself not weakly dominated in $\Conv(\Uwald(\O))$. By \cref{thm:cct:bdry are the Bayes optimal}, in fact $Z\in \Uwald(\O)$, and moreover, it is in fact a Bayes point; as required.
\end{proof}


\begin{theorem}[Main theorem: a version of Wald's Complete Class Theorem]
	Suppose $\Uwald(\O)$ is closed and convex; or that $\Uwald$ is Bayes-continuous and Bayes-existing. Then $o$ is Bayes iff $o$ is not strictly dominated.
\end{theorem}
\subsection{Applying Wald's Complete Class Theorem to probabilistic picking strategies for imprecise decision theories}

Suppose
\begin{itemize}
\item $\O$ is a specified collection of probabilistic picking strategies, $\PrPickStrategies$.
\item $\mu^*$ is a measure over $\D$.
\item $\Uwald(\prpickstrat)(\omega):=\Exp_{\mu^*}\U(\prpickstrat)(\omega)=\Exp_{\prpickstrat_{D}} \U(a)(\omega)$
\end{itemize}

Recall \cref{def:EU-complete} of $\PrPickStrategies$ being $\EU$-complete, and observe that Bayes-existing immediately follows. 
\begin{lemma}\label{thm:cct-appln:existing}
	If $\PrPickStrategies$ is EU-complete, then it is Bayes-existing. 
\end{lemma}
\begin{proof}
	By \cref{thm:EU appendix}, $\prpickstrat$ ${\mu^*}$-surely picks for $\EU_p$ iff it is a maximiser of  \cref{thm:EU appendix}. %\todooldinfo{where's the ref??} 
\end{proof}

Recall the definition of requires almost everywhere decisiveness from \cref{def:suff spread}.
\begin{lemma}\label{thm:cct-appln:cts}
	If ${\mu^*}$ requires almost everywhere decisiveness (and countably additive), then $\Uwald(\prpickstrat)(\omega):=\Exp_{\mu^*}\U({\prpickstrat_{D}})(\omega)=\Exp_{\mu^*}\Exp_{\prpickstrat_D}\U(a)(\omega)$ is Bayes continuous. 
\end{lemma}
\begin{proof}
Suppose $p^*$ is a probability function over $\Omega$. And suppose $p_1,p_2\ldots$ is a sequence of probability functions over $\Omega$ that converges on $p^*$, that is, $p_1,p_2\ldots \longrightarrow p^*$. We will show that for any $\prpickstrat^{p^*}$ which is Bayes for $p^*$ and $\prpickstrat^{p_n}$ which are Bayes for $p_n$, then $\Uwald(\prpickstrat^{p_n})(\omega)\longrightarrow\Uwald(\prpickstrat^{p^*})(\omega)$. 

Now suppose that $D$ is such that: (i) $\EU_{p^*}(D)= \{a^*\}$, (ii) $\prpickstrat^{p_n}_D(\EU_{p_n}(D))=1$, and (iii) $\prpickstrat^{p^*}_D(\EU_{p^*}(D))=1$, so that $\prpickstrat^{p^*}_D(\{a^*\}) = 1$. 
For every $a$,  $\Exp_{p_n}(a)\longrightarrow\Exp_{p^*}(a)$. If $D$ is finite, there must be some $N$ such that for all $n>N$, also $\EU_{p_n}(D)=\{a^*\}$. And then $\U(\prpickstrat^{p_n})(\omega, D)=\Exp_{\prpickstrat^{p_n}_D}a(\omega)=a^*(\omega)= \Exp_{\prpickstrat^{p^*}_D}a(\omega) = \U(\prpickstrat^{p^*})(\omega, D)$.  

If $D$ is infinite, although compact, one can use the Berge's Maximum Theorem to observe that $\EU_p(D)$ is upper hemi-continuous, so that if $p_n\longrightarrow p^*$ and $V$ is an open set with $\EU_{p^*}(D)\subseteq V$, then there is some $N$ such that for all $n>N$, $\EU_{p_n}(D)\subseteq V$. Let $V_{\epsilon}=\{a\given \abs{a(\omega)-a^*(\omega)}<\epsilon\}$, which is open containing $\EU_{p^*}(D)=\{a^*\}$. So there is some $N$ such that for all $n>N$, any $a_n\in\EU_{p_n}(D)$ is in $V_{\epsilon}$, and so $\abs{a(\omega)-a^*(\omega)}<\epsilon$; thus also $\abs{\Exp_{v_D^{p_n}} a(\omega)- a^*(\omega)}<\epsilon$; so the utility profiles converge. 

%$\epsilon>0$, then there is an $N$ such that for all $a_n\in\EU_{p_n}(D)$ there is some $a^*\in\EU_{p^*}(D)$ with $\sum_\omega |a(\omega)-a^*(\omega)|<\epsilon $. Since there is only one such $a^*$ by assumption that $\EU_{p^*}(D)=\{a^*\}$, in fact, for all $\epsilon>0$, there is $N$ such that $\Exp_{\prpickstrat_D^{p_n}} a(\omega)$ 
% so if $a\neq a^*$, 
% $$\U(\prpickstrat^{p_n})(\omega, D)=\Exp_{\prpickstrat^{p_n}_D}a(\omega)\longrightarrow \Exp_{\prpickstrat^{p^*}_D}a(\omega) = a^*(\omega)=\U(\prpickstrat^{p^*})(\omega, D).$$ 
%In fact, as we have assumed that $D$ is finite, there is $N$ such that, for $n > N$, $\EU_{p_n}(D)=\{a^*\}$. \todooldinfo{This is by ???}

Next, we show that the set of $D$ for which (i), (ii), and (iii) hold has measure 1, and so (since the utilities are bounded, by a Dominated Convergence Theorem):
$$\Exp_{\mu^*}\U(\prpickstrat^{p_n})(\omega) \longrightarrow \Exp_{\mu^*} \U(\prpickstrat^{p^*})(\omega)$$which is what we wish to prove.

We have supposed that $\prpickstrat^{p^*}$ is Bayes for $p^*$ and $\prpickstrat^{p_n}$ are Bayes for $p_n$. Thus, by \cref{thm:EU appendix}, $\prpickstrat^{p^*}$ ${\mu^*}$-surely picks for $\EU_{p^*}$
and $\prpickstrat^{p_n}$ ${\mu^*}$-surely pick for $\EU_{p_n}$. 
Let:
\begin{itemize}
	\item $\X^* = \Set{D \in \D \given \prpickstrat^{p^*}_D(\EU_{p^*}(D)) = 1 }$
	\item $\X^n = \Set{D \in \D \given \prpickstrat^{p_n}_D(\EU_{p^n}(D)) = 1 }$
	\item $\Y^* = \Set{D \in \D \given \EU_{p^*}(D) \emph{ is a singleton}}$
\end{itemize}
We thus know that ${\mu^*}(\X^*) = {\mu^*}(\X^n)=1$. We also know that ${\mu^*}(\Y^*) = 1$ by assumption of ${\mu^*}$ requiring almost everywhere decisiveness. And so, since ${\mu^*}$ is countably additive,
$${\mu^*}\left (\X^* \cap \bigcap^n_{n=1} \X^n \cap \Y^* \right ) = 1.$$This completes the proof.
\end{proof}

\begin{corollary}\label{thm:cct-applns:corr}
	Suppose that $\PrPickStrategies$ is EU-complete, $\mu^*$ requires almost everywhere decisiveness and is countably additive. 
	
%	If $\prpickstrat$ does not $\mu^*$-surely pick for $p$, for any probability $p$, then there is some $\prpickstrat'$
	
	If there is no probability $p$ over $\Omega$ for which $\prpickstrat$ maximises $\Exp_{p}[\Exp_{\mu^*}[\U(\prpickstrat)]]$; then there is some $\prpickstrat'$ such that $\Exp_{\mu^*}\U(\prpickstrat')(\omega)>\Exp_{\mu^*}\U(\prpickstrat)(\omega)$ for all $\omega\in\Omega$. 
\end{corollary}
\begin{proof}
	This is immediate from \cref{thm:cct,thm:cct-appln:existing,thm:cct-appln:cts}.
\end{proof}
\begin{corollary}
		Suppose that $\PrPickStrategies$ is EU-complete, $\mu^*$ requires almost everywhere decisiveness and is countably additive. 
		
	If $\prpickstrat$ does not $\mu^*$-surely pick for any probability $p$, then there is some $\prpickstrat'$ such that for all probabilities $p$, $\Exp_{p\times\mu^*}[\U(\prpickstrat')]>\Exp_{p\times\mu^*}[\U(\prpickstrat)]$. 
\end{corollary}
\begin{proof}
	From \cref{thm:EU appendix}, such $\prpickstrat$ thus does not maximise $\Exp_{p\times\mu^*}[\U(\prpickstrat)]$ for any $p$. Observe, also that $\Exp_p[\Exp_{\mu^*}[\U(\prpickstrat)]]=\Exp_{p\times\mu^*}[\U(\prpickstrat)]$. So by \cref{thm:cct-applns:corr}, there is some $\prpickstrat'$ with  $\Exp_{\mu^*}\U(\prpickstrat')(\omega)>\Exp_{\mu^*}\U(\prpickstrat)(\omega)$ for all $\omega$; and thus, $\Exp_{p}[\Exp_{\mu^*}[\U(\prpickstrat')]]>\Exp_p[\Exp_{\mu^*}[\U(\prpickstrat)]]$ for all probabilities $p$; which gives us the claim.
\end{proof}
\cref{thm:max-nu,thm:max-nu-reg-nec,thm:gamma-nu} follow from this. 
%%	thm:chiocefns-nu-reg-nec,thm:gamma}
%	 follow from this, making use of \cref{thm:EU appendix}.
%	 
%{\color{violet}	
%	 \begin{proof}[Proof illustration]
%	 	If $\prpickstrat$ does not $\mu^*$-surely pick for any probability $p$, then it is not a maximiser of $\Exp_p[\Uwald(\prpickstrat)]=\Exp_p[\Exp_{\mu^*}\U(\prpickstrat)]=\Exp_{p\times\mu^*}[\U(\prpickstrat)]$ for any $p$; i.e., it is not Bayes.
%	 	
%	 	It is thus is dominated, i.e., there is $\prpickstrat'$ with $\Uwald(\prpickstrat')(\omega)>\Uwald(\prpickstrat)(\omega)$ for all $\omega$; i.e., $\Exp_{\mu^*}\U(\prpickstrat')(\omega)>\Exp_{\mu^*}\U(\prpickstrat)(\omega)$ for all $\omega$; and thus, $\Exp_{p}\Exp_{\mu^*}[\U(\prpickstrat')]>\Exp_p\Exp_{\mu^*}[\U(\prpickstrat)]$ for all probabilities $p$. 
%	 \end{proof}
%}
\todooldinfo{If we're rewriting the EU result, check how this looks!}












\end{document}










\subsection{An example}
We can construct an example to illustrate our result using the Ellsberg paradox. 

\begin{quote}
	An urn contains 90 balls. You know that 30 of them are red, and the remaining 60 are black and yellow, but you don’t know how many are black and how many are yellow. I am about to draw a ball from the urn.
\end{quote}
And so, if the states of the world are \emph{Red}, \emph{Black}, \emph{Yellow}, you might naturally take your credal set to be $\IP = \Set{p \given p(\emph{Red}) = \sfrac{1}{3}\ \&\ p(\emph{Black})+p(\emph{Yellow}) = \sfrac{2}{3}}$.
Now consider the following two possible decision problems, $D^{\mathrm{Ellsberg}}_1$ and $D^{\mathrm{Ellsberg}}_2$:
$$
\begin{array}{r|ccc}
D^{\mathrm{Ellsberg}}_1 & \emph{Red} & \emph{Black} &\emph{Yellow} \Bstrut \\\hline \Tstrut\IP & \sfrac{1}{3} & \multicolumn{2}{c}{\sfrac{2}{3}}\Bstrut \\\hline \hline\Tstrut 
\text{1A} & \text{\pounds 10} & \text{\pounds 0}  & \text{\pounds 0} \\
\text{1B} & \text{\pounds 1} & \text{\pounds 11}  & \text{\pounds 1} 
\end{array}
\hspace{10mm}
\begin{array}{r|ccc}
D^{\mathrm{Ellsberg}}_2 & \emph{Red} & \emph{Black} &\emph{Yellow} \Bstrut \\\hline \Tstrut\IP & \sfrac{1}{3} & \multicolumn{2}{c}{\sfrac{2}{3}}\Bstrut \\\hline \hline\Tstrut 
\text{2A} & \text{\pounds 11} & \text{\pounds 1}  & \text{\pounds 11} \\
\text{2B} & \text{\pounds 0} & \text{\pounds 10}  & \text{\pounds 10} 
\end{array}
$$
Faced with these decisions, people often give the Ellsberg preferences: $\text{1A} \succ \text{1B}$ and $\text{2B} \succ \text{2A}$.\footnote{In fact, we have added a small constant to the usual versions of 1B and 2A, reflecting the fact that people strictly prefer the usual version of 1A over the usual version of 1B, and so are willing to pay a penalty for making that choice; we've taken that penalty to be \pounds 1.}


We can then calculate which acts maximize expected utility by the lights of a probability function in $\IP$ (\cref{tab:Ellsberg EU recommendations}).

\begin{table}[ht]
	\[
	\begin{array}{lcc}
		\toprule
		\text{Constraint on } p(\emph{Yellow}) & \EU_p(D^{\mathrm{Ellsberg}}_1) & \EU_p(D^{\mathrm{Ellsberg}}_2) \\
		\midrule
		p(\emph{Yellow}) < \tfrac{7}{30} & \{1B'\} & \{2B\} \\[1mm]
		p(\emph{Yellow}) = \tfrac{7}{30} & \{1B'\} & \{2A,\,2B\} \\[1mm]
		\tfrac{7}{30} < p(\emph{Yellow}) < \tfrac{13}{30} & \{1B'\} & \{2A\} \\[1mm]
		p(\emph{Yellow}) = \tfrac{13}{30} & \{1A,\,1B'\} & \{2A'\} \\[1mm]
		p(\emph{Yellow}) > \tfrac{13}{30} & \{1A\} & \{2A'\} \\
		\bottomrule
	\end{array}
	\]
\caption{EU\(_p\) recommendations for decisions \(D^{\mathrm{Ellsberg}}_1\) and \(D^{\mathrm{Ellsberg}}_2\).\label{tab:Ellsberg EU recommendations}}

\end{table}

So, faced with $D^{\mathrm{Ellsberg}}_1$, either 1A or 1B is E-Admissible; and faced with $D^{\mathrm{Ellsberg}}_2$, either 2A or 2B is E-Admissible. And so any picking strategy picks for E-Admissibility in this case. However, the strategy that picks $1A$ from $D^{\mathrm{Ellsberg}}_1$ and $2B$ from $D^{\mathrm{Ellsberg}}_2$ picks for E-Admissibility, but it does not maximize expected utility by the lights of any $\langle p, \mu \rangle$ in $\IB$. That is, it is not in $\EAd_\IB(\S)$. 

There are, however, strategies that pick for E-Admissibility and that E-Admissibility deems permissible: that is, $\s$ picks for $\EAd_\IP$, and $\s$ is in $\EAd_\IB(\S)$. For example, the strategy that picks $1A$ from $D^{\mathrm{Ellsberg}}_1$ and $2A$ from $D^{\mathrm{Ellsberg}}_2$. 

So E-admissibility deems some of its strategies permissible and others not. In fact, for every E-Admissible act in a fixed decision $D$, there is an E-Admissibility strategy which is E-Admissible and which selects that option in $D$. What might be ruled out, however, is certain combinations. For the strategy to be E-Admissible, it requires coordination across the various decision problems. 




%
%
%What does our decision theory tell us to choose in these decisions? 
%
%In $D^{\mathrm{Ellsberg}}_1$, the $\EU_p$ action is to chooses $1A$ if $p(\text{Red})>\sfrac{1}{3}$ and $1B$ if $p(\text{Red})<\sfrac{1}{3}$. When $p(\text{Red})>\sfrac{1}{3}$, it is indifferent and thus both ways of picking in $\EU_p$. Thus, for $D^{\mathrm{Ellsberg}}_1$, both picking $1A$ and $1B$ are E-Admissible actions, as one has a credal set containing both such probabilities. 
%
%In $D^{\mathrm{Ellsberg}}_2$, the $\EU_p$ action is to chooses $2A$ if $p(\text{Yellow})>\sfrac{1}{3}$ and $2B$ if $p(\text{Yellow})<\sfrac{1}{3}$. Thus, again, for $D^{\mathrm{Ellsberg}}_2$, both picking $2A$ and $2B$ are E-Admissible actions, as one has a credal set containing both such probabilities. 
%
%We can now judge which \emph{strategies} are E-Admissible. To do this we need to specify your uncertainty, $\mu$, over which decision you'll be faced with. Let's suppose you think it's equally likely. 




\subsection{$\Gamma$-maximin}\label{sect:gamma}
Some decision theories go beyond E-Admissibility and permit only some subset of the E-Admissible acts. Any such theory will be susceptible to the same self-undermining concerns that we saw affecting E-admissibility above: there will be strategies that pick for the choice function given by that theory that the theory itself deems impermissible. The situation may in fact be worse for such theories because for E-Admissibility there are guaranteed to be some strategies that pick for E-Admissibility that E-Admissibility also permits: strategies that successfully coordinate across decisions. But for more restrictive theories these coordinated strategies may be ruled out by the theory, and we might have the result that all the strategies that pick for the theory's choice function are deemed impermissible by the theory. 

A prominent example of a theory that is more restrictive than E-Admissibility is $\Gamma$-Maximin. 

\begin{definition}[$\Gamma$-Maximin$_\IP$ ($\Maximin_\IP$) ] 
$$
\Maximin_\IP(D) = \left \{ a \in D \mid (\forall a' \in D)\left [\min_{p \in \IP} \Exp_p[\U(a)'] \leq \min_{p \in \IP} \Exp_p[\U(a)] \right ] \right \}
$$
(This should only be applied when these minima exist, e.g., when $\IP$ is a closed convex set.)\todoold{closed? closed and convex?}
\end{definition}
In the Ellsberg case as described above, this requires one to select $1A$ in $D^{\mathrm{Ellsberg}}_1$ and $2B$ in $D^{\mathrm{Ellsberg}}_2$; that is the only strategy recommended by $\Maximin_\IP$ is one that is not E-Admissible. It is thus also not itself acceptable according to Maximin. That is, there is a unique $\Maximin_\IP$ strategy $\s$, and this is not in $\Maximin_{\IB}(\S)$. Maximin undermines its own recommendations, evaluating its unique picking strategy to be impermissible. 

\todooldinfo{Do we have a general result saying that for any $\mu$, Gamma-Maximin undermines all the compatible strategies?? RP: my sense is we can't get this, because there will be some string of decision problems whose $\min_{p \in P}$ is given by a single p in P.}
















% That is, is $\s_E$ in $\Maximin_\IB(\S)$? And the answer is no. If an act $a$ is permissible by the lights of $\Gamma$-Maximin applied to a credal set $\IP$, then there must be some $p$ in $\IP$ for which $a$ maximizes expected utility. After all, as we saw above, if $\s$ maximizes expected utility for $\langle p, \mu \rangle$, then $\s$ picks for $\EU_p$. But $\s_E$ does not maximize expected utility for any $\langle p, \mu \rangle$ in $\IB$. Indeed, this fact accounts for Ellsberg's use of the case: like Allais, he wished to provide natural preferences that could not be captured by expected utility theory. So, at least in the Ellsberg case, where we are uncertain whether we'll face $D^{\mathrm{Ellsberg}}_1$ or $D^{\mathrm{Ellsberg}}_2$, $\Gamma$-Maximin is self-undermining. The only picking strategy that picks for that decision theory with $\IP$---namely, $\s_E$---is not permitted by that decision theory with $\IB$.


%We can then judge which strategies are E-Admissible, i.e., which strategies are such that there is at least one probability function which evaluates them as optimal. To answer this, we have to describe the agent's uncertainty over the states of the world and the decision problems together. We will represent this with a set of probabilities $\IB$ over $\Omega\times\D$, where each probability in $\IB$ treats $\Omega$ and $\D$ as independent. So we can equally think of $\IB$ as a set of pairs $p\times \mu$ where $p$ is in probability function over $\Omega$ and $\mu$ is a probability function over $\D$. For $\IB$ to extend $\IP$, it must be that... \todooldinfo{JK to help with making this stuff correct}


%We now have the tools to apply the criteria of E-Admissibility to strategies themselves. First, we specify the set of picking strategies that E-Admissibility deems permissible:% and we have:%\todooldinfo{is this now a defn or a thm??} 
%\begin{definition}
% $$\EAd_\IB(\Strategies) = \Set{\s \in \Strategies \given (\exists \langle p, \mu \rangle \in \IB)(\forall \s' \in \Strategies)(\Exp_{p, \mu}[\U(\s)]\geq\EU_{p\times\mu}[\U(\s')]})$$
%\end{definition}
%But we know from \cref{thm:eu-uniquely-optimal} that the only strategies which maximise $\EU_{p\times\mu}[\U(\s)]$ are those that $\mu$-surely pick for $\EU_p$. We thus immediately have:
%\begin{theorem}
%	$\s\in\EAd_{\IB}(\Strategies)$ iff $\s$  $\mu$-surely picks for $\EU_p$, for some $p\times \mu\in \IB$. 
%\end{theorem}
%This tells us that the only strategies that are E-Admissible are those that $\mu$ is certain pick for $\EU_p$, for some $p$ in the credal set $\IP$. 





%Suppose you have imprecise credal set $\Set{p_1,p_2}$, where $p_1$ recommends $a_1$ and $p_2$ recommends $a_2$. The E-admissible s

%This requires coordination across decision problems! There is no \emph{inherent} imprecision permissible. \todoold{this final sentence needs to be moved}




%\subsection{Reflections}
\subsection{Can a decision theory require coordination?}
$\Gamma$-Maximin requires one to choose options that the original theory rejects. It undermines its own recommendations. This is a significant problem. 

For E-Admissibility it is not quite so problematic as it only rejects some strategies compatible with its recommendations. What we learn from this is that E-Admissibility requires us to coordinate our picking plans across various decision problems we think we might be faced with. 

This is perhaps quite familiar to supporters of E-Admissibility. The kind of case we have described can also be used to demonstrated diachronic irrationality and it can be responded to by requiring coordination between ones way of picking across time. \todoold{refs etc!!!}

There is something a bit more problematic in applying this strategy to choice strategies rather than diachronic coordination. There's something weird about it that it's not just giving you guidance in each case, which is kind of what we thought a dec theory would do?? For example, when the general formalism is developed for imprecise probability it is presented in terms of so-called choice functions. These prescribe a set of acts from each decision which are those that are not ruled out. It is $C:\D\to\A$; there's nothing in this formalism to describe coordination. 

\todooldinfo{help me with this above section please. RP: I'm not sure what to say either! It seems to me that resolute choice (i.e. coordination) is as good a solution in our case as in the diachronic case? But I take it the two arguments we gave in 1.1 against self-undermining theories are arguments against resolute choice. Yes, you could be resolute, but it gives rise to dilemmas: you have reason beforehand to commit to a coordinated resolute decision, but then when faced with the decision, you've no reason to stick with that commitment.}

\todoold{we thought of a segway into next section, but now I don't see it.}



\subsection{Maximality and more general choice functions}\label{sect:Max}


\todooldinfo{I think this section is the right track, but will need writing up better.}
We have so far looked at the imprecise decision theories of E-Admissibility and $\Gamma$-Maximin, and shown that only the strategies that $\mu$-surely pick for $\EU_p$ for some $p$ in $\IP$ that are evaluated as permissible according to these theories.% because it is only these strategies which are evaluated as optimal according to some probability $p$ in the credal set $\IP$. 
The argument does not, however, apply directly to the decision rule of Maximality. 
Maximality only rejects an available action if there is a particular alternative which every probability in $\IP$ agrees to be better. It is not sufficient that each probability in $\IP$ thinks that something is better. 

\begin{definition}
	$a\in \Maximality_\IP(D)$ iff $$\exists a'\in D\;\forall p\in\IP\quad \Exp_p(a')>\Exp_p(a')$$
\end{definition}
%\todooldinfo{should we introduce more general choice functions too??}






